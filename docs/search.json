[{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Advanced Topics","text":"vignette covers advanced topics power users, researchers, method developers: Understanding Algorithms - Exact vs greedy, complexity analysis Custom Engines - Integrate modeling package (INLA, mgcv, brms) Exact Subset Enumeration - Multiple maximal subsets Performance Optimization - Speed memory considerations Troubleshooting - Common issues solutions Target audience: Users comfortable R programming statistical methods Estimated time: 15-20 minutes","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"exact-vs-greedy-when-to-use-each","dir":"Articles","previous_headings":"1. Understanding the Algorithms","what":"1.1 Exact vs Greedy: When to Use Each","title":"Advanced Topics","text":"corrselect offers two algorithmic approaches corrPrune():","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"exact-mode-graph-theoretic","dir":"Articles","previous_headings":"1. Understanding the Algorithms > 1.1 Exact vs Greedy: When to Use Each","what":"Exact Mode (Graph-Theoretic)","title":"Advanced Topics","text":"Algorithm: Eppstein–Löffler–Strash (ELS) Bron–Kerbosch Complexity: O(2^p) - exponential number predictors Guarantee: Finds largest maximal independent set Use exact mode : p ≤ 20 (feasible runtime) need guaranteed optimal solution Reproducibility critical ’re writing paper (justify optimality)","code":"data(mtcars)  # Exact mode: guaranteed optimal exact_result <- corrPrune(mtcars, threshold = 0.7, mode = \"exact\") cat(\"Exact mode kept:\", ncol(exact_result), \"variables\\n\") #> Exact mode kept: 5 variables"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"greedy-mode-heuristic","dir":"Articles","previous_headings":"1. Understanding the Algorithms > 1.1 Exact vs Greedy: When to Use Each","what":"Greedy Mode (Heuristic)","title":"Advanced Topics","text":"Algorithm: Deterministic iterative removal Complexity: O(p² × k) k = iterations Guarantee: Near-optimal, deterministic Use greedy mode : p > 20 (exact becomes slow) Speed priority Near-optimal acceptable High-dimensional data (p > 100)","code":"# Greedy mode: fast approximation greedy_result <- corrPrune(mtcars, threshold = 0.7, mode = \"greedy\") cat(\"Greedy mode kept:\", ncol(greedy_result), \"variables\\n\") #> Greedy mode kept: 5 variables"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"auto-mode-recommended","dir":"Articles","previous_headings":"1. Understanding the Algorithms > 1.1 Exact vs Greedy: When to Use Each","what":"Auto Mode (Recommended)","title":"Advanced Topics","text":"Automatically selects based p:","code":"# Auto mode: smart switching (exact if p ≤ 20, greedy otherwise) auto_result <- corrPrune(mtcars, threshold = 0.7, mode = \"auto\") cat(\"Auto mode kept:\", ncol(auto_result), \"variables\\n\") #> Auto mode kept: 5 variables"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"complexity-analysis-with-benchmarks","dir":"Articles","previous_headings":"1. Understanding the Algorithms","what":"1.2 Complexity Analysis with Benchmarks","title":"Advanced Topics","text":"Let’s measure runtime scaling:  Key insight: scaling behavior depends heavily correlation structure. moderately correlated dataset (ρ = 0.5^|-j|), exact mode remains practical p ≈ 500, greedy mode scales efficiently beyond p = 1000. exact mode provides guaranteed optimality cost computational complexity, greedy mode offers substantial speed advantages large p near-optimal results practical scenarios.","code":"# Generate datasets with increasing p library(microbenchmark)  benchmark_corrPrune <- function(p_values) {   results <- data.frame(     p = integer(),     exact_time_ms = numeric(),     greedy_time_ms = numeric()   )    for (p in p_values) {     # Generate correlated data     set.seed(123)     cor_mat <- 0.5^abs(outer(1:p, 1:p, \"-\"))     data <- as.data.frame(MASS::mvrnorm(n = 100, mu = rep(0, p), Sigma = cor_mat))      # Exact mode (skip if p too large)     exact_time_ms <- if (p <= 500) {       median(microbenchmark(         corrPrune(data, threshold = 0.7, mode = \"exact\"),         times = 3,  # Few iterations for speed         unit = \"ms\"       )$time) / 1e6  # Convert nanoseconds to milliseconds     } else {       NA     }      # Greedy mode     greedy_time_ms <- median(microbenchmark(       corrPrune(data, threshold = 0.7, mode = \"greedy\"),       times = 3,  # Few iterations for speed       unit = \"ms\"     )$time) / 1e6  # Convert nanoseconds to milliseconds      results <- rbind(results, data.frame(       p = p,       exact_time_ms = round(exact_time_ms, 1),       greedy_time_ms = round(greedy_time_ms, 1)     ))   }    results }  # Benchmark (extended range to show comprehensive scaling behavior) p_values <- c(10, 20, 50, 100, 200, 300, 500, 1000) benchmark <- benchmark_corrPrune(p_values) print(benchmark) #>      p exact_time_ms greedy_time_ms #> 1   10           0.4            0.2 #> 2   20           0.7            0.5 #> 3   50           1.7            0.8 #> 4  100           4.5            1.4 #> 5  200          22.2            3.4 #> 6  300          84.6            6.6 #> 7  500         306.4           15.1 #> 8 1000            NA           53.8 # Visualize scaling # Separate data for exact mode (only where available) and greedy mode (all points) exact_valid <- !is.na(benchmark$exact_time_ms) & benchmark$exact_time_ms > 0 greedy_valid <- !is.na(benchmark$greedy_time_ms) & benchmark$greedy_time_ms > 0  # Replace any zeros with small positive value for log scale exact_times <- benchmark$exact_time_ms greedy_times <- benchmark$greedy_time_ms exact_times[exact_times <= 0 & !is.na(exact_times)] <- 0.01 greedy_times[greedy_times <= 0 & !is.na(greedy_times)] <- 0.01  # Determine y-axis limits from valid data all_valid_times <- c(exact_times[exact_valid], greedy_times[greedy_valid]) ylim <- c(min(all_valid_times) * 0.5, max(all_valid_times) * 2)  # Plot greedy mode (all points) plot(benchmark$p[greedy_valid], greedy_times[greedy_valid],      type = \"b\", col = rgb(0.2, 0.5, 0.8, 1), pch = 19, lwd = 2,      xlab = \"Number of Predictors (p)\",      ylab = \"Time (milliseconds, log scale)\",      main = \"Exact vs Greedy Scaling\",      ylim = ylim,      xlim = range(benchmark$p),      log = \"y\")  # Add exact mode (only where available) lines(benchmark$p[exact_valid], exact_times[exact_valid],       type = \"b\", col = rgb(0.8, 0.2, 0.2, 1), pch = 19, lwd = 2)  # Mark exact mode limit abline(v = 500, lty = 2, col = \"gray50\") text(500, ylim[2] * 0.5, \"Exact mode limit\", pos = 4, col = \"gray30\")  legend(\"topleft\",        legend = c(\"Exact\", \"Greedy\"),        col = c(rgb(0.8, 0.2, 0.2, 1), rgb(0.2, 0.5, 0.8, 1)),        pch = 19,        lwd = 2,        bty = \"o\",        bg = NA)"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"deterministic-tie-breaking","dir":"Articles","previous_headings":"1. Understanding the Algorithms","what":"1.3 Deterministic Tie-Breaking","title":"Advanced Topics","text":"multiple variables identical correlation profiles, corrselect uses deterministic tie-breaking: Tie-breaking rules: 1. Prefer variables lower mean absolute correlation others 2. still tied, prefer lexicographically first (alphabetical) ensures reproducibility across runs, machines, R versions.","code":"# Create data with identical correlations set.seed(123) x1 <- rnorm(100) x2 <- x1 + rnorm(100, sd = 0.1)  # Almost identical to x1 x3 <- x1 + rnorm(100, sd = 0.1)  # Also almost identical to x1 x4 <- rnorm(100)                  # Independent  data_ties <- data.frame(x1, x2, x3, x4)  # Run multiple times - always same result result1 <- corrPrune(data_ties, threshold = 0.95) result2 <- corrPrune(data_ties, threshold = 0.95)  cat(\"Run 1 selected:\", names(result1), \"\\n\") #> Run 1 selected: x2 x4 cat(\"Run 2 selected:\", names(result2), \"\\n\") #> Run 2 selected: x2 x4 cat(\"Identical:\", identical(names(result1), names(result2)), \"\\n\") #> Identical: TRUE"},{"path":[]},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"what-is-a-custom-engine","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.1 Understanding Custom Engines","what":"What is a Custom Engine?","title":"Advanced Topics","text":"custom engine allows integrate modeling framework modelPrune(), just base R’s lm() lme4. enables VIF-based pruning : Bayesian models (INLA, brms, Stan) Additive models (mgcv GAMs) Survival models (coxph, flexsurv) Custom metrics (AIC, BIC, posterior uncertainty)","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"how-custom-engines-work","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.1 Understanding Custom Engines","what":"How Custom Engines Work","title":"Advanced Topics","text":"modelPrune() algorithm follows iterative process: Fit model current predictors Diagnose predictor (compute “badness” metric) Identify worst predictor (highest diagnostic value) Remove exceeds limit threshold Repeat predictors satisfy limit custom engine defines steps 1 2; modelPrune() handles iteration logic.","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"engine-structure-requirements","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.1 Understanding Custom Engines","what":"Engine Structure Requirements","title":"Advanced Topics","text":"custom engine named list two required functions: Key principle: diagnostics function must return higher values worse predictors. inverted metric ensures algorithm removes problematic variables first.","code":"my_engine <- list(   # Required: How to fit the model   fit = function(formula, data, ...) {     # Your model fitting code     # Must return a fitted model object   },    # Required: How to compute diagnostics   diagnostics = function(model, fixed_effects) {     # Compute diagnostic scores for each fixed effect     # Higher values = worse (more likely to be removed)     # Must return a named numeric vector   },    # Optional: Name for error messages   name = \"my_custom_engine\"  # Defaults to \"custom\" )"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"background","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.2 Example: INLA Engine (Bayesian Spatial Models)","what":"Background","title":"Advanced Topics","text":"INLA (Integrated Nested Laplace Approximations) popular package fast Bayesian inference, especially spatial temporal models. Unlike traditional VIF, can use posterior uncertainty pruning criterion: variables high posterior standard deviation contribute less information model.","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"implementation","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.2 Example: INLA Engine (Bayesian Spatial Models)","what":"Implementation","title":"Advanced Topics","text":"","code":"# Custom engine for INLA inla_engine <- list(   name = \"inla\",    fit = function(formula, data, ...) {     # Fit INLA model     INLA::inla(       formula = formula,       data = data,       family = list(...)$family %||% \"gaussian\",       control.compute = list(config = TRUE),       ...     )   },    diagnostics = function(model, fixed_effects) {     # Use posterior SD as \"badness\" metric     # Higher SD = more uncertain = candidate for removal     summary_fixed <- model$summary.fixed     scores <- summary_fixed[, \"sd\"]     names(scores) <- rownames(summary_fixed)      # Return scores for fixed effects only     scores[fixed_effects]   } )  # Usage example pruned <- modelPrune(   y ~ x1 + x2 + x3 + x4,   data = my_data,   engine = inla_engine,   limit = 0.5  # Remove if posterior SD > 0.5 )"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"how-it-works","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.2 Example: INLA Engine (Bayesian Spatial Models)","what":"How It Works","title":"Advanced Topics","text":"fit: Calls INLA::inla() compute posterior distributions diagnostics: Extracts posterior standard deviations model$summary.fixed limit: Threshold acceptable uncertainty (e.g., 0.5 means remove predictors posterior SD > 0.5) Interpretation: Variables high posterior SD coefficients uncertain given data. Removing reduces model complexity without losing much information. use: Spatial models, hierarchical models, disease mapping, ecological modeling INLA.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"background-1","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.3 Example: mgcv Engine (GAMs)","what":"Background","title":"Advanced Topics","text":"Generalized Additive Models (GAMs) allow non-linear relationships via smooth terms. pruning GAMs, want remove parametric (linear) terms weak evidence preserving smooth terms model non-linear patterns.","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"implementation-1","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.3 Example: mgcv Engine (GAMs)","what":"Implementation","title":"Advanced Topics","text":"","code":"# Custom engine for mgcv GAMs mgcv_engine <- list(   name = \"mgcv_gam\",    fit = function(formula, data, ...) {     mgcv::gam(formula, data = data, ...)   },    diagnostics = function(model, fixed_effects) {     # Use p-values as badness metric     # Higher p-value = less significant = candidate for removal     summary_obj <- summary(model)      # Extract parametric term p-values     pvals <- summary_obj$p.pv      # Return p-values for fixed effects     pvals[fixed_effects]   } )  # Usage example pruned <- modelPrune(   y ~ x1 + x2 + s(x3),  # s() for smooth terms   data = my_data,   engine = mgcv_engine,   limit = 0.05  # Remove if p > 0.05 )"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"how-it-works-1","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.3 Example: mgcv Engine (GAMs)","what":"How It Works","title":"Advanced Topics","text":"fit: Calls mgcv::gam() fit additive model diagnostics: Extracts p-values parametric terms (smooth terms) limit: Significance threshold (e.g., 0.05 traditional α = 0.05) Important: modelPrune() removes parametric (linear) terms. Smooth terms specified s(), te(), etc. never removed automatically, ’re part model structure. use: Non-linear regression, ecological response curves, time series trends.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"background-2","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.4 Example: Custom Criterion (AIC-Based)","what":"Background","title":"Advanced Topics","text":"Sometimes want prune based model comparison metrics rather coefficient-level diagnostics. example shows remove variables don’t improve model fit according AIC.","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"implementation-2","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.4 Example: Custom Criterion (AIC-Based)","what":"Implementation","title":"Advanced Topics","text":"","code":"# AIC-based pruning engine aic_engine <- list(   name = \"aic_pruner\",    fit = function(formula, data, ...) {     lm(formula, data = data)   },    diagnostics = function(model, fixed_effects) {     # For each predictor, compute ΔAIC if removed     full_aic <- AIC(model)      scores <- numeric(length(fixed_effects))     names(scores) <- fixed_effects      for (var in fixed_effects) {       # Refit without this variable       reduced_formula <- update(formula(model), paste(\"~ . -\", var))       reduced_model <- lm(reduced_formula, data = model$model)        # ΔAIC: negative means removing improves model       # We negate so \"higher = worse\" convention holds       scores[var] <- -(AIC(reduced_model) - full_aic)     }      scores   } )  # Usage: Remove predictors with ΔAIC < -2 (improve AIC by > 2 when removed) pruned <- modelPrune(   y ~ x1 + x2 + x3,   data = my_data,   engine = aic_engine,   limit = -2 )"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"how-it-works-2","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.4 Example: Custom Criterion (AIC-Based)","what":"How It Works","title":"Advanced Topics","text":"fit: Standard linear model diagnostics: variable, refit model without variable compute ΔAIC limit: Threshold ΔAIC (e.g., -2 means “remove AIC improves 2 points”) Key insight: score negated (multiplied -1) maintain “higher worse” convention. variable score > -2 means removing worsen AIC 2, ’s kept. use: Model selection focused parsimony, comparing nested models, VIF isn’t right metric. Alternative metrics: can adapt pattern BIC, likelihood ratio tests, model comparison criterion.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"automatic-validation","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.5 Validation and Error Handling","what":"Automatic Validation","title":"Advanced Topics","text":"modelPrune() automatically validates custom engines catch common errors early:","code":"# Invalid engine: missing 'diagnostics' bad_engine <- list(   fit = function(formula, data, ...) lm(formula, data = data)   # Missing 'diagnostics' )  tryCatch({   modelPrune(mpg ~ ., data = mtcars, engine = bad_engine, limit = 5) }, error = function(e) {   cat(\"Error:\", e$message, \"\\n\") }) #> Error: Custom engine missing required fields: diagnostics #> Required: 'fit' and 'diagnostics'"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"validation-checklist","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.5 Validation and Error Handling","what":"Validation Checklist","title":"Advanced Topics","text":"custom engine must satisfy requirements: Structure: Named list fit diagnostics functions fit returns model object: Must return something diagnostics function can consume diagnostics returns numeric vector: characters, factors, types Correct length: One diagnostic value per fixed effect (excluding intercept) Named vector: Names must match variable names exactly missing values: NA, NaN, Inf cause errors Higher = worse: Diagnostic values must increase problematic predictors","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"debugging-tips","dir":"Articles","previous_headings":"2. Custom Engines for modelPrune() > 2.5 Validation and Error Handling","what":"Debugging Tips","title":"Advanced Topics","text":"custom engine fails, check:","code":"# Test your fit function in isolation test_model <- my_engine$fit(y ~ x1 + x2, data = my_data) summary(test_model)  # Does it work?  # Test your diagnostics function test_diag <- my_engine$diagnostics(test_model, c(\"x1\", \"x2\")) print(test_diag)  # Numeric? Named? Correct length?  # Check that \"higher = worse\" convention is satisfied # The variable with the highest score should be the one you'd remove first"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"when-you-need-all-maximal-subsets","dir":"Articles","previous_headings":"3. Exact Subset Enumeration","what":"3.1 When You Need ALL Maximal Subsets","title":"Advanced Topics","text":"corrPrune() returns single subset. Sometimes want maximal subsets:","code":"# corrPrune: Single subset single <- corrPrune(mtcars, threshold = 0.7) cat(\"corrPrune returned:\", ncol(single), \"variables\\n\") #> corrPrune returned: 5 variables  # corrSelect: ALL maximal subsets (use higher threshold to ensure subsets exist) all_subsets <- corrSelect(mtcars, threshold = 0.9) show(all_subsets) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: pearson #>   Threshold:   0.900 #>   Subsets:     2 valid combinations #>   Data Rows:   32 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] mpg, disp, hp, drat, wt, qsec...  0.527  0.888    10 #>   [ 2] mpg, cyl, hp, drat, wt, qsec,...  0.531  0.868    10"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"exploring-multiple-subsets","dir":"Articles","previous_headings":"3. Exact Subset Enumeration","what":"3.2 Exploring Multiple Subsets","title":"Advanced Topics","text":"multiple maximal subsets exist, can explore :","code":"if (length(all_subsets@subset_list) > 0) {   # Display first few subsets   cat(\"First few maximal subsets:\\n\")   for (i in seq_len(min(3, length(all_subsets@subset_list)))) {     cat(sprintf(\"\\nSubset %d (avg corr = %.3f):\\n\", i, all_subsets@avg_corr[i]))     cat(\" \", paste(all_subsets@subset_list[[i]], collapse = \", \"), \"\\n\")   }    # Analyze subset characteristics   subset_sizes <- lengths(all_subsets@subset_list)   cat(\"\\nSubset sizes:\\n\")   print(table(subset_sizes))    cat(\"\\nAverage correlations:\\n\")   print(summary(all_subsets@avg_corr)) } else {   cat(\"No subsets found at threshold 0.9\\n\") } #> First few maximal subsets: #>  #> Subset 1 (avg corr = 0.527): #>   mpg, disp, hp, drat, wt, qsec, vs, am, gear, carb  #>  #> Subset 2 (avg corr = 0.531): #>   mpg, cyl, hp, drat, wt, qsec, vs, am, gear, carb  #>  #> Subset sizes: #> subset_sizes #> 10  #>  2  #>  #> Average correlations: #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.5269  0.5280  0.5290  0.5290  0.5301  0.5311"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"extracting-specific-subsets","dir":"Articles","previous_headings":"3. Exact Subset Enumeration","what":"3.3 Extracting Specific Subsets","title":"Advanced Topics","text":"","code":"if (length(all_subsets@subset_list) > 0) {   # Extract subset with lowest average correlation   best_idx <- which.min(all_subsets@avg_corr)   best_subset <- corrSubset(all_subsets, mtcars, which = best_idx)    cat(\"Best subset (lowest avg correlation):\\n\")   print(names(best_subset))    # Extract subset with most predictors   subset_sizes <- lengths(all_subsets@subset_list)   largest_idx <- which.max(subset_sizes)   largest_subset <- corrSubset(all_subsets, mtcars, which = largest_idx)    cat(\"\\nLargest subset:\\n\")   print(names(largest_subset)) } else {   cat(\"No subsets to extract at threshold 0.9\\n\") } #> Best subset (lowest avg correlation): #>  [1] \"mpg\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\" \"carb\" #>  #> Largest subset: #>  [1] \"mpg\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\" \"carb\""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"advanced-domain-specific-subset-selection","dir":"Articles","previous_headings":"3. Exact Subset Enumeration","what":"3.4 Advanced: Domain-Specific Subset Selection","title":"Advanced Topics","text":"can define custom criteria choosing among multiple subsets:","code":"if (length(all_subsets@subset_list) > 0) {   # Custom criterion: Prefer subsets with specific variables   preferred_vars <- c(\"mpg\", \"hp\", \"wt\")    # Compute score: number of preferred variables in each subset   scores <- sapply(all_subsets@subset_list, function(vars) {     sum(preferred_vars %in% vars)   })    # Select subset with most preferred variables   best_idx <- which.max(scores)   cat(\"Subset with most preferred variables (score:\", scores[best_idx], \"):\\n\")   cat(paste(all_subsets@subset_list[[best_idx]], collapse = \", \"), \"\\n\")    # Extract as data frame   preferred_subset <- corrSubset(all_subsets, mtcars, which = best_idx)   print(head(preferred_subset)) } else {   cat(\"No subsets available for custom selection\\n\") } #> Subset with most preferred variables (score: 3 ): #> mpg, disp, hp, drat, wt, qsec, vs, am, gear, carb  #>                    mpg disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1  225 105 2.76 3.460 20.22  1  0    3    1"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"precomputed-correlation-matrices","dir":"Articles","previous_headings":"4. Performance Optimization","what":"4.1 Precomputed Correlation Matrices","title":"Advanced Topics","text":"repeated pruning different thresholds, precompute correlation matrix: Use precomputed matrices : Testing multiple thresholds Cross-validation workflows Sensitivity analysis","code":"# Create larger dataset for meaningful timing comparison set.seed(123) large_data <- as.data.frame(matrix(rnorm(100 * 50), ncol = 50))  # Benchmark: Recompute correlation every time time1 <- median(microbenchmark(   {     result1 <- corrPrune(large_data, threshold = 0.7)     result2 <- corrPrune(large_data, threshold = 0.8)     result3 <- corrPrune(large_data, threshold = 0.9)   },   times = 3,   unit = \"ms\" )$time) / 1e6  # Convert nanoseconds to milliseconds  # Benchmark: Compute correlation once, reuse cor_matrix <- cor(large_data) time2 <- median(microbenchmark(   {     result1 <- MatSelect(cor_matrix, threshold = 0.7)     result2 <- MatSelect(cor_matrix, threshold = 0.8)     result3 <- MatSelect(cor_matrix, threshold = 0.9)   },   times = 3,   unit = \"ms\" )$time) / 1e6  # Convert nanoseconds to milliseconds  cat(sprintf(\"Recomputing each time: %.1f ms\\n\", time1)) #> Recomputing each time: 4.1 ms cat(sprintf(\"Precomputed matrix: %.1f ms\\n\", time2)) #> Precomputed matrix: 2.1 ms cat(sprintf(\"Speedup: %.1fx faster\\n\", time1 / time2)) #> Speedup: 2.0x faster"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"memory-considerations-for-large-data","dir":"Articles","previous_headings":"4. Performance Optimization","what":"4.2 Memory Considerations for Large Data","title":"Advanced Topics","text":"large datasets (n > 10,000, p > 500):","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"memory-efficient-correlation-computation","dir":"Articles","previous_headings":"4. Performance Optimization > 4.2 Memory Considerations for Large Data","what":"Memory-Efficient Correlation Computation","title":"Advanced Topics","text":"","code":"# Standard (memory-intensive for large n) cor_matrix <- cor(large_data)  # Memory-efficient alternative (for very large n) # Process in chunks if needed compute_cor_chunked <- function(data, chunk_size = 1000) {   n <- nrow(data)   n_chunks <- ceiling(n / chunk_size)    # Use online algorithm or chunked computation   # (Implementation depends on your data size) }"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"sparse-correlation-matrices","dir":"Articles","previous_headings":"4. Performance Optimization > 4.2 Memory Considerations for Large Data","what":"Sparse Correlation Matrices","title":"Advanced Topics","text":"correlations low, consider sparse storage:","code":"# Convert to sparse format (requires Matrix package) library(Matrix)  # Compute correlation cor_mat <- cor(data)  # Threshold and convert to sparse cor_sparse <- cor_mat cor_sparse[abs(cor_sparse) < 0.3] <- 0  # Set low correlations to 0 cor_sparse <- Matrix(cor_sparse, sparse = TRUE)  # Memory savings object.size(cor_mat) object.size(cor_sparse)"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"parallel-processing-strategies","dir":"Articles","previous_headings":"4. Performance Optimization","what":"4.3 Parallel Processing Strategies","title":"Advanced Topics","text":"multiple independent pruning operations: Note: corrselect doesn’t parallelize internally (reproducibility), can parallelize across multiple analyses.","code":"library(parallel)  # Create cluster cl <- makeCluster(detectCores() - 1)  # Export functions to cluster clusterEvalQ(cl, library(corrselect))  # Parallel pruning with different thresholds thresholds <- seq(0.5, 0.9, by = 0.1) results <- parLapply(cl, thresholds, function(thresh) {   corrPrune(my_data, threshold = thresh) })  # Clean up stopCluster(cl)"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"choosing-the-right-mode","dir":"Articles","previous_headings":"4. Performance Optimization","what":"4.4 Choosing the Right Mode","title":"Advanced Topics","text":"Decision tree mode selection:","code":"p ≤ 15:  Use \"exact\" (fast enough, guaranteed optimal) 15 < p ≤ 25:  Use \"exact\" if time permits, \"greedy\" if speed critical p > 25:  Use \"greedy\" or \"auto\" p > 100: Always use \"greedy\""},{"path":[]},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"error-no-valid-subsets-found","dir":"Articles","previous_headings":"5. Troubleshooting > 5.1 Common Errors and Solutions","what":"Error: “No valid subsets found”","title":"Advanced Topics","text":"Cause: Threshold strict - variables exceed Solutions: 1. Increase threshold 2. Use force_in keep least one variable 3. Check data near-duplicates","code":"# Example: All correlations > 0.9 set.seed(123) x <- rnorm(100) high_cor_data <- data.frame(   x1 = x,   x2 = x + rnorm(100, sd = 0.01),   x3 = x + rnorm(100, sd = 0.01) )  tryCatch({   corrPrune(high_cor_data, threshold = 0.5) }, error = function(e) {   cat(\"Error:\", e$message, \"\\n\") }) #> Error: No valid subsets found that satisfy the threshold constraint # Solution 1: Increase threshold result <- corrPrune(high_cor_data, threshold = 0.95) #> Error in corrPrune(high_cor_data, threshold = 0.95): No valid subsets found that satisfy the threshold constraint print(names(result)) #> Error: object 'result' not found  # Solution 2: Force keep one variable result <- corrPrune(high_cor_data, threshold = 0.5, force_in = \"x1\") #> Error in corrPrune(high_cor_data, threshold = 0.5, force_in = \"x1\"): No valid subsets found that satisfy the threshold constraint print(names(result)) #> Error: object 'result' not found"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"error-force_in-variables-conflict-with-threshold","dir":"Articles","previous_headings":"5. Troubleshooting > 5.1 Common Errors and Solutions","what":"Error: force_in variables conflict with threshold","title":"Advanced Topics","text":"Cause: Variables force_in |correlation| > threshold Solution: Either increase threshold reduce force_in set","code":"# x1 and x2 are highly correlated tryCatch({   corrPrune(high_cor_data, threshold = 0.5, force_in = c(\"x1\", \"x2\")) }, error = function(e) {   cat(\"Error:\", e$message, \"\\n\") }) #> Error: Variables in 'force_in' violate the threshold constraint. Example: 'x1' and 'x2' have association 1.000 > 0.500"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"error-vif-computation-fails-in-modelprune","dir":"Articles","previous_headings":"5. Troubleshooting > 5.1 Common Errors and Solutions","what":"Error: VIF computation fails in modelPrune()","title":"Advanced Topics","text":"Cause: Perfect multicollinearity (R² = 1) Solution: Use corrPrune() first remove perfect collinearity:","code":"# Create perfect multicollinearity perfect_data <- data.frame(   y = rnorm(100),   x1 = rnorm(100),   x2 = rnorm(100) ) perfect_data$x3 <- perfect_data$x1 + perfect_data$x2  # Perfect collinearity  tryCatch({   modelPrune(y ~ ., data = perfect_data, limit = 5) }, error = function(e) {   cat(\"Error:\", e$message, \"\\n\") }) #> Warning in summary.lm(fit): essentially perfect fit: summary may be unreliable #> Warning in summary.lm(fit): essentially perfect fit: summary may be unreliable #> Warning in summary.lm(fit): essentially perfect fit: summary may be unreliable #>               y          x1          x2 #> 1   -0.71524219 -0.07355602 -0.60189285 #> 2   -0.75268897 -1.16865142 -0.99369859 #> 3   -0.93853870 -0.63474826  1.02678506 #> 4   -1.05251328 -0.02884155  0.75106130 #> 5   -0.43715953  0.67069597 -1.50916654 #> 6    0.33117917 -1.65054654 -0.09514745 #> 7   -2.01421050 -0.34975424 -0.89594782 #> 8    0.21198043  0.75640644 -2.07075107 #> 9    1.23667505 -0.53880916  0.15012013 #> 10   2.03757402  0.22729192 -0.07921171 #> 11   1.30117599  0.49222857 -0.09736927 #> 12   0.75677476  0.26783502  0.21615254 #> 13  -1.72673040  0.65325768  0.88246516 #> 14  -0.60150671 -0.12270866  0.20559750 #> 15  -0.35204646 -0.41367651 -0.61643584 #> 16   0.70352390 -2.64314895 -0.73479925 #> 17  -0.10567133 -0.09294102 -0.13180279 #> 18  -1.25864863  0.43028470  0.31001699 #> 19   1.68443571  0.53539884 -1.03968035 #> 20   0.91139129 -0.55527835 -0.18430887 #> 21   0.23743027  1.77950291  0.96726726 #> 22   1.21810861  0.28642442 -0.10828009 #> 23  -1.33877429  0.12631586 -0.69842067 #> 24   0.66082030  1.27226678 -0.27594517 #> 25  -0.52291238 -0.71846622  1.11464855 #> 26   0.68374552 -0.45033862  0.55004396 #> 27  -0.06082195  2.39745248  1.23667580 #> 28   0.63296071  0.01112919  0.13909786 #> 29   1.33551762  1.63356842  0.41027510 #> 30   0.00729009 -1.43850664 -0.55845691 #> 31   1.01755864 -0.19051680  0.60537067 #> 32  -1.18843404  0.37842390 -0.50633354 #> 33  -0.72160444  0.30003855 -1.42056550 #> 34   1.51921771 -1.00563626  0.12799297 #> 35   0.37738797  0.01925927  1.94585122 #> 36  -2.05222282 -1.07742065  0.80091434 #> 37  -1.36403745  0.71270333  1.16525339 #> 38  -0.20078102  1.08477509  0.35885572 #> 39   0.86577940 -2.22498770 -0.60855718 #> 40  -0.10188326  1.23569346 -0.20224086 #> 41   0.62418747 -1.24104450 -0.27324811 #> 42   0.95900538  0.45476927 -0.46869978 #> 43   1.67105483  0.65990264  0.70416728 #> 44   0.05601673 -0.19988983 -1.19736350 #> 45  -0.05198191 -0.64511396  0.86636613 #> 46  -1.75323736  0.16532102  0.86415249 #> 47   0.09932759  0.43881870 -1.19862236 #> 48  -0.57185006  0.88330282  0.63949200 #> 49  -0.97400958 -2.05233698  2.43022665 #> 50  -0.17990623 -1.63637927 -0.55721548 #> 51   1.01494317  1.43040234  0.84490424 #> 52  -1.99274849  1.04662885 -0.78220185 #> 53  -0.42727929  0.43528895  1.11071142 #> 54   0.11663728  0.71517841  0.24982472 #> 55  -0.89320757  0.91717492  1.65191539 #> 56   0.33390294 -2.66092280 -1.45897073 #> 57   0.41142992  1.11027710 -0.05129789 #> 58  -0.03303616 -0.48498760 -0.52692518 #> 59  -2.46589819  0.23061683 -0.19726487 #> 60   2.57145815 -0.29515780 -0.62957874 #> 61  -0.20529926  0.87196495 -0.83384358 #> 62   0.65119328 -0.34847245  0.57872237 #> 63   0.27376649  0.51850377 -1.08758071 #> 64   1.02467323 -0.39068498  1.48403093 #> 65   0.81765945 -1.09278721 -1.18620659 #> 66  -0.20979317  1.21001051  0.10107915 #> 67   0.37816777  0.74090001  0.53298929 #> 68  -0.94540883  1.72426224  0.58673534 #> 69   0.85692301  0.06515393 -0.30174666 #> 70  -0.46103834  1.12500275  0.07950200 #> 71   2.41677335  1.97541905  0.96126415 #> 72  -1.65104890 -0.28148212 -1.45646592 #> 73  -0.46398724 -1.32295111 -0.78173971 #> 74   0.82537986 -0.23935157  0.32040231 #> 75   0.51013255 -0.21404124 -0.44478198 #> 76  -0.58948104  0.15168050  1.37000399 #> 77  -0.99678074  1.71230498  0.67325386 #> 78   0.14447570 -0.32614389  0.07216675 #> 79  -0.01430741  0.37300466 -1.50775732 #> 80  -1.79028124 -0.22768406  0.02610023 #> 81   0.03455107  0.02045071 -0.31641587 #> 82   0.19023032  0.31405766 -0.10234651 #> 83   0.17472640  1.32821470 -1.18155923 #> 84  -1.05501704  0.12131838  0.49865804 #> 85   0.47613328  0.71284232 -1.03895644 #> 86   1.37857014  0.77886003 -0.22622198 #> 87   0.45623640  0.91477327  0.38142583 #> 88  -1.13558847 -0.57439455 -0.78351579 #> 89  -0.43564547  1.62688121  0.58299141 #> 90   0.34610362 -0.38095674 -1.31651040 #> 91  -0.64704563 -0.10578417 -2.80977468 #> 92  -2.15764634  1.40405027  0.46496799 #> 93   0.88425082  1.29408391  0.84053983 #> 94  -0.82947761 -1.08999187 -0.28584542 #> 95  -0.57356027 -0.87307100  0.50412625 #> 96   1.50390061 -1.35807906 -1.15591653 #> 97  -0.77414493  0.18184719 -0.12714861 #> 98   0.84573154  0.16484087 -1.94151838 #> 99  -1.26068288  0.36411469  1.18118089 #> 100 -0.35454240  0.55215771  1.85991086 # Two-step approach step1 <- corrPrune(perfect_data[, -1], threshold = 0.99) step2_data <- data.frame(y = perfect_data$y, step1) result <- modelPrune(y ~ ., data = step2_data, limit = 5) #> Warning in summary.lm(fit): essentially perfect fit: summary may be unreliable #> Warning in summary.lm(fit): essentially perfect fit: summary may be unreliable #> Warning in summary.lm(fit): essentially perfect fit: summary may be unreliable print(attr(result, \"selected_vars\")) #> [1] \"x1\" \"x2\""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"for-corrprune-correlation-threshold","dir":"Articles","previous_headings":"5. Troubleshooting > 5.2 Threshold Selection Guidance","what":"For corrPrune() (Correlation Threshold)","title":"Advanced Topics","text":"Conservative (strict): threshold = 0.5: low redundancy, may lose information Use : Interpretability critical, small sample size Moderate (recommended): threshold = 0.7: Balances redundancy information retention Use : Standard regression, general analysis Lenient: threshold = 0.9: removes near-duplicates Use : Large sample size, prediction focus","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"for-modelprune-vif-limit","dir":"Articles","previous_headings":"5. Troubleshooting > 5.2 Threshold Selection Guidance","what":"For modelPrune() (VIF Limit)","title":"Advanced Topics","text":"Strict: limit = 2: low multicollinearity, may -prune Use : Small sample size, interpretability critical Moderate (recommended): limit = 5: Standard threshold literature Use : General regression analysis Lenient: limit = 10: Tolerates multicollinearity Use : Large sample size, prediction focus","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"empirical-approach-visualize-first","dir":"Articles","previous_headings":"5. Troubleshooting > 5.2 Threshold Selection Guidance","what":"Empirical Approach: Visualize First","title":"Advanced Topics","text":"Strategy: Choose threshold curve begins plateau.","code":"data(mtcars)  # Visualize correlation distribution cor_mat <- cor(mtcars) cor_vec <- cor_mat[upper.tri(cor_mat)]  par(mfrow = c(1, 2))  # Histogram of correlations hist(abs(cor_vec), breaks = 30,      main = \"Distribution of |Correlations|\",      xlab = \"|Correlation|\",      col = \"lightblue\") abline(v = c(0.5, 0.7, 0.9), col = c(\"red\", \"blue\", \"green\"), lwd = 2, lty = 2) legend(\"topright\",        legend = c(\"0.5 (strict)\", \"0.7 (moderate)\", \"0.9 (lenient)\"),        col = c(\"red\", \"blue\", \"green\"), lwd = 2, lty = 2,        bty = \"o\", bg = NA)  # Subset size vs threshold thresholds <- seq(0.3, 0.95, by = 0.05) sizes <- sapply(thresholds, function(t) {   tryCatch({     ncol(corrPrune(mtcars, threshold = t))   }, error = function(e) NA) })  plot(thresholds, sizes, type = \"b\",      xlab = \"Threshold\",      ylab = \"Number of Variables Retained\",      main = \"Threshold Sensitivity\",      col = \"blue\", lwd = 2) abline(h = ncol(mtcars), lty = 2, col = \"gray\") text(0.3, ncol(mtcars), \"Original\", pos = 3)"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"single-predictor-after-pruning","dir":"Articles","previous_headings":"5. Troubleshooting > 5.3 Handling Edge Cases","what":"Single Predictor After Pruning","title":"Advanced Topics","text":"","code":"# Very strict threshold may leave only 1 variable strict_result <- corrPrune(mtcars, threshold = 0.3) cat(\"Variables remaining:\", ncol(strict_result), \"\\n\") #> Variables remaining: 2  # Check if result is usable if (ncol(strict_result) < 2) {   cat(\"Warning: Only 1 variable remaining. Consider:\\n\")   cat(\"  1. Increasing threshold\\n\")   cat(\"  2. Using force_in to keep important variables\\n\") }"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"all-variables-removed","dir":"Articles","previous_headings":"5. Troubleshooting > 5.3 Handling Edge Cases","what":"All Variables Removed","title":"Advanced Topics","text":"","code":"# Impossible threshold tryCatch({   corrPrune(mtcars, threshold = 0.0) }, error = function(e) {   cat(\"Error:\", e$message, \"\\n\") }) #> Error: `threshold` must be in the range (0, 1]."},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"mixed-type-data","dir":"Articles","previous_headings":"5. Troubleshooting > 5.3 Handling Edge Cases","what":"Mixed-Type Data","title":"Advanced Topics","text":"","code":"# Create mixed data mixed_data <- mtcars mixed_data$cyl <- factor(mixed_data$cyl) mixed_data$am <- factor(mixed_data$am)  # Use assocSelect for mixed types result <- assocSelect(mixed_data, threshold = 0.6) show(result) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: mixed #>   AssocMethod: numeric_factor = eta, numeric_numeric = pearson, factor_numeric #>                = eta, factor_factor = cramersv #>   Threshold:   0.600 #>   Subsets:     20 valid combinations #>   Data Rows:   32 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] wt, vs, gear, carb                0.436  0.583     4 #>   [ 2] vs, am, carb                      0.265  0.570     3 #>   [ 3] wt, qsec, gear                    0.324  0.583     3 #>   [ 4] disp, carb, am                    0.348  0.591     3 #>   [ 5] drat, vs, carb                    0.367  0.570     3 #>   ... (15 more combinations)"},{"path":[]},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"for-exploratory-analysis","dir":"Articles","previous_headings":"6. Best Practices > 6.1 Workflow Recommendations","what":"For Exploratory Analysis","title":"Advanced Topics","text":"","code":"# 1. Visualize correlations corrplot::corrplot(cor(data), method = \"circle\")  # 2. Try multiple thresholds results <- lapply(c(0.5, 0.7, 0.9), function(t) {   corrPrune(data, threshold = t) })  # 3. Compare subset sizes sapply(results, ncol)  # 4. Choose based on your needs final_data <- results[[2]]  # threshold = 0.7"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"for-publication-quality-analysis","dir":"Articles","previous_headings":"6. Best Practices > 6.1 Workflow Recommendations","what":"For Publication-Quality Analysis","title":"Advanced Topics","text":"","code":"# 1. Use exact mode for reproducibility and optimality data_pruned <- corrPrune(data, threshold = 0.7, mode = \"exact\")  # 2. Document in methods section cat(sprintf(   \"Variables were pruned using corrselect::corrPrune() with threshold = 0.7, \",   \"exact mode, retaining %d of %d original predictors.\",   ncol(data_pruned), ncol(data) ))  # 3. Report which variables were removed removed <- attr(data_pruned, \"removed_vars\") cat(\"Removed variables:\", paste(removed, collapse = \", \"))"},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"combining-with-other-methods","dir":"Articles","previous_headings":"6. Best Practices","what":"6.2 Combining with Other Methods","title":"Advanced Topics","text":"","code":"# Comprehensive variable selection pipeline pipeline <- function(data, response) {   # Step 1: Remove correlations   step1 <- corrPrune(data, threshold = 0.7, mode = \"auto\")    # Step 2: VIF cleanup   step2_data <- data.frame(response = response, step1)   step2 <- modelPrune(response ~ ., data = step2_data, limit = 5)    # Step 3: Feature importance (optional)   if (requireNamespace(\"Boruta\", quietly = TRUE)) {     boruta_result <- Boruta::Boruta(response ~ ., data = step2)     important <- Boruta::getSelectedAttributes(boruta_result)     final_data <- step2[, c(\"response\", important)]   } else {     final_data <- step2   }    final_data }"},{"path":[]},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"algorithms","dir":"Articles","previous_headings":"7. Summary > Key Takeaways","what":"Algorithms","title":"Advanced Topics","text":"Use exact mode p ≤ 20 (optimal, reproducible) Use greedy mode p > 20 (fast, near-optimal) Use auto mode let corrselect decide","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"custom-engines","dir":"Articles","previous_headings":"7. Summary > Key Takeaways","what":"Custom Engines","title":"Advanced Topics","text":"Integrate modeling package (INLA, mgcv, brms) Define custom pruning criteria (AIC, BIC, p-values) Two required functions: fit diagnostics","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"optimization","dir":"Articles","previous_headings":"7. Summary > Key Takeaways","what":"Optimization","title":"Advanced Topics","text":"Precompute correlation matrices multiple thresholds Use greedy mode large p Parallelize across analyses, within","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"troubleshooting-1","dir":"Articles","previous_headings":"7. Summary > Key Takeaways","what":"Troubleshooting","title":"Advanced Topics","text":"Visualize correlation distribution choosing threshold Use force_in protect important variables Two-step pruning (corrPrune → modelPrune) robustness","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"references","dir":"Articles","previous_headings":"","what":"8. References","title":"Advanced Topics","text":"Algorithms: Eppstein, D., Löffler, M., & Strash, D. (2010). Listing maximal cliques sparse graphs near-optimal time. Symposium Algorithms Computation. Bron, C., & Kerbosch, J. (1973). Algorithm 457: Finding cliques undirected graph. Communications ACM, 16(9), 575-577. Multicollinearity: O’Brien, R. M. (2007). caution regarding rules thumb variance inflation factors. Quality & Quantity, 41(5), 673-690. Belsley, D. ., Kuh, E., & Welsch, R. E. (1980). Regression Diagnostics. Wiley. Software: INLA: Rue, H., Martino, S., & Chopin, N. (2009). Approximate Bayesian inference latent Gaussian models. Journal Royal Statistical Society: Series B, 71(2), 319-392. mgcv: Wood, S. N. (2017). Generalized Additive Models: Introduction R (2nd ed.). Chapman Hall/CRC.","code":""},{"path":"https://gillescolling.com/corrselect/articles/advanced.html","id":"see-also","dir":"Articles","previous_headings":"8. References","what":"See Also","title":"Advanced Topics","text":"vignette(\"quickstart\") - 5-minute introduction vignette(\"workflows\") - Real-world examples vignette(\"comparison\") - vs caret, Boruta, glmnet vignette(\"corrselect_vignette\") - Original exact methods vignette ?corrPrune - Association-based pruning ?modelPrune - Model-based pruning ?corrSelect - Exact subset enumeration","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Comparison with Alternatives","text":"vignette compares corrselect’s graph-theoretic approach four established methods multicollinearity management variable selection: caret::findCorrelation() - Greedy correlation-based pruning Boruta - Random forest permutation importance glmnet - L1/L2 regularization (LASSO/Ridge) Manual VIF removal - Iterative variance inflation factor thresholding comparison examines algorithmic differences, performance characteristics, appropriate use cases. Evaluations use bioclim_example dataset (19 bioclimatic variables, n=100n = 100). See vignette(\"theory\") mathematical foundations. See vignette(\"quickstart\") usage examples.","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"evaluation-dataset","dir":"Articles","previous_headings":"","what":"Evaluation Dataset","title":"Comparison with Alternatives","text":"Block structure present: correlations range -0.15 0.97.","code":"data(bioclim_example) predictors <- bioclim_example[, -1]  # Exclude response response <- bioclim_example[, 1]  cat(\"Variables:\", ncol(predictors), \"\\n\") #> Variables: 19 cat(\"Observations:\", nrow(predictors), \"\\n\") #> Observations: 100 cat(\"Response: species_richness (continuous)\\n\") #> Response: species_richness (continuous) cor_matrix <- cor(predictors)  # Correlation heatmap col_pal <- colorRampPalette(c(\"#3B4992\", \"white\", \"#EE0000\"))(100)  par(mar = c(1, 1, 3, 1)) nc <- ncol(cor_matrix) nr <- nrow(cor_matrix) image(seq_len(nc), seq_len(nr), t(cor_matrix[nr:1, ]),       col = col_pal,       xlab = \"\", ylab = \"\", axes = FALSE,       main = \"Bioclimatic Variable Correlations (p = 19)\",       zlim = c(-1, 1)) axis(1, at = seq_len(nc), labels = colnames(cor_matrix), las = 2, cex.axis = 0.7) axis(2, at = nc:1, labels = colnames(cor_matrix), las = 2, cex.axis = 0.7)  for (i in seq_len(nc)) {   for (j in seq_len(nr)) {     text_col <- if (abs(cor_matrix[j, i]) > 0.6) \"white\" else \"black\"     text(i, nr - j + 1, sprintf(\"%.2f\", cor_matrix[j, i]),          cex = 0.5, col = text_col)   } }"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"method","dir":"Articles","previous_headings":"Comparison 1: caret::findCorrelation()","what":"Method","title":"Comparison with Alternatives","text":"caret’s findCorrelation() applies greedy iterative removal: Identify pair maximum |rij||r_{ij}| Remove variable larger mean absolute correlation Repeat |rij|<τ|r_{ij}| < \\tau Non-deterministic: results depend internal ordering. Typically removes variables graph-theoretic methods.","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"execution","dir":"Articles","previous_headings":"Comparison 1: caret::findCorrelation()","what":"Execution","title":"Comparison with Alternatives","text":"corrselect retains variables (|Scorrselect|≥|Scaret||S_{\\text{corrselect}}| \\ge |S_{\\text{caret}}|) via maximal clique enumeration satisfying identical threshold constraint.","code":"if (requireNamespace(\"caret\", quietly = TRUE)) {   # Apply caret's greedy algorithm   to_remove_caret <- caret::findCorrelation(cor_matrix, cutoff = 0.7)   result_caret <- predictors[, -to_remove_caret]    cat(\"caret results:\\n\")   cat(\"  Variables retained:\", ncol(result_caret), \"\\n\")   cat(\"  Variables removed:\", length(to_remove_caret), \"\\n\")   cat(\"  Removed:\", paste(colnames(predictors)[to_remove_caret], collapse = \", \"), \"\\n\") } #> caret results: #>   Variables retained: 10  #>   Variables removed: 9  #>   Removed: BIO8, BIO7, BIO5, BIO4, BIO3, BIO9, BIO1, BIO11, BIO15 # Apply corrselect (exact mode) result_corrselect <- corrPrune(predictors, threshold = 0.7, mode = \"exact\")  cat(\"\\ncorrselect results:\\n\") #>  #> corrselect results: cat(\"  Variables retained:\", ncol(result_corrselect), \"\\n\") #>   Variables retained: 12 cat(\"  Variables removed:\", length(attr(result_corrselect, \"removed_vars\")), \"\\n\") #>   Variables removed: 7 cat(\"  Removed:\", paste(attr(result_corrselect, \"removed_vars\"), collapse = \", \"), \"\\n\") #>   Removed: BIO2, BIO4, BIO5, BIO7, BIO8, BIO10, BIO15"},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"distribution-comparison","dir":"Articles","previous_headings":"Comparison 1: caret::findCorrelation()","what":"Distribution Comparison","title":"Comparison with Alternatives","text":"","code":"# Extract correlations cor_orig <- cor(predictors) cor_corrselect <- cor(result_corrselect)  if (requireNamespace(\"caret\", quietly = TRUE)) {   cor_caret <- cor(result_caret)    # Overlaid histogram comparing all three   hist(abs(cor_orig[upper.tri(cor_orig)]),        breaks = 30,        main = \"Distribution of Absolute Correlations\",        xlab = \"Absolute Correlation\",        col = rgb(0.5, 0.5, 0.5, 0.4),        xlim = c(0, 1))    hist(abs(cor_caret[upper.tri(cor_caret)]),        breaks = 30,        col = rgb(0.8, 0.2, 0.2, 0.4),        add = TRUE)    hist(abs(cor_corrselect[upper.tri(cor_corrselect)]),        breaks = 30,        col = rgb(0.2, 0.5, 0.8, 0.4),        add = TRUE)    abline(v = 0.7, col = \"black\", lwd = 2, lty = 2)    legend(\"topright\",          legend = c(            paste0(\"Original (\", ncol(predictors), \" vars)\"),            paste0(\"caret (\", ncol(result_caret), \" vars)\"),            paste0(\"corrselect (\", ncol(result_corrselect), \" vars)\"),            \"Threshold\"          ),          fill   = c(            rgb(0.5, 0.5, 0.5, 0.4),            rgb(0.8, 0.2, 0.2, 0.4),            rgb(0.2, 0.5, 0.8, 0.4),            NA          ),          border = c(\"white\", \"white\", \"white\", NA),          lty    = c(NA, NA, NA, 2),          lwd    = c(NA, NA, NA, 2),          col    = c(NA, NA, NA, \"black\"),          bty    = \"o\",          bg = NA) }"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"applications","dir":"Articles","previous_headings":"Comparison 1: caret::findCorrelation()","what":"Applications","title":"Comparison with Alternatives","text":"caret: Exploratory analysis, non-critical reproducibility requirements. corrselect: Reproducible research, maximal variable retention, forced variable constraints, mixed data types.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"method-1","dir":"Articles","previous_headings":"Comparison 2: Boruta","what":"Method","title":"Comparison with Alternatives","text":"Boruta tests variable importance via random forest permutation: Create shadow features (permuted copies) Fit random forest original + shadow features Test: importance(Xi)>max⁡(importance(shadow))\\text{importance}(X_i) > \\max(\\text{importance}(\\text{shadow})) Iteratively confirm/reject convergence Orthogonal objective: Boruta selects predictive variables (supervised). corrselect removes redundant variables (unsupervised).","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"execution-1","dir":"Articles","previous_headings":"Comparison 2: Boruta","what":"Execution","title":"Comparison with Alternatives","text":"Different variable sets selected: Boruta optimizes prediction; corrselect minimizes redundancy.","code":"if (requireNamespace(\"Boruta\", quietly = TRUE)) {   # Boruta: \"Which variables predict species_richness?\"   set.seed(123)   boruta_result <- Boruta::Boruta(     species_richness ~ .,     data    = bioclim_example,     maxRuns = 100   )    cat(\"Boruta variable importance screening:\\n\")   print(table(boruta_result$finalDecision))    important_vars <- names(boruta_result$finalDecision[     boruta_result$finalDecision == \"Confirmed\"   ])    cat(\"\\n  Confirmed predictors:\", length(important_vars), \"\\n\")   cat(\" \", paste(important_vars, collapse = \", \"), \"\\n\") } #> Boruta variable importance screening: #>  #> Tentative Confirmed  Rejected  #>         0         6        13  #>  #>   Confirmed predictors: 6  #>   BIO12, BIO13, BIO14, BIO15, BIO16, BIO17 # corrselect: \"Which variables are redundant?\" corrselect_result <- corrPrune(predictors, threshold = 0.7)  cat(\"\\ncorrselect multicollinearity pruning:\\n\") #>  #> corrselect multicollinearity pruning: cat(\"  Non-redundant variables:\", ncol(corrselect_result), \"\\n\") #>   Non-redundant variables: 12 cat(\" \", paste(names(corrselect_result), collapse = \", \"), \"\\n\") #>   BIO1, BIO3, BIO6, BIO9, BIO11, BIO12, BIO13, BIO14, BIO16, BIO17, BIO18, BIO19"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"sequential-application","dir":"Articles","previous_headings":"Comparison 2: Boruta","what":"Sequential Application","title":"Comparison with Alternatives","text":"Stage 1 removes redundancy (reproducible). Stage 2 tests importance (stochastic). Stage 3 fits model non-redundant, predictive variables.","code":"# Stage 1: Correlation-based pruning data_pruned <- corrPrune(raw_data, threshold = 0.7)  # Stage 2: Importance testing boruta_result <- Boruta::Boruta(response ~ ., data = cbind(response, data_pruned)) final_vars <- names(boruta_result$finalDecision[   boruta_result$finalDecision == \"Confirmed\" ])  # Stage 3: Final model final_model <- lm(response ~ ., data = cbind(response, data_pruned)[, c(\"response\", final_vars)])"},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"applications-1","dir":"Articles","previous_headings":"Comparison 2: Boruta","what":"Applications","title":"Comparison with Alternatives","text":"Boruta: Prediction-focused analysis response variable. corrselect: Multicollinearity removal, exploratory analysis without response, reproducible selection. Sequential: High-dimensional correlated data requiring redundancy removal importance testing.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"method-2","dir":"Articles","previous_headings":"Comparison 3: glmnet (LASSO/Ridge)","what":"Method","title":"Comparison with Alternatives","text":"glmnet minimizes regularized loss: minβ12n∥y−Xβ∥22+λ[α∥β∥1+(1−α)∥β∥22] \\min_{\\beta} \\frac{1}{2n} \\|y - X\\beta\\|_2^2 + \\lambda \\left[\\alpha \\|\\beta\\|_1 + (1-\\alpha) \\|\\beta\\|_2^2\\right] α=1\\alpha = 1: LASSO (L1 penalty, sparse β\\beta) α=0\\alpha = 0: Ridge (L2 penalty, shrinkage) λ\\lambda: Cross-validation selected Difference: glmnet performs soft selection (shrinkage) optimizing prediction. corrselect performs hard selection (removal) based correlation structure.","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"execution-2","dir":"Articles","previous_headings":"Comparison 3: glmnet (LASSO/Ridge)","what":"Execution","title":"Comparison with Alternatives","text":"glmnet selects fewer variables (|Sglmnet|≤|Scorrselect||S_{\\text{glmnet}}| \\le |S_{\\text{corrselect}}|) optimizing prediction. corrselect maximizes retention correlation constraint.","code":"if (requireNamespace(\"glmnet\", quietly = TRUE)) {   # Fit LASSO with cross-validation   X <- as.matrix(predictors)   y <- response    set.seed(123)   cv_lasso <- glmnet::cv.glmnet(X, y, alpha = 1)    # Extract non-zero coefficients at lambda.1se (conservative choice)   coef_lasso <- stats::coef(cv_lasso, s = \"lambda.1se\")   selected_lasso <- rownames(coef_lasso)[coef_lasso[, 1] != 0][-1]  # Remove intercept    cat(\"glmnet (LASSO, λ = lambda.1se):\\n\")   cat(\"  Variables retained:\", length(selected_lasso), \"\\n\")   cat(\" \", paste(selected_lasso, collapse = \", \"), \"\\n\") } #> glmnet (LASSO, λ = lambda.1se): #>   Variables retained: 4  #>   BIO1, BIO12, BIO15, BIO16 if (requireNamespace(\"glmnet\", quietly = TRUE)) {   # Compare model performance   model_glmnet <- lm(species_richness ~ .,                      data = bioclim_example[, c(\"species_richness\", selected_lasso)])    model_corrselect <- lm(species_richness ~ .,                          data = cbind(species_richness = response, result_corrselect))    cat(\"\\nModel comparison (OLS on selected variables):\\n\")   cat(\"  glmnet:     R² =\", round(summary(model_glmnet)$r.squared, 3),       \"with\", length(selected_lasso), \"predictors\\n\")   cat(\"  corrselect: R² =\", round(summary(model_corrselect)$r.squared, 3),       \"with\", ncol(result_corrselect), \"predictors\\n\") } #>  #> Model comparison (OLS on selected variables): #>   glmnet:     R² = 0.983 with 4 predictors #>   corrselect: R² = 0.909 with 12 predictors"},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"coefficient-comparison","dir":"Articles","previous_headings":"Comparison 3: glmnet (LASSO/Ridge)","what":"Coefficient Comparison","title":"Comparison with Alternatives","text":"Left: L1 penalty shrinks coefficients toward zero (biased). Right: OLS pruned variables (unbiased). glmnet optimizes prediction shrinkage. corrselect preserves effect sizes.","code":"if (requireNamespace(\"glmnet\", quietly = TRUE)) {   par(mfrow = c(1, 2), mar = c(8, 4, 3, 2))    # glmnet coefficients (shrinkage)   coef_vals <- coef_lasso[coef_lasso[, 1] != 0, ][-1]   barplot(sort(abs(coef_vals), decreasing = TRUE),           las = 2,           main = \"glmnet: Shrunk Coefficients\",           ylab = \"Absolute Coefficient Value\",           col = \"salmon\",           cex.names = 0.7)    # corrselect: unbiased OLS coefficients   coef_corrselect <- coef(model_corrselect)[-1]  # Remove intercept   barplot(sort(abs(coef_corrselect), decreasing = TRUE),           las = 2,           main = \"corrselect: Unbiased OLS Coefficients\",           ylab = \"Absolute Coefficient Value\",           col = rgb(0.2, 0.5, 0.8, 0.7),           cex.names = 0.7) }"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"applications-2","dir":"Articles","previous_headings":"Comparison 3: glmnet (LASSO/Ridge)","what":"Applications","title":"Comparison with Alternatives","text":"glmnet: Prediction-focused, high-dimensional (p>np > n), accepts biased coefficients. corrselect: Interpretable coefficients, exploratory analysis, explicit correlation constraint, unregularized modeling. Sequential: Correlation pruning (corrselect) followed sparse prediction (glmnet).","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"method-3","dir":"Articles","previous_headings":"Comparison 4: modelPrune() vs Manual VIF Removal","what":"Method","title":"Comparison with Alternatives","text":"Variance Inflation Factor quantifies predictor multicollinearity: VIFj=11−Rj2 \\text{VIF}_j = \\frac{1}{1 - R^2_j} Rj2R^2_j results regressing XjX_j remaining predictors. Thresholds: VIF < 5: Low collinearity VIF < 10: Moderate (acceptable) VIF ≥ 10: High (problematic) Manual approach: iteratively remove max(VIF) VIF < threshold.","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"manual-implementation","dir":"Articles","previous_headings":"Comparison 4: modelPrune() vs Manual VIF Removal","what":"Manual Implementation","title":"Comparison with Alternatives","text":"","code":"# Manual iterative VIF removal manual_vif_removal <- function(formula, data, threshold = 5, max_iter = 10) {   require(car)    # Get response variable name   response_var <- all.vars(formula)[1]    # Get predictor names (handles ~ . notation)   model <- lm(formula, data = data)   current_vars <- names(coef(model))[-1]  # Exclude intercept    removed_vars <- character(0)   vif_vals <- car::vif(model)    while (max(vif_vals) > threshold && length(current_vars) > 1 && length(removed_vars) < max_iter) {     # Remove variable with highest VIF     var_to_remove <- names(which.max(vif_vals))     removed_vars <- c(removed_vars, var_to_remove)     cat(\"Iteration\", length(removed_vars), \": Removing\", var_to_remove,         \"(VIF =\", round(max(vif_vals), 2), \")\\n\")      # Update variable list and refit     current_vars <- setdiff(current_vars, var_to_remove)     new_formula <- as.formula(paste(response_var, \"~\", paste(current_vars, collapse = \" + \")))     model <- lm(new_formula, data = data)     vif_vals <- car::vif(model)   }    list(model = model, iterations = length(removed_vars),        vif = vif_vals, removed = removed_vars, converged = max(vif_vals) <= threshold) }  # Run manual VIF removal if (requireNamespace(\"car\", quietly = TRUE)) {   cat(\"Manual VIF removal (iterative):\\n\")   manual_result <- manual_vif_removal(species_richness ~ ., data = bioclim_example, threshold = 5)   cat(\"\\nVariables kept:\", length(manual_result$vif), \"\\n\")   if (!manual_result$converged) {     cat(\"(Stopped at max_iter = 10; VIF threshold not yet reached)\\n\")   } } #> Manual VIF removal (iterative): #> Loading required package: car #> Loading required package: carData #> Iteration 1 : Removing BIO2 (VIF = 5.83 ) #> Iteration 2 : Removing BIO7 (VIF = 5.66 ) #> Iteration 3 : Removing BIO5 (VIF = 5.03 ) #>  #> Variables kept: 16"},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"modelprune-comparison","dir":"Articles","previous_headings":"","what":"modelPrune() Comparison","title":"Comparison with Alternatives","text":"","code":"# Run modelPrune modelprune_result <- modelPrune(species_richness ~ ., data = bioclim_example, limit = 5)  cat(\"\\nmodelPrune results:\\n\") #>  #> modelPrune results: cat(\"Variables removed:\", attr(modelprune_result, \"removed_vars\"), \"\\n\") #> Variables removed: BIO2 BIO7 BIO5 cat(\"Variables kept:\", length(attr(modelprune_result, \"selected_vars\")), \"\\n\") #> Variables kept: 16  # Extract final model final_model <- attr(modelprune_result, \"final_model\") if (requireNamespace(\"car\", quietly = TRUE)) {   cat(\"\\nFinal VIF values:\\n\")   print(round(car::vif(final_model), 2)) } #>  #> Final VIF values: #>  BIO1  BIO3  BIO4  BIO6  BIO8  BIO9 BIO10 BIO11 BIO12 BIO13 BIO14 BIO15 BIO16  #>  2.09  3.68  3.81  2.57  4.03  4.27  4.96  3.06  1.76  2.51  3.11  3.01  2.43  #> BIO17 BIO18 BIO19  #>  2.88  2.82  1.70"},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"visual-vif-comparison","dir":"Articles","previous_headings":"","what":"Visual: VIF Comparison","title":"Comparison with Alternatives","text":"","code":"if (requireNamespace(\"car\", quietly = TRUE)) {   # Compute VIF for original model   model_full <- lm(species_richness ~ ., data = bioclim_example)   vif_before <- car::vif(model_full)    # VIF after modelPrune   vif_after <- car::vif(final_model)    # Combined barplot   par(mar = c(8, 4, 4, 2))   all_vars <- unique(c(names(vif_before), names(vif_after)))   vif_combined <- data.frame(     before = vif_before[match(all_vars, names(vif_before))],     after = vif_after[match(all_vars, names(vif_after))]   )   vif_combined[is.na(vif_combined)] <- 0   vif_combined <- vif_combined[order(vif_combined$before, decreasing = TRUE), ]    # Show top 15   n_show <- min(15, nrow(vif_combined))   barplot(t(as.matrix(vif_combined[1:n_show, ])),           beside = TRUE,           las = 2,           main = \"VIF Before and After modelPrune()\",           ylab = \"VIF\",           col = c(rgb(0.8, 0.2, 0.2, 0.7), rgb(0.2, 0.5, 0.8, 0.7)),           cex.names = 0.6,           names.arg = rownames(vif_combined)[1:n_show])   abline(h = 5, col = \"black\", lwd = 2, lty = 2)   legend(\"topright\",          legend = c(\"Before\", \"After\", \"Limit = 5\"),          fill   = c(rgb(0.8, 0.2, 0.2, 0.7), rgb(0.2, 0.5, 0.8, 0.7), NA),          border = c(\"white\", \"white\", NA),          lty    = c(NA, NA, 2),          lwd    = c(NA, NA, 2),          col    = c(NA, NA, \"black\"),          bty    = \"o\",          bg = NA) }"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"applications-3","dir":"Articles","previous_headings":"Visual: VIF Comparison","what":"Applications","title":"Comparison with Alternatives","text":"Manual VIF: Educational use, diagnostic understanding, legacy workflows. modelPrune(): Production pipelines, forced variable constraints, reproducible documentation.","code":""},{"path":[]},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"corrselect-distinguishing-features","dir":"Articles","previous_headings":"Summary","what":"corrselect Distinguishing Features","title":"Comparison with Alternatives","text":"Maximal clique enumeration: Optimal retention |rij|<τ|r_{ij}| < \\tau constraint Deterministic: ELS Bron-Kerbosch algorithms guarantee reproducibility Flexible: Forced variables, mixed types, greedy/exact modes Unbiased estimates: Hard removal preserves coefficient interpretability Model-agnostic: Correlation-based preprocessing","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"integrated-workflow","dir":"Articles","previous_headings":"Summary","what":"Integrated Workflow","title":"Comparison with Alternatives","text":"","code":"# Correlation pruning data_pruned <- corrPrune(raw_data, threshold = 0.7)  # VIF refinement model_data <- modelPrune(response ~ ., data = data_pruned, limit = 5)  # Importance testing (optional) if (requireNamespace(\"Boruta\", quietly = TRUE)) {   boruta_result <- Boruta::Boruta(response ~ ., data = model_data)   important_vars <- names(boruta_result$finalDecision[     boruta_result$finalDecision == \"Confirmed\"   ]) }  # Final model: OLS (interpretable) or glmnet (prediction) final_model <- lm(response ~ ., data = model_data[, c(\"response\", important_vars)])"},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Comparison with Alternatives","text":"caret: Kuhn, M. (2008). Building predictive models R using caret package. Journal Statistical Software, 28(5), 1-26. doi:10.18637/jss.v028.i05 Boruta: Kursa, M. B., & Rudnicki, W. R. (2010). Feature selection Boruta package. Journal Statistical Software, 36(11), 1-13. doi:10.18637/jss.v036.i11 glmnet: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1-22. doi:10.18637/jss.v033.i01 VIF: Belsley, D. ., Kuh, E., & Welsch, R. E. (1980). Regression Diagnostics: Identifying Influential Data Sources Collinearity. Wiley. doi:10.1002/0471725153","code":""},{"path":"https://gillescolling.com/corrselect/articles/comparison.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Comparison with Alternatives","text":"vignette(\"quickstart\") - Interface overview usage examples vignette(\"workflows\") - Domain-specific workflows (genomics, ecology, surveys) vignette(\"advanced\") - Algorithm selection performance tuning vignette(\"theory\") - Graph-theoretic foundations formal proofs","code":""},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Correlation Subset Selection with corrselect","text":"corrselect identifies maximal subsets variables whose pairwise correlations stay chosen threshold. process reduces multicollinearity redundancy modeling, preserving interpretability. Unlike greedy stepwise approaches, corrselect exhaustively searches valid subsets using fast, exact algorithms. fully model-agnostic, making suitable preprocessing step regression, clustering, feature selection, analyses. Given threshold t∈(0,1)t \\(0,1), functions corrSelect() (data-frame interface) MatSelect() (matrix interface) enumerate maximal subsets SS variables satisfying: ∀,j∈S,≠j:|rij|<t \\forall , j \\S,\\ \\neq j: \\ |r_{ij}| < t rijr_{ij} denotes chosen correlation measure variables ii jj. Enumeration relies two exact graph-theoretic algorithms: Eppstein–Löffler–Strash (ELS), degeneracy-ordered backtracking algorithm optimized sparse graphs. Bron–Kerbosch (BK), classical recursive clique-finding method, optional pivoting reduce search space. Results returned CorrCombo S4 object containing subset’s variable names summary statistics (avg_corr, min_corr, max_corr). can extract subsets original data via corrSubset(). procedure depend downstream model, cleanly separates “feature curation” “model fitting” supports multiple correlation measures (pearson, spearman, kendall, bicor, distance, maximal).","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"simulated-numeric-example","dir":"Articles","previous_headings":"Quick Start (CorrSelect)","what":"Simulated numeric example","title":"Correlation Subset Selection with corrselect","text":"","code":"set.seed(42) n <- 100 df <- data.frame(   A = rnorm(n),   B = rnorm(n),   C = rnorm(n),   D = rnorm(n),   E = rnorm(n) ) df$F <- df$A * 0.9 + rnorm(n, sd = 0.1)  # strongly correlated with A"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"basic-selection","dir":"Articles","previous_headings":"Quick Start (CorrSelect)","what":"Basic selection","title":"Correlation Subset Selection with corrselect","text":"","code":"res <- corrSelect(df, threshold = 0.7) res #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: pearson #>   Threshold:   0.700 #>   Subsets:     2 valid combinations #>   Data Rows:   100 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] F, B, C, D, E                     0.082  0.185     5 #>   [ 2] A, B, C, D, E                     0.083  0.185     5 as.data.frame(res) #>                      VarName01 VarName02 VarName03 VarName04 VarName05 #> Subset01 [avg=0.082]         F         B         C         D         E #> Subset02 [avg=0.083]         A         B         C         D         E corrSubset(res, df, which = 1)[1:10,] #>              F          B          C            D           E #> 1   1.33677667  1.2009654 -2.0009292 -0.004620768  1.33491259 #> 2  -0.41675087  1.0447511  0.3337772  0.760242168 -0.86927176 #> 3   0.32656994 -1.0032086  1.1713251  0.038990913  0.05548695 #> 4   0.58317730  1.8484819  2.0595392  0.735072142  0.04906691 #> 5   0.29182614 -0.6667734 -1.3768616 -0.146472627 -0.57835573 #> 6  -0.11532450  0.1055138 -1.1508556 -0.057887335 -0.99873866 #> 7   1.25744892 -0.4222559 -0.7058214  0.482369466 -0.00243278 #> 8  -0.18188872 -0.1223502 -1.0540558  0.992943637  0.65551188 #> 9   1.69450003  0.1881930 -0.6457437 -1.246395498  1.47684228 #> 10  0.02717808  0.1191610 -0.1853780 -0.033487525 -1.90915279"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"forcing-variables-into-all-subsets","dir":"Articles","previous_headings":"Quick Start (CorrSelect)","what":"Forcing variables into all subsets","title":"Correlation Subset Selection with corrselect","text":"","code":"res2 <- corrSelect(df, threshold = 0.7, force_in = \"A\") res2 #> CorrCombo object #> ----------------- #>   Method:      els #>   Correlation: pearson #>   Threshold:   0.700 #>   Subsets:     1 valid combinations #>   Data Rows:   100 used in correlation #>   Forced-in:   A #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] A, B, C, D, E                     0.083  0.185     5"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"using-a-different-correlation-method","dir":"Articles","previous_headings":"Quick Start (CorrSelect)","what":"Using a different correlation method","title":"Correlation Subset Selection with corrselect","text":"","code":"res3 <- corrSelect(df, threshold = 0.6, cor_method = \"spearman\") res3 #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: spearman #>   Threshold:   0.600 #>   Subsets:     2 valid combinations #>   Data Rows:   100 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] F, B, C, D, E                     0.088  0.191     5 #>   [ 2] A, B, C, D, E                     0.090  0.206     5"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"matrix-interface-matselect","dir":"Articles","previous_headings":"","what":"Matrix Interface (MatSelect)","title":"Correlation Subset Selection with corrselect","text":"already computed correlation matrix want apply method precomputed correlations: Selecting subsets: Force variable 1 every subset:","code":"mat <- cor(df) res4 <- MatSelect(mat, threshold = 0.7) res4 #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Threshold:   0.700 #>   Subsets:     2 valid combinations #>   Data Rows:   6 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] F, B, C, D, E                     0.082  0.185     5 #>   [ 2] A, B, C, D, E                     0.083  0.185     5 MatSelect(mat, threshold = 0.5) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Threshold:   0.500 #>   Subsets:     2 valid combinations #>   Data Rows:   6 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] F, B, C, D, E                     0.082  0.185     5 #>   [ 2] A, B, C, D, E                     0.083  0.185     5 MatSelect(mat, threshold = 0.5, force_in = 1) #> CorrCombo object #> ----------------- #>   Method:      els #>   Threshold:   0.500 #>   Subsets:     1 valid combinations #>   Data Rows:   6 used in correlation #>   Forced-in:   A #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] A, B, C, D, E                     0.083  0.185     5"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"mixed-data-types-assocselect","dir":"Articles","previous_headings":"","what":"Mixed Data Types (assocSelect)","title":"Correlation Subset Selection with corrselect","text":"","code":"df_ass <- data.frame(   height = rnorm(15, 170, 10),   weight = rnorm(15, 70, 12),   group  = factor(rep(LETTERS[1:3], each = 5)),   score  = ordered(sample(c(\"low\",\"med\",\"high\"), 15, TRUE)) )  # keep every subset whose internal associations ≤ 0.6 res5 <- assocSelect(df_ass, threshold = 0.6) res5 #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: mixed #>   AssocMethod: numeric_numeric = pearson, numeric_factor = eta, numeric_ordered #>                = spearman, factor_ordered = cramersv #>   Threshold:   0.600 #>   Subsets:     1 valid combinations #>   Data Rows:   15 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] height, weight, group, score      0.267  0.554     4"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"changing-correlation-method","dir":"Articles","previous_headings":"","what":"Changing Correlation Method","title":"Correlation Subset Selection with corrselect","text":"default, corrSelect() uses Pearson correlation. can choose alternatives cor_method argument: \"pearson\": linear correlation (default) \"spearman\": rank-based monotonic association \"kendall\": Kendall’s tau \"bicor\": robust biweight midcorrelation (WGCNA::bicor) \"distance\": distance correlation (energy::dcor) \"maximal\": maximal information coefficient (minerva::mine) Example:","code":"res6 <- corrSelect(df, threshold = 0.7, cor_method = \"spearman\") res6 #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: spearman #>   Threshold:   0.700 #>   Subsets:     2 valid combinations #>   Data Rows:   100 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] F, B, C, D, E                     0.088  0.191     5 #>   [ 2] A, B, C, D, E                     0.090  0.206     5"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"handling-mixed-data-types","dir":"Articles","previous_headings":"","what":"Handling Mixed Data Types","title":"Correlation Subset Selection with corrselect","text":"function assocSelect() extends corrSelect() support mixed data types — including numeric, factor, ordered variables — using appropriate association measures variable pair. Instead single correlation matrix, constructs generalized association matrix using following logic: defaults numeric-numeric, numeric-ordered, ordered-ordered associations can changed via arguments: combinations use fixed methods (eta cramersv) appropriate measuring association strength.","code":"assocSelect(df_ass,   method_num_num = \"kendall\",   method_num_ord = \"spearman\",   method_ord_ord = \"kendall\" ) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: mixed #>   AssocMethod: numeric_numeric = kendall, numeric_factor = eta, numeric_ordered #>                = spearman, factor_ordered = cramersv #>   Threshold:   0.700 #>   Subsets:     1 valid combinations #>   Data Rows:   15 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] height, weight, group, score      0.270  0.554     4"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"example-with-mixed-types","dir":"Articles","previous_headings":"Handling Mixed Data Types","what":"Example with Mixed Types","title":"Correlation Subset Selection with corrselect","text":"pairwise association bounded [0,1] treated analogously correlation.","code":"df_ass <- data.frame(   height = rnorm(10),   weight = rnorm(10),   group  = factor(sample(c(\"A\", \"B\"), 10, replace = TRUE)),   score  = ordered(sample(1:3, 10, replace = TRUE)) )  res7 <- assocSelect(df_ass, threshold = 1, method = \"bron-kerbosch\", use_pivot = TRUE) res7 #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: mixed #>   AssocMethod: numeric_numeric = pearson, numeric_factor = eta, numeric_ordered #>                = spearman, factor_ordered = cramersv #>   Threshold:   1.000 #>   Subsets:     1 valid combinations #>   Data Rows:   10 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] height, weight, group, score      0.336  0.495     4"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"theory","dir":"Articles","previous_headings":"","what":"Theory","title":"Correlation Subset Selection with corrselect","text":"Given symmetric correlation matrix R∈ℝp×pR \\\\mathbb{R}^{p \\times p}, seek maximal subsets S⊆{1,…,p}S \\subseteq \\{1, \\dots, p\\} : ∀,j∈S,≠j:|Rij|<t \\forall , j \\S,\\ \\neq j: \\ |R_{ij}| < t fixed threshold t∈(0,1)t \\(0, 1). equivalent finding maximal cliques thresholded correlation graph, : Nodes represent variables Edges connect nodes whose absolute correlation threshold maximal clique corresponds variable subset extended without violating correlation limit.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"els-eppsteinlöfflerstrash","dir":"Articles","previous_headings":"Algorithms","what":"ELS (Eppstein–Löffler–Strash)","title":"Correlation Subset Selection with corrselect","text":"ELS algorithm efficiently enumerates maximal cliques sparse graph using degeneracy ordering: Compute degeneracy ordering v1,…,vpv_1, \\dots, v_p. ii, extend current clique SS {vi}\\{v_i\\} within candidate set C={vi+1,…,vp}C = \\{v_{+1}, \\dots, v_p\\}. Recursively build cliques, pruning vertices can added. Formally, define: extend(S,C)={S,C=∅,⋃v∈Cextend(S∪{v},C\\(N(v)∪{v})),otherwise. \\text{extend}(S, C) = \\begin{cases} S, & C = \\emptyset, \\\\ \\bigcup_{v \\C} \\text{extend}(S \\cup \\{v\\},\\ C \\setminus (N(v) \\cup \\{v\\})), & \\text{otherwise}. \\end{cases} ELS avoids redundant exploration, achieving good performance typical correlation graphs.","code":""},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"bronkerbosch-with-pivoting","dir":"Articles","previous_headings":"Algorithms","what":"Bron–Kerbosch (with Pivoting)","title":"Correlation Subset Selection with corrselect","text":"classical Bron–Kerbosch algorithm enumerates maximal cliques via recursive backtracking optional pivoting: Let RR = current clique, PP = prospective nodes, XX = excluded nodes. : BK(R,P,X)={report(R),P=X=∅,v∈P\\N(u):BK(R∪{v},P∩N(v),X∩N(v)),P←P\\{v},X←X∪{v}. \\text{BK}(R, P, X) = \\begin{cases} \\text{report}(R), & P = X = \\emptyset, \\\\ \\text{} v \\P \\setminus N(u): \\\\ \\quad \\text{BK}(R \\cup \\{v\\},\\ P \\cap N(v),\\ X \\cap N(v)), \\ \\quad P \\leftarrow P \\setminus \\{v\\},\\ X \\leftarrow X \\cup \\{v\\}. \\end{cases} Choosing pivot u∈P∪Xu \\P \\cup X iterating P\\N(u)P \\setminus N(u) reduces recursive calls.","code":""},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"why-corrselect","dir":"Articles","previous_headings":"","what":"Why corrselect?","title":"Correlation Subset Selection with corrselect","text":"existing R tools: Filter one variable time (e.g. findCorrelation) Use greedy backward-selection heuristics enumerate valid subsets corrselect uniquely provides: Exact enumeration maximal subsets Support multiple correlation measures Optional forcing variables Full inspection via CorrCombo objects Fast C++ implementations via Rcpp makes ideal pipelines interpretability completeness essential.","code":""},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"inspecting-results","dir":"Articles","previous_headings":"","what":"Inspecting Results","title":"Correlation Subset Selection with corrselect","text":"Convert results downstream use: Extract individual subsets: Summarize correlation metrics:","code":"df_res <- as.data.frame(res) head(df_res) #>                      VarName01 VarName02 VarName03 VarName04 VarName05 #> Subset01 [avg=0.082]         F         B         C         D         E #> Subset02 [avg=0.083]         A         B         C         D         E lapply(corrSubset(res, df, which = 1:2), function(x) head(x, 10)) #> $Subset1 #>              F          B          C            D           E #> 1   1.33677667  1.2009654 -2.0009292 -0.004620768  1.33491259 #> 2  -0.41675087  1.0447511  0.3337772  0.760242168 -0.86927176 #> 3   0.32656994 -1.0032086  1.1713251  0.038990913  0.05548695 #> 4   0.58317730  1.8484819  2.0595392  0.735072142  0.04906691 #> 5   0.29182614 -0.6667734 -1.3768616 -0.146472627 -0.57835573 #> 6  -0.11532450  0.1055138 -1.1508556 -0.057887335 -0.99873866 #> 7   1.25744892 -0.4222559 -0.7058214  0.482369466 -0.00243278 #> 8  -0.18188872 -0.1223502 -1.0540558  0.992943637  0.65551188 #> 9   1.69450003  0.1881930 -0.6457437 -1.246395498  1.47684228 #> 10  0.02717808  0.1191610 -0.1853780 -0.033487525 -1.90915279 #>  #> $Subset2 #>              A          B          C            D           E #> 1   1.37095845  1.2009654 -2.0009292 -0.004620768  1.33491259 #> 2  -0.56469817  1.0447511  0.3337772  0.760242168 -0.86927176 #> 3   0.36312841 -1.0032086  1.1713251  0.038990913  0.05548695 #> 4   0.63286260  1.8484819  2.0595392  0.735072142  0.04906691 #> 5   0.40426832 -0.6667734 -1.3768616 -0.146472627 -0.57835573 #> 6  -0.10612452  0.1055138 -1.1508556 -0.057887335 -0.99873866 #> 7   1.51152200 -0.4222559 -0.7058214  0.482369466 -0.00243278 #> 8  -0.09465904 -0.1223502 -1.0540558  0.992943637  0.65551188 #> 9   2.01842371  0.1881930 -0.6457437 -1.246395498  1.47684228 #> 10 -0.06271410  0.1191610 -0.1853780 -0.033487525 -1.90915279 # Number and size of subsets length(res@subset_list) #> [1] 2 summary(lengths(res@subset_list)) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       5       5       5       5       5       5  # Summaries of within-subset correlations summary(res@max_corr) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   0.185   0.185   0.185   0.185   0.185   0.185 summary(res@avg_corr) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #> 0.08162 0.08185 0.08208 0.08208 0.08232 0.08255"},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"corrcombo-object-structure","dir":"Articles","previous_headings":"","what":"CorrCombo Object Structure","title":"Correlation Subset Selection with corrselect","text":"CorrCombo S4 object contains: subset_list: list character vectors (variable names) avg_corr, min_corr, max_corr: numeric vectors correlation metrics threshold, forced_in, search_type, cor_method, n_rows_used Attribute use_pivot (applicable) Inspect slots:","code":"str(res@subset_list) #> List of 2 #>  $ : chr [1:5] \"F\" \"B\" \"C\" \"D\" ... #>  $ : chr [1:5] \"A\" \"B\" \"C\" \"D\" ..."},{"path":"https://gillescolling.com/corrselect/articles/corrselect_vignette.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Correlation Subset Selection with corrselect","text":"","code":"sessionInfo() #> R version 4.5.1 (2025-06-13 ucrt) #> Platform: x86_64-w64-mingw32/x64 #> Running under: Windows 11 x64 (build 26200) #>  #> Matrix products: default #>   LAPACK version 3.12.1 #>  #> locale: #> [1] LC_COLLATE=English_United States.utf8  #> [2] LC_CTYPE=English_United States.utf8    #> [3] LC_MONETARY=English_United States.utf8 #> [4] LC_NUMERIC=C                           #> [5] LC_TIME=English_United States.utf8     #>  #> time zone: Europe/Luxembourg #> tzcode source: internal #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] corrselect_3.0.0 #>  #> loaded via a namespace (and not attached): #>  [1] digest_0.6.37     desc_1.4.3        R6_2.6.1          fastmap_1.2.0     #>  [5] xfun_0.53         cachem_1.1.0      knitr_1.50        htmltools_0.5.8.1 #>  [9] rmarkdown_2.30    lifecycle_1.0.4   cli_3.6.5         sass_0.4.10       #> [13] pkgdown_2.1.3     textshaping_1.0.3 jquerylib_0.1.4   systemfonts_1.2.3 #> [17] compiler_4.5.1    tools_4.5.1       ragg_1.5.0        evaluate_1.0.5    #> [21] bslib_0.9.0       Rcpp_1.1.0        yaml_2.3.10       jsonlite_2.0.0    #> [25] rlang_1.1.6       fs_1.6.6          htmlwidgets_1.6.4"},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"what-corrselect-does","dir":"Articles","previous_headings":"","what":"What corrselect Does","title":"Quick Start","text":"corrselect identifies removes redundant variables based pairwise correlation association. Given threshold τ\\tau, finds subsets pairwise associations satisfy |aij|<τ|a_{ij}| < \\tau (see vignette(\"theory\") mathematical formulation).","code":""},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"interface-hierarchy","dir":"Articles","previous_headings":"","what":"Interface Hierarchy","title":"Quick Start","text":"corrselect provides three levels interface:","code":""},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"level-1-simple-pruning","dir":"Articles","previous_headings":"Interface Hierarchy","what":"Level 1: Simple Pruning","title":"Quick Start","text":"corrPrune() - Removes redundant predictors based pairwise correlation: Returns single pruned dataset response variable required Fast greedy exact search modelPrune() - Reduces VIF regression models: Returns single pruned dataset response Iteratively removes high-VIF predictors Works lm, glm, lme4, glmmTMB","code":""},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"level-2-structured-subset-selection","dir":"Articles","previous_headings":"Interface Hierarchy","what":"Level 2: Structured Subset Selection","title":"Quick Start","text":"corrSelect() - Returns maximal subsets (numeric data): Enumerates maximal valid subsets satisfying threshold (see vignette(\"theory\")) Provides full metadata (size, avg_corr, max_corr, min_corr) Exact greedy search assocSelect() - Returns maximal subsets (mixed-type data): Handles numeric, factor, ordered variables Uses appropriate association measures per variable pair Exact greedy search","code":""},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"level-3-low-level-matrix-interface","dir":"Articles","previous_headings":"Interface Hierarchy","what":"Level 3: Low-Level Matrix Interface","title":"Quick Start","text":"MatSelect() - Direct matrix input: Accepts precomputed correlation/association matrices data preprocessing Useful repeated analyses","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"corrprune-association-based-pruning","dir":"Articles","previous_headings":"Quick Examples","what":"corrPrune(): Association-Based Pruning","title":"Quick Start","text":"Variables removed: corrPrune() selects among multiple maximal subsets: multiple maximal subsets exist (common), corrPrune() returns subset lowest average absolute correlation. selection criterion balances three goals: Minimize redundancy: Lower average correlation means independent variables Maximize information: Prefers diverse variable combinations tightly clustered ones Deterministic behavior: Always returns result data explore maximal subsets instead just optimal one, use corrSelect() (see ).","code":"library(corrselect) data(mtcars)  # Remove correlated predictors (threshold = 0.7) pruned <- corrPrune(mtcars, threshold = 0.7)  # Results cat(sprintf(\"Reduced from %d to %d variables\\n\", ncol(mtcars), ncol(pruned))) #> Reduced from 11 to 5 variables names(pruned) #> [1] \"mpg\"  \"drat\" \"qsec\" \"gear\" \"carb\" attr(pruned, \"removed_vars\") #> [1] \"cyl\"  \"disp\" \"hp\"   \"wt\"   \"vs\"   \"am\""},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"modelprune-vif-based-pruning","dir":"Articles","previous_headings":"Quick Examples","what":"modelPrune(): VIF-Based Pruning","title":"Quick Start","text":"","code":"# Prune based on VIF (limit = 5) model_data <- modelPrune(   formula = mpg ~ .,   data = mtcars,   limit = 5 )  # Results cat(\"Variables kept:\", paste(attr(model_data, \"selected_vars\"), collapse = \", \"), \"\\n\") #> Variables kept: drat, qsec, vs, am, gear, carb cat(\"Variables removed:\", paste(attr(model_data, \"removed_vars\"), collapse = \", \"), \"\\n\") #> Variables removed: disp, cyl, wt, hp"},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"corrselect-enumerate-all-maximal-subsets","dir":"Articles","previous_headings":"Quick Examples","what":"corrSelect(): Enumerate All Maximal Subsets","title":"Quick Start","text":"Inspect subsets: Extract specific subset:","code":"results <- corrSelect(mtcars, threshold = 0.7) show(results) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: pearson #>   Threshold:   0.700 #>   Subsets:     15 valid combinations #>   Data Rows:   32 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] mpg, drat, qsec, gear, carb       0.416  0.700     5 #>   [ 2] cyl, drat, qsec, gear, carb       0.434  0.700     5 #>   [ 3] mpg, drat, vs, gear, carb         0.466  0.700     5 #>   [ 4] wt, qsec, am, carb                0.373  0.692     4 #>   [ 5] wt, qsec, gear, carb              0.388  0.656     4 #>   ... (10 more combinations) as.data.frame(results)[1:5, ]  # First 5 subsets #>                      VarName01 VarName02 VarName03 VarName04 VarName05 #> Subset01 [avg=0.416]       mpg      drat      qsec      gear      carb #> Subset02 [avg=0.434]       cyl      drat      qsec      gear      carb #> Subset03 [avg=0.466]       mpg      drat        vs      gear      carb #> Subset04 [avg=0.373]        wt      qsec        am      carb      <NA> #> Subset05 [avg=0.388]        wt      qsec      gear      carb      <NA> subset_data <- corrSubset(results, mtcars, which = 1) names(subset_data) #> [1] \"mpg\"  \"drat\" \"qsec\" \"gear\" \"carb\""},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"assocselect-mixed-type-data","dir":"Articles","previous_headings":"Quick Examples","what":"assocSelect(): Mixed-Type Data","title":"Quick Start","text":"","code":"# Create mixed-type data df <- data.frame(   x1 = rnorm(100),   x2 = rnorm(100),   cat1 = factor(sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE)),   ord1 = ordered(sample(1:5, 100, replace = TRUE)) )  # Handle mixed types automatically results_mixed <- assocSelect(df, threshold = 0.5) show(results_mixed) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: mixed #>   AssocMethod: numeric_numeric = pearson, numeric_factor = eta, numeric_ordered #>                = spearman, factor_ordered = cramersv #>   Threshold:   0.500 #>   Subsets:     1 valid combinations #>   Data Rows:   100 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] x1, x2, cat1, ord1                0.077  0.184     4"},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"protecting-variables","dir":"Articles","previous_headings":"","what":"Protecting Variables","title":"Quick Start","text":"Use force_in ensure specific variables always retained:","code":"# Force \"mpg\" to remain in all subsets pruned_force <- corrPrune(   data = mtcars,   threshold = 0.7,   force_in = \"mpg\" )  # Verify forced variable is present \"mpg\" %in% names(pruned_force) #> [1] TRUE"},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"threshold-selection","dir":"Articles","previous_headings":"","what":"Threshold Selection","title":"Quick Start","text":"Common threshold guidelines: detailed threshold selection strategies including visualization techniques sensitivity analysis, see vignette(\"advanced\").","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/quickstart.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Quick Start","text":"vignette(\"workflows\") - Complete real-world workflows (ecological, survey, genomic, mixed models) vignette(\"advanced\") - Algorithmic control custom engines vignette(\"comparison\") - Comparison caret, Boruta, glmnet vignette(\"theory\") - Theoretical foundations formulation ?corrPrune, ?modelPrune, ?corrSelect, ?assocSelect, ?MatSelect","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Theory and Formulation","text":"vignette presents mathematical formulation graph-theoretic foundations underlying corrselect. Variable subset selection correlation constraints formulated maximal independent set problem threshold graphs, enabling exact enumeration via established algorithms computational graph theory. vignette defines formal problem statement, explains graph-theoretic representation, details three implemented algorithms (Bron-Kerbosch, Eppstein-Löffler-Strash, greedy heuristic), analyzes computational complexity, provides comprehensive references theoretical literature. practical usage examples, see vignette(\"quickstart\") vignette(\"workflows\"). algorithmic control performance tuning, see vignette(\"advanced\").","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"terminology","dir":"Articles","previous_headings":"","what":"Terminology","title":"Theory and Formulation","text":"section defines core terms used throughout documentation. vignettes refer back definitions.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"association-measure","dir":"Articles","previous_headings":"Terminology","what":"Association measure","title":"Theory and Formulation","text":"symmetric functiona:𝒳×𝒳→ℝa: \\mathcal{X} \\times \\mathcal{X} \\\\mathbb{R} quantifies relationship two variables. Common cases: Numeric–numeric: Pearson’s ρ\\rho, Spearman’s ρs\\rho_s, Kendall’s τK\\tau_K Categorical–categorical: Cramér’s V Numeric–factor: eta-squared η2\\eta^2 measures used package normalized |aij|∈[0,1]|a_{ij}| \\[0,1].","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"association-matrix","dir":"Articles","previous_headings":"Terminology","what":"Association matrix","title":"Theory and Formulation","text":"symmetric p×pp \\times p matrix AA whose entry aija_{ij} association variables ii jj. diagonal satisfies aii=1a_{ii} = 1. correlation-based analysis, AA typically comes cor().","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"threshold-tau","dir":"Articles","previous_headings":"Terminology","what":"Threshold (τ\\tau)","title":"Theory and Formulation","text":"user-defined cutoff (0,1)(0,1). Pairs |aij|≥τ|a_{ij}| \\ge \\tau considered strongly associated appear valid subset. Common choices: τ=0.7\\tau = 0.7: modelling τ=0.8\\tau = 0.8: genomics τ=0.5\\tau = 0.5: stringent pruning","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"valid-subset","dir":"Articles","previous_headings":"Terminology","what":"Valid subset","title":"Theory and Formulation","text":"subset S⊆{1,…,p}S \\subseteq \\{1,\\dots,p\\} satisfying|aij|<τ|a_{ij}| < \\tau distinct ,j∈Si, j \\S. pairwise associations within SS remain threshold.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"maximal-valid-subset","dir":"Articles","previous_headings":"Terminology","what":"Maximal valid subset","title":"Theory and Formulation","text":"valid subset enlarged. Formally, variable v∉Sv \\notin S satisfies |avi|<τ|a_{vi}| < \\tau ∈Si \\S. (“Maximal” “maximum”, refers largest possible subset.)","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"threshold-graph","dir":"Articles","previous_headings":"Terminology","what":"Threshold graph","title":"Theory and Formulation","text":"undirected graph G=(V,E)G = (V, E) : vertex VV represents variable edge (,j)(,j) exists exactly |aij|<τ|a_{ij}| < \\tau Edges therefore connect compatible (low-association) variables.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"clique","dir":"Articles","previous_headings":"Terminology","what":"Clique","title":"Theory and Formulation","text":"subset vertices every pair connected edge. threshold graph, cliques correspond valid subsets.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"maximal-clique","dir":"Articles","previous_headings":"Terminology","what":"Maximal clique","title":"Theory and Formulation","text":"clique extended adding additional vertex. Maximal cliques correspond exactly maximal valid subsets.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"forced-in-variables-force_in","dir":"Articles","previous_headings":"Terminology","what":"Forced-in variables (force_in)","title":"Theory and Formulation","text":"set F⊆VF \\subseteq V variables must appear returned solutions. maximal cliques containing elements FF considered.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"els-eppsteinlöfflerstrash","dir":"Articles","previous_headings":"Terminology","what":"ELS (Eppstein–Löffler–Strash)","title":"Theory and Formulation","text":"degeneracy-based algorithm maximal clique enumeration. Recommended force_in used. Complexity: O(d⋅3d/3)O(d \\cdot 3^{d/3}), dd graph’s degeneracy.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"bronkerbosch","dir":"Articles","previous_headings":"Terminology","what":"Bron–Kerbosch","title":"Theory and Formulation","text":"classical backtracking algorithm enumerating maximal cliques, optionally pivoting. Used default force_in specified. Worst-case complexity: O(3p/3)O(3^{p/3}).","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"greedy-mode","dir":"Articles","previous_headings":"Terminology","what":"Greedy mode","title":"Theory and Formulation","text":"fast heuristic constructs single maximal clique via greedy selection. Runs O(p2)O(p^2). guarantee largest possible subset.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"exact-mode","dir":"Articles","previous_headings":"Terminology","what":"Exact mode","title":"Theory and Formulation","text":"Enumerates maximal cliques using ELS Bron–Kerbosch. Identifies maximum (largest) valid subset.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"auto-mode","dir":"Articles","previous_headings":"Terminology","what":"Auto mode","title":"Theory and Formulation","text":"Chooses method automatically: exact mode p≤20p \\le 20 greedy mode p>20p > 20 balances optimality computational cost.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"intuitive-overview","dir":"Articles","previous_headings":"","what":"Intuitive Overview","title":"Theory and Formulation","text":"diving formal definitions, let’s build intuition simple conceptual overview.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"the-core-idea","dir":"Articles","previous_headings":"Intuitive Overview","what":"The Core Idea","title":"Theory and Formulation","text":"Imagine dataset many predictors, highly correlated. example: Temperature noon temperature 2pm (likely correlated ~0.9) Monthly income annual income (perfectly correlated) Survey items “satisfied” “feel happy” (correlated ~0.7) building statistical models, including highly correlated predictors creates problems: Coefficient instability: Small data changes cause large coefficient swings Inflated variance: Standard errors become unreliable Interpretability issues: Hard isolate individual predictor effects solution: remove redundant predictors keeping many variables possible.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"how-corrselect-works","dir":"Articles","previous_headings":"Intuitive Overview","what":"How corrselect Works","title":"Theory and Formulation","text":"corrselect transforms statistical problem graph problem: Represent variables nodes graph Draw edges compatible variables (correlation threshold τ) Find maximal groups nodes connected (maximal cliques) maximal clique represents valid subset: group variables every pair correlation τ.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"why-maximal-not-maximum","dir":"Articles","previous_headings":"Intuitive Overview","what":"Why “Maximal” Not “Maximum”?","title":"Theory and Formulation","text":"maximal subset extended adding variables—’s locally complete. maximum subset single largest possible subset—globally optimal. corrselect finds maximal subsets : Real datasets often multiple equally good solutions may prefer smaller subset containing specific variables Comparing alternatives reveals correlation structure Exact enumeration feasible typical problem sizes","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"toy-example-4-variables","dir":"Articles","previous_headings":"Intuitive Overview","what":"Toy Example (4 Variables)","title":"Theory and Formulation","text":"Consider 4 variables correlation matrix: Observations: V1 V2 highly correlated (0.85) - likely redundant V3 V4 moderately correlated (0.75) -group correlations low (0.10-0.18) Set threshold τ = 0.7. pairs violate threshold? V1-V2: |0.85| ≥ 0.7 ✗ (high) V3-V4: |0.75| ≥ 0.7 ✗ (high) pairs: < 0.7 ✓ (acceptable)","code":"library(corrselect)  # Construct a simple 4x4 correlation matrix cor_4var <- matrix(c(   1.00, 0.85, 0.10, 0.15,   0.85, 1.00, 0.12, 0.18,   0.10, 0.12, 1.00, 0.75,   0.15, 0.18, 0.75, 1.00 ), nrow = 4, byrow = TRUE)  colnames(cor_4var) <- rownames(cor_4var) <- paste0(\"V\", 1:4)  # Display matrix print(cor_4var) #>      V1   V2   V3   V4 #> V1 1.00 0.85 0.10 0.15 #> V2 0.85 1.00 0.12 0.18 #> V3 0.10 0.12 1.00 0.75 #> V4 0.15 0.18 0.75 1.00"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"graph-representation","dir":"Articles","previous_headings":"Intuitive Overview","what":"Graph Representation","title":"Theory and Formulation","text":"Now build threshold graph edges connect compatible variables (correlation < 0.7). Text representation: Let’s verify code: Interpretation: edge exists Vi Vj can coexist valid subset. V1 connects : V3, V4 (V2) V2 connects : V3, V4 (V1) V3 connects : V1, V2 (V4) V4 connects : V1, V2 (V3)","code":"Variables: V1, V2, V3, V4  Edges (|correlation| < 0.7):   V1 —— V3  (cor = 0.10)   V1 —— V4  (cor = 0.15)   V2 —— V3  (cor = 0.12)   V2 —— V4  (cor = 0.18)  Missing edges (|correlation| ≥ 0.7):   V1 ⨯ V2  (cor = 0.85, too high)   V3 ⨯ V4  (cor = 0.75, too high)  Maximal cliques (maximal variable subsets):   {V1, V3}: Both connected, cannot add V2 or V4   {V1, V4}: Both connected, cannot add V2 or V3   {V2, V3}: Both connected, cannot add V1 or V4   {V2, V4}: Both connected, cannot add V1 or V3 # Adjacency matrix for threshold graph (edges where |cor| < 0.7) adj_matrix <- abs(cor_4var) < 0.7 diag(adj_matrix) <- FALSE  # No self-loops  # Visualize as adjacency matrix cat(\"Threshold graph edges (1 = edge exists):\\n\") #> Threshold graph edges (1 = edge exists): print(adj_matrix * 1) #>    V1 V2 V3 V4 #> V1  0  0  1  1 #> V2  0  0  1  1 #> V3  1  1  0  0 #> V4  1  1  0  0"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"visual-graph-representation","dir":"Articles","previous_headings":"Intuitive Overview","what":"Visual Graph Representation","title":"Theory and Formulation","text":"Let’s visualize threshold graph nodes edges:  Graph interpretation: Nodes: variable vertex Edges (blue lines): Connect variables correlation < 0.7 (compatible pairs) V1-V2: edge (corr = 0.85) V3-V4: edge (corr = 0.75) Maximal cliques (groups everyone connects everyone): {V1, V3}: V1—V3 edge exists ✓, add V2 (V1—V2 edge) V4 (V3—V4 edge) {V1, V4}: V1—V4 edge exists ✓, add V2 (V1—V2 edge) V3 (V3—V4 edge) {V2, V3}: V2—V3 edge exists ✓, add V1 (V1—V2 edge) V4 (V3—V4 edge) {V2, V4}: V2—V4 edge exists ✓, add V1 (V1—V2 edge) V3 (V3—V4 edge) visual representation makes graph-theoretic formulation concrete: finding maximal valid variable subsets equivalent finding maximal cliques threshold graph.","code":"# Node positions (arranged in a square for clarity) node_pos <- matrix(c(   0, 1,    # V1 (top-left)   2, 1,    # V2 (top-right)   0, 0,    # V3 (bottom-left)   2, 0     # V4 (bottom-right) ), ncol = 2, byrow = TRUE)  # Plot setup par(mar = c(2, 2, 3, 2)) plot(node_pos, type = \"n\", xlim = c(-0.5, 2.5), ylim = c(-0.5, 1.5),      xlab = \"\", ylab = \"\", axes = FALSE,      main = \"Threshold Graph (τ = 0.7)\\nEdges connect variables with |correlation| < 0.7\")  # Draw edges (where correlation < 0.7) edge_color <- rgb(0.2, 0.5, 0.8, 0.6) edge_lwd <- 2  for (i in 1:4) {   for (j in 1:4) {     if (i < j && adj_matrix[i, j]) {       segments(node_pos[i, 1], node_pos[i, 2],                node_pos[j, 1], node_pos[j, 2],                col = edge_color, lwd = edge_lwd)     }   } }  # Draw nodes node_size <- 0.15 for (i in 1:4) {   # Node circle   symbols(node_pos[i, 1], node_pos[i, 2],           circles = node_size, add = TRUE,           inches = FALSE, bg = NA, fg = \"black\", lwd = 2)    # Node label   text(node_pos[i, 1], node_pos[i, 2],        labels = paste0(\"V\", i), cex = 1.2, font = 2) }  # Add correlation annotations text(1, 1.35, \"cor = 0.85\\n(too high, no edge)\", cex = 0.8, col = \"red\") text(1, -0.35, \"cor = 0.75\\n(too high, no edge)\", cex = 0.8, col = \"red\")  # Legend showing maximal cliques legend(\"right\",        legend = c(\"Maximal cliques:\", \"{V1, V3}\", \"{V1, V4}\", \"{V2, V3}\", \"{V2, V4}\"),        bty = \"o\", bg = NA, cex = 0.9,        pch = c(NA, 19, 19, 19, 19),        col = c(NA, \"black\", \"black\", \"black\", \"black\"))  # Add box around graph box()"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"network-visualization-with-cor_example","dir":"Articles","previous_headings":"Intuitive Overview","what":"Network Visualization with cor_example","title":"Theory and Formulation","text":"larger matrices, can use igraph visualize threshold graph structure. Let’s demonstrate cor_example, 20 variables known block structure:  Interpretation: Variables within high-correlation blocks (red, orange) edges pairwise correlations exceed 0.7 Variables within low-correlation blocks (light blue, dark blue) many edges correlations 0.7 Maximal cliques tend include variables multiple low-correlation blocks explains corrselect tends select variables Block 3 Block 4: compatible neighbors","code":"data(cor_example)  # Build threshold graph (edges where |correlation| < 0.7) threshold <- 0.7 adj_mat <- abs(cor_example) < threshold diag(adj_mat) <- FALSE  if (requireNamespace(\"igraph\", quietly = TRUE)) {   library(igraph)    # Create graph from adjacency matrix   g <- graph_from_adjacency_matrix(adj_mat, mode = \"undirected\")    # Find maximal cliques   cliques <- max_cliques(g)   cat(sprintf(\"Found %d maximal cliques at threshold %.1f\\n\", length(cliques), threshold))    # Color nodes by which block they belong to   block_colors <- c(rep(\"#d73027\", 5),   # Block 1 (V1-V5): high correlation                     rep(\"#fc8d59\", 5),   # Block 2 (V6-V10): moderate                     rep(\"#91bfdb\", 5),   # Block 3 (V11-V15): low                     rep(\"#4575b4\", 5))   # Block 4 (V16-V20): minimal    # Plot network   par(mar = c(1, 1, 3, 1))   plot(g,        vertex.size = 10,        vertex.color = block_colors,        vertex.label.cex = 0.8,        vertex.label.color = \"black\",        edge.color = rgb(0.5, 0.5, 0.5, 0.3),        edge.width = 1,        layout = layout_with_fr(g),        main = sprintf(\"Threshold Graph (τ = %.1f): Variables with |cor| < %.1f are connected\",                      threshold, threshold))    # Add legend   legend(\"topleft\",          legend = c(\"Block 1 (V1-V5): High cor\",                    \"Block 2 (V6-V10): Moderate cor\",                    \"Block 3 (V11-V15): Low cor\",                    \"Block 4 (V16-V20): Minimal cor\"),          fill = c(\"#d73027\", \"#fc8d59\", \"#91bfdb\", \"#4575b4\"),          bty = \"o\", bg = NA, cex = 0.8) } else {   cat(\"Install igraph for network visualization: install.packages('igraph')\\n\")   cat(\"Adjacency matrix (first 5×5 block):\\n\")   print(adj_mat[1:5, 1:5] * 1) } #>  #> Attaching package: 'igraph' #> The following objects are masked from 'package:stats': #>  #>     decompose, spectrum #> The following object is masked from 'package:base': #>  #>     union #> Found 5 maximal cliques at threshold 0.7"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"finding-maximal-cliques","dir":"Articles","previous_headings":"Intuitive Overview","what":"Finding Maximal Cliques","title":"Theory and Formulation","text":"clique group every pair connected. graph: Potential cliques: {V1, V3}: connect ✓ {V1, V4}: connect ✓ {V2, V3}: connect ✓ {V2, V4}: connect ✓ {V1, V3, V4}: V3 connect V4? ✗ {V2, V3, V4}: V3 connect V4? ✗ Maximal cliques: Can 2-variable clique extended? {V1, V3} Add V2? (V1–V2 connected). Add V4? (V3–V4 connected).Maximal ✓ {V1, V4} Add V2? (V1–V2 connected). Add V3? (V3–V4 connected).Maximal ✓ {V2, V3} Add V1? (V1–V2 connected). Add V4? (V3–V4 connected).Maximal ✓ {V2, V4} Add V1? (V1–V2 connected). Add V3? (V3–V4 connected).Maximal ✓ 4 maximal cliques size 2, representing valid variable subset. Let corrselect confirm : Result interpretation: 4 maximal subsets found (predicted) size 2 (extended ) satisfies |correlation| < 0.7 pairs Mean/max correlations low (well threshold)","code":"results <- MatSelect(cor_4var, threshold = 0.7, method = \"bron-kerbosch\") show(results) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Threshold:   0.700 #>   Subsets:     4 valid combinations #>   Data Rows:   4 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] V1, V3                            0.100  0.100     2 #>   [ 2] V2, V3                            0.120  0.120     2 #>   [ 3] V1, V4                            0.150  0.150     2 #>   [ 4] V2, V4                            0.180  0.180     2"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"key-insight","dir":"Articles","previous_headings":"Intuitive Overview","what":"Key Insight","title":"Theory and Formulation","text":"toy example shows corrselect enumerates solutions: ’s single “best” subset—4 equally valid Choice depends domain knowledge (variables theoretically important?) Seeing options reveals correlation structure (two clusters: {V1,V2} {V3,V4}) Real datasets similar structure variables complex clustering.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"intuitive-problem-statement","dir":"Articles","previous_headings":"Problem Formulation","what":"Intuitive Problem Statement","title":"Theory and Formulation","text":"core problem straightforward: given set pp variables known pairwise associations (correlations), identify largest possible subsets every pair variables association user-defined threshold τ\\tau. Think social network variables people edges represent “get along well” (low correlation). want find maximal groups people everyone gets along everyone else group. group “maximal” add people without introducing conflict. variables may designated “must include” (forced-). social network analogy, VIPs must every group, search groups containing VIPs.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"formal-problem-statement","dir":"Articles","previous_headings":"Problem Formulation","what":"Formal Problem Statement","title":"Theory and Formulation","text":"Input: Association matrix ∈ℝp×pA \\\\mathbb{R}^{p \\times p} aij=ajia_{ij} = a_{ji} aii=1a_{ii} = 1 ii Threshold τ∈(0,1)\\tau \\(0, 1) Optional forced-set F⊆{1,…,p}F \\subseteq \\{1, \\dots, p\\} Constraints: subset S⊆{1,…,p}S \\subseteq \\{1, \\dots, p\\} valid : F⊆SF \\subseteq S (FF specified) |aij|<τ|a_{ij}| < \\tau ,j∈Si, j \\S ≠ji \\neq j Objective: Find maximal valid subsets 𝒮={S1,…,Sm}\\mathcal{S} = \\{S_1, \\dots, S_m\\} SkS_k maximal variable v∉Skv \\notin S_k satisfies |avi|<τ|a_{vi}| < \\tau ∈Ski \\S_k. Output: Collection 𝒮\\mathcal{S} containing maximal valid subsets.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"association-matrix-1","dir":"Articles","previous_headings":"Problem Formulation","what":"Association Matrix","title":"Theory and Formulation","text":"Given pp variables, compute association matrix ∈ℝp×pA \\\\mathbb{R}^{p \\times p} : aij=association(Xi,Xj) a_{ij} = \\text{association}(X_i, X_j) numeric variables, aija_{ij} may correlation coefficient (Pearson, Spearman, Kendall, etc.). mixed-type variables, aija_{ij} chosen based variable types: measures bounded: aij∈[0,1]a_{ij} \\[0, 1] aij∈[−1,1]a_{ij} \\[-1, 1].","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"threshold-constraint","dir":"Articles","previous_headings":"Problem Formulation","what":"Threshold Constraint","title":"Theory and Formulation","text":"Fix threshold τ∈(0,1)\\tau \\(0, 1). subset S⊆{1,…,p}S \\subseteq \\{1, \\dots, p\\} valid : ∀,j∈S,≠j:|aij|<τ \\forall , j \\S,\\ \\neq j: \\quad |a_{ij}| < \\tau","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"maximal-valid-subsets","dir":"Articles","previous_headings":"Problem Formulation","what":"Maximal Valid Subsets","title":"Theory and Formulation","text":"valid subset SS maximal variable k∉Sk \\notin S satisfies: |aki|<τfor ∈S |a_{ki}| < \\tau \\quad \\text{} \\S","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"why-graphs","dir":"Articles","previous_headings":"Graph-Theoretic Interpretation","what":"Why Graphs?","title":"Theory and Formulation","text":"variable selection problem natural graph representation connects decades research computational graph theory. viewing variables nodes “compatible pairs” (low correlation) edges, transform statistical problem well-studied graph problem proven algorithms. representation powerful : Efficiency: Graph algorithms exploit structural properties (sparsity, degeneracy) faster computation Exactness: can enumerate solutions, just find one Forced variables: Graph algorithms naturally handle constraints (forced-sets) key insight: group mutually compatible variables (valid subset) exactly clique graph edges represent compatibility.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"threshold-graph-1","dir":"Articles","previous_headings":"Graph-Theoretic Interpretation","what":"Threshold Graph","title":"Theory and Formulation","text":"Define threshold graph G=(V,E)G = (V, E) : V={1,…,p}V = \\{1, \\dots, p\\} (nodes represent variables) (,j)∈E(, j) \\E |aij|<τ|a_{ij}| < \\tau (edges connect compatible variables) Note reversal: edge (,j)(, j) means variables ii jj low correlation (can coexist). complement typical “correlation graph” edges represent high correlation.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"maximal-cliques","dir":"Articles","previous_headings":"Graph-Theoretic Interpretation","what":"Maximal Cliques","title":"Theory and Formulation","text":"valid subset SS corresponds clique GG: pairs SS connected. maximal valid subset corresponds maximal clique: clique extended. Finding maximal valid subsets equivalent enumerating maximal cliques GG.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"example-6-variable-threshold-graph","dir":"Articles","previous_headings":"Graph-Theoretic Interpretation","what":"Example: 6-Variable Threshold Graph","title":"Theory and Formulation","text":"Consider 6 variables correlation matrix: Threshold graph construction τ=0.7\\tau = 0.7:  Interpreting visualization: left panel shows correlation matrix clear block structure. Variables V1-V3 highly correlated (correlations 0.75-0.85, shown red), V4-V6 (correlations 0.55-0.65). -block correlations low (0.10-0.30, shown blue/white). right panel shows corresponding threshold graph τ=0.7\\tau = 0.7. edge connects two variables absolute correlation 0.7 (compatible variables). Note : Within high-correlation groups: V1-V3 edges connecting (correlations exceed 0.7). Similarly, V4-V6 fully connected. groups: V1-V3 V4-V6 pairs edges (low -group correlation). graph structure immediately reveals two maximal cliques: {V1,V2,V3}\\{V1, V2, V3\\} {V4,V5,V6}\\{V4, V5, V6\\}. variable can added either clique without violating threshold constraint. Identify maximal cliques: Interpreting results: MatSelect identified two maximal subsets size 3, exactly matching visual graph analysis. subsets satisfy |aij|<0.7|a_{ij}| < 0.7 pairs within subset. Subset 1 (V1, V2, V3): Mean correlation 0.80, max 0.85 - highly redundant within-group Subset 2 (V4, V5, V6): Mean correlation 0.60, max 0.65 - moderately correlated within-group Neither subset can extended: adding variable group introduce pair correlation 0.7. symmetric example, subsets equal size equally valid solutions. practice, might choose based domain knowledge (prefer variables established theory) downstream model performance.","code":"# Create example correlation matrix set.seed(123) cor_6var <- matrix(c(   1.00, 0.85, 0.75, 0.20, 0.15, 0.10,   0.85, 1.00, 0.80, 0.25, 0.20, 0.15,   0.75, 0.80, 1.00, 0.30, 0.25, 0.20,   0.20, 0.25, 0.30, 1.00, 0.65, 0.55,   0.15, 0.20, 0.25, 0.65, 1.00, 0.60,   0.10, 0.15, 0.20, 0.55, 0.60, 1.00 ), nrow = 6, byrow = TRUE)  rownames(cor_6var) <- colnames(cor_6var) <- paste0(\"V\", 1:6)  # Display correlation matrix print(round(cor_6var, 2)) #>      V1   V2   V3   V4   V5   V6 #> V1 1.00 0.85 0.75 0.20 0.15 0.10 #> V2 0.85 1.00 0.80 0.25 0.20 0.15 #> V3 0.75 0.80 1.00 0.30 0.25 0.20 #> V4 0.20 0.25 0.30 1.00 0.65 0.55 #> V5 0.15 0.20 0.25 0.65 1.00 0.60 #> V6 0.10 0.15 0.20 0.55 0.60 1.00 library(corrselect)  # Build adjacency matrix for threshold graph tau <- 0.7 adj_matrix <- abs(cor_6var) < tau diag(adj_matrix) <- FALSE  # Visualize correlation structure and threshold graph par(mfrow = c(1, 2))  # Panel 1: Correlation heatmap col_pal <- colorRampPalette(c(\"#3B4992\", \"white\", \"#EE0000\"))(100) image(1:6, 1:6, t(cor_6var[6:1, ]),       col = col_pal,       xlab = \"\", ylab = \"\",       main = \"Correlation Matrix\",       axes = FALSE,       zlim = c(-1, 1)) axis(1, at = 1:6, labels = colnames(cor_6var), cex.axis = 0.8) axis(2, at = 6:1, labels = colnames(cor_6var), cex.axis = 0.8)  # Add correlation values for (i in 1:6) {   for (j in 1:6) {     col_text <- if (abs(cor_6var[j, i]) > 0.5) \"white\" else \"black\"     text(i, 7 - j, sprintf(\"%.2f\", cor_6var[j, i]),          cex = 0.7, col = col_text)   } } abline(h = 3.5, lwd = 2, lty = 2, col = \"black\") abline(v = 3.5, lwd = 2, lty = 2, col = \"black\")  # Panel 2: Threshold graph (edges where |cor| < tau) plot.new() plot.window(xlim = c(0, 1), ylim = c(0, 1)) title(main = sprintf(\"Threshold Graph (τ = %.1f)\", tau))  # Node positions (2 groups based on correlation structure) pos <- matrix(c(   0.2, 0.8,  # V1   0.3, 0.6,  # V2   0.1, 0.4,  # V3   0.7, 0.8,  # V4   0.8, 0.5,  # V5   0.9, 0.3   # V6 ), ncol = 2, byrow = TRUE)  # Draw edges (compatible variable pairs) for (i in 1:5) {   for (j in (i + 1):6) {     if (adj_matrix[i, j]) {       lines(c(pos[i, 1], pos[j, 1]), c(pos[i, 2], pos[j, 2]),             col = \"gray70\", lwd = 1.5)     }   } }  # Draw nodes points(pos[, 1], pos[, 2], pch = 21, cex = 4,        bg = c(rep(rgb(0.8, 0.2, 0.2, 0.7), 3),               rep(rgb(0.2, 0.5, 0.8, 0.7), 3)),        col = \"black\", lwd = 2)  # Add labels text(pos[, 1], pos[, 2], labels = colnames(cor_6var),      col = \"white\", font = 2, cex = 0.8)  # Add legend legend(\"bottomleft\",        legend = c(\"Clique 1 (V1-V3)\", \"Clique 2 (V4-V6)\", \"Edge: |cor| < 0.7\"),        pch = c(21, 21, NA),        pt.bg = c(rgb(0.8, 0.2, 0.2, 0.7), rgb(0.2, 0.5, 0.8, 0.7), NA),        pt.cex = 2,        lty = c(NA, NA, 1),        col = c(\"black\", \"black\", \"gray70\"),        lwd = c(2, 2, 1.5),        bty = \"o\",        bg = NA,        cex = 0.7) par(mfrow = c(1, 1)) # Run MatSelect to find all maximal subsets results <- MatSelect(cor_6var, threshold = 0.7, method = \"els\") show(results) #> CorrCombo object #> ----------------- #>   Method:      els #>   Threshold:   0.700 #>   Subsets:     3 valid combinations #>   Data Rows:   6 used in correlation #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] V1, V4, V5, V6                    0.375  0.650     4 #>   [ 2] V2, V4, V5, V6                    0.400  0.650     4 #>   [ 3] V3, V4, V5, V6                    0.425  0.650     4"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"from-theory-to-implementation","dir":"Articles","previous_headings":"","what":"From Theory to Implementation","title":"Theory and Formulation","text":"mathematical concepts defined earlier map directly onto function arguments behavior package. correspondence outlined .","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"threshold-tau-threshold-argument","dir":"Articles","previous_headings":"From Theory to Implementation","what":"Threshold (τ\\tau) → threshold argument","title":"Theory and Formulation","text":"Controls edges appear threshold graph. corrPrune(data, threshold = 0.7) keeps edge |aij|<0.7|a_{ij}| < 0.7 Lower thresholds → stricter pruning → sparser graphs → smaller valid subsets","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"maximal-cliques-returned-subsets","dir":"Articles","previous_headings":"From Theory to Implementation","what":"Maximal cliques → Returned subsets","title":"Theory and Formulation","text":"corrSelect() returns maximal cliques (full exact enumeration) corrPrune() returns one maximal clique (greedy exact, depending mode) clique corresponds exactly valid variable subset satisfying threshold constraint","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"forced-in-set-f-force_in-argument","dir":"Articles","previous_headings":"From Theory to Implementation","what":"Forced-in set (FF) → force_in argument","title":"Theory and Formulation","text":"Ensures certain variables appear every returned subset. corrPrune(data, threshold = 0.7, force_in = c(\"age\", \"gender\")) Internally, algorithm verifies FF valid subset (pairs FF satisfy |aij|<τ|a_{ij}| < \\tau)","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"search-type-mode-and-method-arguments","dir":"Articles","previous_headings":"From Theory to Implementation","what":"Search type → mode and method arguments","title":"Theory and Formulation","text":"mode = \"exact\" Enumerates maximal cliques using ELS Bron–Kerbosch mode = \"greedy\" Constructs single maximal clique via greedy heuristic mode = \"auto\" Uses exact mode p≤20p \\le 20, greedy mode larger pp Choice enumeration algorithm: method = \"els\" Eppstein–Löffler–Strash; recommended force_in specified method = \"bron-kerbosch\" Bron–Kerbosch pivoting (default)","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"association-matrix-a-data-input-and-matrix-based-functions","dir":"Articles","previous_headings":"From Theory to Implementation","what":"Association matrix (AA) → Data input and matrix-based functions","title":"Theory and Formulation","text":"association structure enters algorithm: corrPrune(data) Computes correlation matrix internally finds cliques MatSelect(cor_matrix) Uses precomputed association matrix directly assocSelect(data) Computes mixed association measures (Pearson, eta-squared, Cramér’s V) selection","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"graph-density-performance-considerations","dir":"Articles","previous_headings":"From Theory to Implementation","what":"Graph density → Performance considerations","title":"Theory and Formulation","text":"Depends directly threshold: Sparse graphs (low τ\\tau) edges → fast exact enumeration Dense graphs (high τ\\tau) Many edges → potentially exponential growth maximal cliques properties motivate auto mode: exact small pp, greedy larger pp.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"example-mapping","dir":"Articles","previous_headings":"From Theory to Implementation","what":"Example mapping","title":"Theory and Formulation","text":"Mathematical formulation: Find maximal S⊆{1,…,p} |aij|<0.7∀,j∈S,{age}⊆S. \\text{Find maximal } S \\subseteq \\{1,\\dots,p\\} \\text{ } |a_{ij}| < 0.7\\ \\forall , j \\S,\\ \\text{} \\{ \\text{age} \\} \\subseteq S. Implementation: returned object contains maximal cliques, representing valid variable subset satisfies threshold constraint includes required variables.","code":"results <- corrSelect(   data      = mydata,   threshold = 0.7,      # τ = 0.7   force_in  = \"age\",    # F = {age}   mode      = \"exact\",  # enumerate all maximal cliques   method    = \"els\"     # use ELS algorithm (recommended with force_in) )"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"exact-enumeration","dir":"Articles","previous_headings":"Search Algorithms","what":"Exact Enumeration","title":"Theory and Formulation","text":"Two algorithms enumerate maximal cliques exactly:","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"eppsteinlöfflerstrash-els","dir":"Articles","previous_headings":"Search Algorithms > Exact Enumeration","what":"Eppstein–Löffler–Strash (ELS)","title":"Theory and Formulation","text":"Uses degeneracy ordering structure search: Compute degeneracy ordering v1,…,vpv_1, \\dots, v_p viv_i, extend cliques within candidates {vi+1,…,vp}\\{v_{+1}, \\dots, v_p\\} Recursively build cliques, pruning extension possible Formally, define: extend(R,P)={{R},P=∅⋃v∈Pextend(R∪{v},P∩N(v)),otherwise \\text{extend}(R, P) = \\begin{cases} \\{R\\}, & P = \\emptyset \\\\ \\bigcup_{v \\P} \\text{extend}(R \\cup \\{v\\}, P \\cap N(v)), & \\text{otherwise} \\end{cases} N(v)N(v) denotes neighbors vv GG.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"bronkerbosch-1","dir":"Articles","previous_headings":"Search Algorithms > Exact Enumeration","what":"Bron–Kerbosch","title":"Theory and Formulation","text":"Classical recursive backtracking optional pivoting. Let RR = current clique, PP = candidates, XX = excluded nodes: BK(R,P,X)={report(R),P=X=∅v∈P\\N(u):BK(R∪{v},P∩N(v),X∩N(v))P←P\\{v},X←X∪{v} \\text{BK}(R, P, X) = \\begin{cases} \\text{report}(R), & P = X = \\emptyset \\\\ \\text{} v \\P \\setminus N(u): \\\\ \\quad \\text{BK}(R \\cup \\{v\\}, P \\cap N(v), X \\cap N(v)) \\\\ \\quad P \\leftarrow P \\setminus \\{v\\}, \\quad X \\leftarrow X \\cup \\{v\\} \\end{cases} Pivot u∈P∪Xu \\P \\cup X reduces recursive calls.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"pseudocode-for-practitioners","dir":"Articles","previous_headings":"Search Algorithms > Exact Enumeration","what":"Pseudocode for Practitioners","title":"Theory and Formulation","text":"Eppstein-Löffler-Strash (ELS) Algorithm: Bron-Kerbosch Algorithm: Complexity: ELS: O(d⋅3d/3)O(d \\cdot 3^{d/3}) dd degeneracy (much faster sparse graphs) Bron-Kerbosch: O(3p/3)O(3^{p/3}) worst case, improved pivoting Greedy (): O(p2)O(p^2) deterministic polynomial time","code":"Algorithm: ELS_MaxCliques(Graph G, ForceIn F) Input: Threshold graph G = (V, E), forced variables F ⊆ V Output: All maximal cliques containing F  1. Validate F forms a clique in G 2. Compute degeneracy ordering: v₁, v₂, ..., vₚ 3. Initialize results = []  4. For each vertex vᵢ in ordering:    a. If vᵢ ∉ F and vᵢ not adjacent to all in F:       Skip vᵢ (cannot extend F)     b. candidates = {vⱼ : j > i and vⱼ adjacent to vᵢ} ∩ N(F)    c. Extend(clique = {vᵢ} ∪ F, candidates, results)  5. Return results  Subroutine: Extend(R, P, results)   If P is empty:     Add R to results if maximal     Return    For each v in P:     neighbors = P ∩ N(v)     Extend(R ∪ {v}, neighbors, results) Algorithm: BronKerbosch(Graph G, use_pivot = TRUE) Input: Threshold graph G = (V, E), pivot flag Output: All maximal cliques  1. Initialize: R = ∅ (current clique)               P = V (candidates)               X = ∅ (excluded) 2. Call BK(R, P, X, results) 3. Return results  Subroutine: BK(R, P, X, results)   If P = ∅ and X = ∅:     Add R to results (R is maximal)     Return    If use_pivot:     Choose pivot u from P ∪ X with max |P ∩ N(u)|     iterate = P \\ N(u)  # Skip neighbors of pivot   Else:     iterate = P    For each vertex v in iterate:     BK(R ∪ {v},        P ∩ N(v),     # Only candidates adjacent to v        X ∩ N(v),     # Only excluded adjacent to v        results)      P = P \\ {v}      # Remove v from candidates     X = X ∪ {v}      # Add v to excluded"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"greedy-heuristic","dir":"Articles","previous_headings":"Search Algorithms","what":"Greedy Heuristic","title":"Theory and Formulation","text":"Iteratively removes variables highest average absolute association pairs satisfy |aij|<τ|a_{ij}| < \\tau. Returns single valid subset (necessarily maximal optimal). Complexity: O(p2)O(p^2) vs O(2p)O(2^p) exact enumeration.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"eppsteinlöfflerstrash-els-1","dir":"Articles","previous_headings":"Algorithm Pseudocode","what":"Eppstein–Löffler–Strash (ELS)","title":"Theory and Formulation","text":"Input: Graph G=(V,E)G = (V, E), forced-set FF Output: maximal cliques containing FF Complexity: O(d⋅3d/3)O(d \\cdot 3^{d/3}) dd degeneracy GG. Properties: Exact: enumerates maximal cliques Efficient sparse graphs: performs best threshold graph low density Forced-support: implemented initializing search R=FR = F","code":"Algorithm ELS(G, F):   # Step 1: Compute degeneracy ordering   deg_order ← ComputeDegeneracyOrdering(G)    # Step 2: Initialize with forced-in variables   R ← F  (current clique)   P ← V \\ F  (candidate vertices)   X ← ∅  (excluded vertices)    # Step 3: Validate forced-in set   for each i, j ∈ F:     if (i, j) ∉ E:       return ∅  (infeasible)    # Step 4: Recursively enumerate maximal cliques   return EnumerateCliques(R, P, X, deg_order)  Subroutine EnumerateCliques(R, P, X, ordering):   # Base case: no candidates, no exclusions → maximal clique   if P = ∅ and X = ∅:     report R as maximal clique     return    # Recursive case: extend with each candidate   for each v ∈ P (in degeneracy order):     # Extend clique     R' ← R ∪ {v}      # Update candidates: keep only neighbors of v     P' ← P ∩ N(v)      # Update exclusions: keep only neighbors of v     X' ← X ∩ N(v)      # Recurse     EnumerateCliques(R', P', X', ordering)      # Move v from candidates to exclusions     P ← P \\ {v}     X ← X ∪ {v}  Subroutine ComputeDegeneracyOrdering(G):   # Degeneracy ordering: v_1, ..., v_p where each v_i has   # minimum degree in G[{v_i, ..., v_p}]    ordering ← [ ]   remaining ← V    while remaining ≠ ∅:     # Find vertex with minimum degree in induced subgraph     v ← argmin_{u ∈ remaining} |N(u) ∩ remaining|      # Add to ordering (reverse order)     ordering.prepend(v)     remaining ← remaining \\ {v}    return ordering"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"bronkerbosch-with-pivoting","dir":"Articles","previous_headings":"Algorithm Pseudocode","what":"Bron–Kerbosch with Pivoting","title":"Theory and Formulation","text":"Input: Graph G=(V,E)G = (V, E), forced-set FF Output: maximal cliques containing FF Complexity: O(3p/3)O(3^{p/3}) maximal cliques (worst-case). Properties: Exact: enumerates maximal cliques Pivoting reduces recursion: fewer recursive calls tighter branching Classical algorithm (1973) Pivot selection: Choosing uu maximum degree PP minimizes branching Without pivoting: O(2p)O(2^p) recursive calls (exponential) pivoting: O(3p/3)O(3^{p/3}) recursive calls (still exponential significantly tighter)","code":"Algorithm BronKerbosch(G, F):   # Step 1: Initialize   R ← F  (current clique)   P ← V \\ F  (candidate vertices)   X ← ∅  (excluded vertices)    # Step 2: Validate forced-in set   for each i, j ∈ F:     if (i, j) ∉ E:       return ∅  (infeasible)    # Step 3: Restrict candidates to neighbors of forced-in set   if F ≠ ∅:     P ← P ∩ (⋂_{v ∈ F} N(v))    # Step 4: Enumerate   return BK(R, P, X)  Subroutine BK(R, P, X):   # Base case: no candidates, no exclusions → maximal clique   if P = ∅ and X = ∅:     report R as maximal clique     return    # Choose pivot: vertex with most neighbors in P   u ← argmax_{v ∈ P ∪ X} |N(v) ∩ P|    # Iterate over non-neighbors of pivot (reduces recursion)   for each v ∈ P \\ N(u):     # Extend clique     R' ← R ∪ {v}      # Update candidates: keep only neighbors of v     P' ← P ∩ N(v)      # Update exclusions: keep only neighbors of v     X' ← X ∩ N(v)      # Recurse     BK(R', P', X')      # Move v from candidates to exclusions     P ← P \\ {v}     X ← X ∪ {v}"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"greedy-heuristic-1","dir":"Articles","previous_headings":"Algorithm Pseudocode","what":"Greedy Heuristic","title":"Theory and Formulation","text":"Input: Association matrix AA, threshold τ\\tau, forced-set FF Output: Single valid subset (necessarily maximal) Complexity:O(p2k)O(p^2 k), kk number variables removed. Properties: Fast: polynomial-time procedure Deterministic: identical input always yields identical output Non-optimal: guarantee maximal largest subset Forced-support: variables FF never removed Tie-breaking: several variables share average absolute correlation: Prefer variable lower median absolute correlation still tied, prefer lexicographically first variable name","code":"Algorithm GreedyPrune(A, τ, F):   # Step 1: Initialize with all variables   S ← {1, ..., p}    # Step 2: Validate forced-in set   if F ≠ ∅:     for each i, j ∈ F:       if |a_ij| ≥ τ:         return ∅  (infeasible)    # Step 3: Iteratively remove highest-correlated variables   while ∃ i, j ∈ S: |a_ij| ≥ τ:     # Compute average absolute correlation for each variable     for each v ∈ S \\ F:       avg[v] ← (1 / |S| - 1) × Σ_{u ∈ S, u ≠ v} |a_vu|      # Remove variable with highest average correlation     v_max ← argmax_{v ∈ S \\ F} avg[v]     S ← S \\ {v_max}    # Step 4: Return pruned subset   return S"},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"forced-variables","dir":"Articles","previous_headings":"","what":"Forced Variables","title":"Theory and Formulation","text":"Constraint: variables F⊆{1,…,p}F \\subseteq \\{1, \\dots, p\\} must appear returned subsets.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"graph-modification","dir":"Articles","previous_headings":"Forced Variables","what":"Graph Modification","title":"Theory and Formulation","text":"Modify search: Require F⊆SF \\subseteq S valid SS Verify |aij|<τ|a_{ij}| < \\tau ,j∈Fi, j \\F (else problem infeasible) Search maximal extensions FF within remaining variables Formally, find maximal cliques GG containing FF.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"correlation-vs-association","dir":"Articles","previous_headings":"","what":"Correlation vs Association","title":"Theory and Formulation","text":"Correlation: aij∈[−1,1]a_{ij} \\[-1, 1], use |aij||a_{ij}| threshold constraint Association: aij∈[0,1]a_{ij} \\[0, 1], use aija_{ij} directly Mixed-type data uses association matrix measures chosen per variable-pair type.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"exact-enumeration-1","dir":"Articles","previous_headings":"Complexity Analysis","what":"Exact Enumeration","title":"Theory and Formulation","text":"Worst-case: O(3p/3)O(3^{p/3}) maximal cliques possible Performance depends graph density: Sparse (low τ\\tau): fewer edges, faster enumeration Dense (high τ\\tau): many edges, exponential growth","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"greedy-heuristic-2","dir":"Articles","previous_headings":"Complexity Analysis","what":"Greedy Heuristic","title":"Theory and Formulation","text":"Time: O(p2k)O(p^2 k) kk = iterations Space: O(p2)O(p^2) storing associations Deterministic: input produces output","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"output-structure","dir":"Articles","previous_headings":"","what":"Output Structure","title":"Theory and Formulation","text":"selection functions return CorrCombo S4 object containing: subset_list: list character vectors (variable names per subset) avg_corr: numeric vector (mean |aij||a_{ij}| within subset) min_corr: numeric vector (min |aij||a_{ij}| within subset) max_corr: numeric vector (max |aij||a_{ij}| within subset) threshold: value τ\\tau forced_in: forced variable names cor_method: correlation/association measure used n_rows_used: sample size removing missing values Results sorted : Subset size (descending) Average absolute association (ascending)","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"design-philosophy","dir":"Articles","previous_headings":"","what":"Design Philosophy","title":"Theory and Formulation","text":"section explains key design decisions underlying corrselect, addressing common questions certain choices made.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"why-maximal-not-maximum-1","dir":"Articles","previous_headings":"Design Philosophy","what":"Why “Maximal” Not “Maximum”?","title":"Theory and Formulation","text":"Maximal: extended (locally optimal) Maximum: Largest possible size (globally optimal) corrselect enumerates maximal subsets rather just finding single maximum subset. ? Multiple equally good solutions: Real datasets often many maximal subsets equal similar size. Returning one discards valuable information alternative variable combinations. Domain knowledge integration: Users may prefer slightly smaller subset containing specific variables globally largest subset. options enables informed choice. Sensitivity analysis: Comparing multiple maximal subsets reveals structural properties correlation matrix (e.g., tight vs loosely connected clusters). Computational feasibility: Finding maximum clique NP-complete often harder enumerating maximal cliques practice. Modern maximal clique algorithms (ELS, Bron-Kerbosch) highly efficient typical correlation structures.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"why-hard-threshold-not-soft-constraint","dir":"Articles","previous_headings":"Design Philosophy","what":"Why Hard Threshold Not Soft Constraint?","title":"Theory and Formulation","text":"corrselect enforces hard threshold: |aij|<τ|a_{ij}| < \\tau pairs. Alternative approaches use soft constraints (penalty functions, regularization). hard thresholds? Interpretability: “pair exceeds τ\\tau” clear, verifiable guarantee. Soft constraints produce solutions pairs may exceed τ\\tau unknown magnitude. Reproducibility: Hard thresholds produce deterministic results. Soft constraints often require tuning parameters (penalty weights, convergence criteria) affect reproducibility. Domain-specific requirements: Fields like ecological modeling established thresholds (e.g., τ=0.7\\tau = 0.7 WorldClim variables) based empirical evidence. Hard thresholds directly implement guidelines. Exact enumeration: Hard constraints enable graph-theoretic formulation exact algorithms. Soft constraints typically require heuristic optimization.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"why-graph-algorithms-not-optimization","dir":"Articles","previous_headings":"Design Philosophy","what":"Why Graph Algorithms Not Optimization?","title":"Theory and Formulation","text":"corrselect uses specialized graph algorithms (ELS, Bron-Kerbosch) rather general optimization frameworks (integer programming, metaheuristics). ? Asymptotic efficiency: Graph algorithms exploit structural properties (sparsity, degeneracy) unavailable generic solvers. sparse graphs (low τ\\tau), yields orders--magnitude speedups. Exact enumeration: Graph algorithms guarantee finding maximal cliques. Optimization approaches typically find one solution. Forced variables: Graph algorithms naturally handle forced-constraints via initialization. Optimization approaches require additional constraints may degrade performance. Established theory: Maximal clique enumeration 50+ years algorithmic development proven complexity bounds implementation strategies.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"why-pairwise-associations-only","dir":"Articles","previous_headings":"Design Philosophy","what":"Why Pairwise Associations Only?","title":"Theory and Formulation","text":"corrselect considers pairwise associations, higher-order interactions (partial correlations, conditional independence). ? Computational tractability: Higher-order interactions require exponentially computation. pp variables, pairwise methods scale O(p2)O(p^2), kk-way interactions scale O(pk)O(p^k). Clear interpretation: “Variables ii jj correlate r=0.85r = 0.85” directly interpretable. Partial correlations require careful conditioning set selection can counterintuitive. Robustness: Pairwise correlations stable moderate sample sizes. Partial correlations require n≫pn \\gg p sensitive model misspecification. Method generality: Pairwise associations work variable type (numeric, categorical, mixed). Higher-order methods often require strong distributional assumptions. higher-order methods appropriate: goal causal discovery (identifying conditional independence structure), use methods like PC algorithm constraint-based causal inference. corrselect focuses reducing redundancy predictive modeling descriptive analysis.","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"why-enumerate-all-solutions-not-just-return-one","dir":"Articles","previous_headings":"Design Philosophy","what":"Why Enumerate All Solutions Not Just Return One?","title":"Theory and Formulation","text":"Many correlation-pruning tools (e.g., caret::findCorrelation) return single subset. corrselect can return maximal subsets (exact mode) one (greedy mode). offer exhaustive enumeration? Alternative solutions: Multiple maximal subsets represent genuinely different variable combinations satisfy threshold constraint. Arbitrarily choosing one discards information. Downstream analysis: Different subsets may preferred different models research questions. Enumerating options enables post-hoc selection criteria. Documentation: reproducible research (especially JOSS/CRAN submissions), documenting valid solutions provides transparency algorithmic choices. Computational cost: typical use cases (p≤30p \\leq 30, τ≥0.5\\tau \\geq 0.5), exhaustive enumeration completes milliseconds. infeasible, greedy mode provides fast approximation. use greedy mode: High-dimensional data (p>50p > 50), dense correlation structure (high τ\\tau), single solution suffices (e.g., automated pipelines).","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"graph-theoretic-algorithms","dir":"Articles","previous_headings":"References","what":"Graph-Theoretic Algorithms","title":"Theory and Formulation","text":"Maximal clique enumeration Foundation ELS algorithm: degeneracy-based maximal clique enumeration Complexity O(d⋅3d/3)O(d \\cdot 3^{d/3}), dd graph degeneracy Used force_in specified Foundation Bron–Kerbosch algorithm: classic backtracking pivoting Default exact enumeration method Pivoting strategy: refined pivot rules Bron–Kerbosch Establishes O(3p/3)O(3^{p/3}) worst-case bound Graph degeneracy: Degeneracy ordering: Fundamental concept sparse graph algorithms Used ELS algorithm efficient enumeration Independent sets vertex cover: Maximal independent set enumeration: Theoretical foundation threshold graph problem Complements maximal clique enumeration (clique complement graph)","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"multicollinearity-and-variable-selection","dir":"Articles","previous_headings":"References","what":"Multicollinearity and Variable Selection","title":"Theory and Formulation","text":"Variance inflation factor (VIF): Standard reference: VIF computation condition indices collinearity diagnosis Foundation modelPrune() approach Critical evaluation: Common VIF thresholds (5, 10) may inappropriate contexts Recommends context-specific threshold selection Ridge regression: Alternative approach handling multicollinearity via regularization Contrasts hard variable removal (corrselect approach) Correlation-based variable selection: Survey: Overview filter, wrapper, embedded methods variable selection Context correlation-based filtering (filter method) Correlation-based feature selection (CFS): Alternative criterion combining feature-class correlation feature-feature correlation Differs corrselect’s pairwise-approach","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"association-measures","dir":"Articles","previous_headings":"References","what":"Association Measures","title":"Theory and Formulation","text":"Numeric associations: Pearson correlation: Standard linear association measure Spearman’s rank correlation: Monotonic association ordered data Kendall’s tau: Alternative rank-based measure, robust outliers Categorical associations: Cramér’s V: Chi-squared-based association categorical variables Used assocSelect() factor-factor pairs Chi-squared test: Foundation Cramér’s V Mixed-type associations: Eta-squared (correlation ratio): Association numeric categorical variables Used assocSelect() numeric-factor pairs","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"threshold-graph-theory","dir":"Articles","previous_headings":"References","what":"Threshold Graph Theory","title":"Theory and Formulation","text":"Comprehensive reference: Mathematical properties threshold graphs Theoretical foundation threshold-based graph construction Threshold graph characterization: Alternative definitions recognition algorithms","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"computational-complexity","dir":"Articles","previous_headings":"References","what":"Computational Complexity","title":"Theory and Formulation","text":"Standard reference: Complexity classes, NP-completeness Context understanding exponential enumeration complexity Theoretical bound: Proof graphs can 3p/33^{p/3} maximal cliques Justifies worst-case complexity exact enumeration","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"applications","dir":"Articles","previous_headings":"References","what":"Applications","title":"Theory and Formulation","text":"Bioclimatic modeling: Species distribution models: Evaluation multicollinearity approaches ecology Recommends threshold τ=0.7\\tau = 0.7 bioclimatic variables Genomics: High-dimensional biology: Variable selection genomics proteomics Context correlation-based filtering gene expression analysis","code":""},{"path":"https://gillescolling.com/corrselect/articles/theory.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Theory and Formulation","text":"vignette(\"quickstart\") - Interface overview examples vignette(\"workflows\") - Real-world workflow examples vignette(\"advanced\") - Algorithmic control custom engines vignette(\"comparison\") - Comparison alternatives","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Complete Workflows: Real-World Examples","text":"vignette demonstrates integration corrselect complete modeling workflows, raw predictors final models correlation constraints. Four applied settings illustrate interface selection workflow composition. See vignette(\"quickstart\") interface descriptions vignette(\"theory\") mathematical foundations. workflow showcases different aspects package: Ecological Modeling: Two-stage pruning (correlation + VIF) environmental predictors Survey Data: Protecting key variables reducing redundancy questionnaires High-Dimensional Data: Greedy algorithms gene expression (p >> n scenarios) Mixed Models: VIF-based pruning fixed effects hierarchical data","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"workflow-1-ecological-modeling","dir":"Articles","previous_headings":"","what":"Workflow 1: Ecological Modeling","title":"Complete Workflows: Real-World Examples","text":"Goal: Build interpretable species distribution model highly correlated bioclimatic variables. Challenge: WorldClim’s 19 bioclimatic variables contain many temperature precipitation metrics mathematically related (e.g., mean temperature, minimum temperature, maximum temperature). Using variables leads multicollinearity, unstable coefficients, poor model interpretation. Strategy: Two-stage pruning: corrPrune() removes pairwise correlations > 0.7 modelPrune() refines using variance inflation factors (VIF) approach balances model fit interpretability numerical stability.","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"data","dir":"Articles","previous_headings":"Workflow 1: Ecological Modeling","what":"Data","title":"Complete Workflows: Real-World Examples","text":"dataset contains 500 sampling locations 19 bioclimatic predictors species richness response. bioclimatic variables standard WorldClim metrics (BIO1-BIO19) measuring temperature precipitation patterns.","code":"library(corrselect) data(bioclim_example)  # Data structure dim(bioclim_example) #> [1] 100  20 head(names(bioclim_example)) #> [1] \"species_richness\" \"BIO1\"             \"BIO2\"             \"BIO3\"             #> [5] \"BIO4\"             \"BIO5\"  # Response variable summary(bioclim_example$species_richness) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>     5.0    96.0   118.5   120.2   144.2   206.0"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"correlation-based-pruning","dir":"Articles","previous_headings":"Workflow 1: Ecological Modeling","what":"Correlation-based pruning","title":"Complete Workflows: Real-World Examples","text":"start removing variables pairwise correlations exceeding 0.7. mode = \"auto\" setting uses exact algorithm (Bron-Kerbosch) enumerate maximal subsets, selects largest one. pruning successfully reduced dimensionality ensuring remaining pair exceeds 0.7 threshold. selected variables span temperature precipitation domains, maintaining ecological interpretability.","code":"# Remove highly correlated predictors bio_clean <- corrPrune(   data = bioclim_example[, -1],  # Exclude response   threshold = 0.7,   mode = \"auto\" )  # How much did we reduce? cat(sprintf(\"Reduced from %d → %d variables\\n\",             ncol(bioclim_example) - 1,             ncol(bio_clean))) #> Reduced from 19 → 12 variables  # Which variables were kept? head(attr(bio_clean, \"selected_vars\"), 10) #>  [1] \"BIO1\"  \"BIO3\"  \"BIO6\"  \"BIO9\"  \"BIO11\" \"BIO12\" \"BIO13\" \"BIO14\" \"BIO16\" #> [10] \"BIO17\""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"correlation-distribution","dir":"Articles","previous_headings":"Workflow 1: Ecological Modeling","what":"Correlation distribution","title":"Complete Workflows: Real-World Examples","text":"histogram shows corrPrune() reshapes correlation structure. pruning (red), many variable pairs exceed 0.7 threshold. pruning (blue), pairwise correlations fall threshold.","code":"cor_before <- cor(bioclim_example[, -1]) cor_after  <- cor(bio_clean)  vals_before <- abs(cor_before[upper.tri(cor_before)]) vals_after  <- abs(cor_after[upper.tri(cor_after)])  # Common x limits xlim <- c(0, 1)  # Shared breaks for fair comparison breaks <- seq(0, 1, length.out = 30)  # Before histogram hist(vals_before,      breaks = breaks,      freq = FALSE,      main = \"Distribution of Absolute Correlations\",      xlab = \"Absolute Correlation\",      col = rgb(0.8, 0.2, 0.2, 0.4),      border = \"white\",      xlim = xlim)  # After histogram hist(vals_after,      breaks = breaks,      freq = FALSE,      col = rgb(0.2, 0.5, 0.8, 0.4),      border = \"white\",      add = TRUE)  # Threshold line abline(v = 0.7, col = \"black\", lty = 2, lwd = 2)  legend(\"topright\",        legend = c(\"Before\", \"After\", \"Threshold\"),        fill   = c(rgb(0.8, 0.2, 0.2, 0.4),                   rgb(0.2, 0.5, 0.8, 0.4),                   NA),        border = c(\"white\", \"white\", NA),        lty    = c(NA, NA, 2),        lwd    = c(NA, NA, 2),        col    = c(NA, NA, \"black\"),        bty    = \"o\",        bg = NA)"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"fit-models","dir":"Articles","previous_headings":"Workflow 1: Ecological Modeling","what":"Fit models","title":"Complete Workflows: Real-World Examples","text":"fit three models compare effect pruning: Full model: 19 bioclimatic variables (baseline) corrPrune: Variables correlations < 0.7 modelPrune: refined using VIF < 5 VIF criterion complements correlation-based pruning detecting multicollinearity involving two variables simultaneously.","code":"# Model 1: Full model (19 variables) model_full <- lm(species_richness ~ ., data = bioclim_example)  # Model 2: After corrPrune (correlation-based pruning) bio_clean_full <- data.frame(   species_richness = bioclim_example$species_richness,   bio_clean ) model_corrprune <- lm(species_richness ~ ., data = bio_clean_full)  # Model 3: Sequential VIF-based refinement bio_final <- modelPrune(   formula = species_richness ~ .,   data = bio_clean_full,   limit = 5 ) model_final <- attr(bio_final, \"final_model\")"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"model-comparison","dir":"Articles","previous_headings":"Workflow 1: Ecological Modeling","what":"Model comparison","title":"Complete Workflows: Real-World Examples","text":"table shows pruning dramatically improves numerical stability (condition number κ) maintaining model fit (adjusted R²). lower κ indicates better-conditioned matrices stable coefficient estimates. Key insights: full model severe multicollinearity (κ > 1000) Correlation-based pruning reduces κ ~10x losing minimal fit VIF-based refinement achieves excellent stability (κ < 100) marginal R² decrease","code":"# Variable counts n_full      <- 19 n_corrprune <- length(attr(bio_clean, \"selected_vars\")) n_final     <- length(attr(bio_final, \"selected_vars\"))  # Compute condition numbers (measure of collinearity) X_full      <- model.matrix(model_full)[, -1] X_corrprune <- model.matrix(model_corrprune)[, -1] X_final     <- model.matrix(model_final)[, -1]  kappa_full      <- kappa(X_full, exact = TRUE) kappa_corrprune <- kappa(X_corrprune, exact = TRUE) kappa_final     <- kappa(X_final, exact = TRUE)  # Summary table comparison <- data.frame(   Step = c(\"Full\", \"corrPrune\", \"+ modelPrune\"),   Predictors = c(n_full, n_corrprune, n_final),   Adj_R2 = c(     summary(model_full)$adj.r.squared,     summary(model_corrprune)$adj.r.squared,     summary(model_final)$adj.r.squared   ),   Kappa = c(kappa_full, kappa_corrprune, kappa_final) )  print(comparison) #>           Step Predictors    Adj_R2    Kappa #> 1         Full         19 0.9821862 914.8146 #> 2    corrPrune         12 0.8962848 538.1157 #> 3 + modelPrune         12 0.8962848 538.1157"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"visualization","dir":"Articles","previous_headings":"Workflow 1: Ecological Modeling","what":"Visualization","title":"Complete Workflows: Real-World Examples","text":"plot illustrates pruning tradeoff: remove variables, condition number (κ) drops dramatically adjusted R² remains high. demonstrates many original 19 variables redundant prediction.","code":"# Extract data n_vars <- comparison$Predictors adj_r2 <- comparison$Adj_R2 kappa  <- comparison$Kappa  # Left y-axis: Adjusted R² par(mar = c(5, 4, 4, 4))  # extra space on the right for second axis  plot(   n_vars, adj_r2,   type = \"b\",   pch  = 19,   cex  = 1.5,   col  = rgb(0.2, 0.5, 0.8, 1),   lwd  = 2,   xlab = \"Number of Predictors\",   ylab = \"Adjusted R²\",   ylim = c(0, 1),   main = \"Pruning Reduces Collinearity While Preserving Fit\" )  # Right y-axis: log10(κ) log_kappa <- log10(kappa)  # Nice ylim for log10(κ) ylim_right <- range(log_kappa, finite = TRUE) ylim_right <- ylim_right * c(0.9, 1.1)  par(new = TRUE) plot(   n_vars, log_kappa,   type = \"b\",   pch  = 17,   cex  = 1.5,   col  = rgb(0.8, 0.2, 0.2, 1),   lwd  = 2,   xaxt = \"n\",   yaxt = \"n\",   xlab = \"\",   ylab = \"\",   ylim = ylim_right )  # Right-hand axis ticks using pretty() on log scale log_ticks <- pretty(log_kappa) kappa_labels <- round(10^log_ticks) axis(4, at = log_ticks, labels = kappa_labels) mtext(\"Condition Number (κ)\", side = 4, line = 3)  # Legend centered at top legend(   \"top\",   inset = 0.02,   legend = c(\"Adjusted R² (higher better)\", \"κ (lower better)\"),   col    = c(     rgb(0.2, 0.5, 0.8, 1),     rgb(0.8, 0.2, 0.2, 1)   ),   pch    = c(19, 17),   lwd    = 2,   horiz  = TRUE,   bty    = \"o\",   bg = NA,   x.intersp = 0.8 )"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"coefficient-stability","dir":"Articles","previous_headings":"Workflow 1: Ecological Modeling","what":"Coefficient stability","title":"Complete Workflows: Real-World Examples","text":"Multicollinearity inflates coefficient variance, making estimates unstable. plot compares coefficients full model (19 variables) final pruned model. Notice : Variables dropped pruned model (shown red-bars) unstable estimates Variables retained models (overlapping bars) show consistent magnitudes pruned model yields clearer, interpretable effect sizes","code":"# Extract coefficients (excluding intercept) coef_full  <- coef(model_full)[-1] coef_final <- coef(model_final)[-1]  # Use *all* variables (not just common ones) all_vars <- union(names(coef_full), names(coef_final))  # Align coefficients to the same full variable list vals_full   <- coef_full[all_vars] vals_pruned <- coef_final[all_vars]  # Replace missing values (variables dropped in a model) with 0 vals_full[is.na(vals_full)]     <- 0 vals_pruned[is.na(vals_pruned)] <- 0  # Colours col_full   <- rgb(0.8, 0.2, 0.2, 0.5) col_pruned <- rgb(0.2, 0.5, 0.8, 0.5)  x <- seq_along(all_vars)  # Symmetric Y range ylim <- range(c(vals_full, vals_pruned)) * 1.15  # Empty plot first plot(   x, vals_full,   type = \"n\",   xaxt = \"n\",   xlab = \"\",   ylab = \"Coefficient\",   main = \"Coefficient Comparison (Full vs modelPrune)\",   ylim = ylim )  axis(1, at = x, labels = all_vars, las = 2, cex.axis = 0.7)  # Full model bars rect(   x - 0.4, 0,   x + 0.4, vals_full,   col = col_full, border = NA )  # Pruned model bars rect(   x - 0.4, 0,   x + 0.4, vals_pruned,   col = col_pruned, border = NA )  legend(   \"topright\",   legend = c(     sprintf(\"Full (%d vars)\", n_full),     sprintf(\"Final (%d vars)\", n_final)   ),   fill = c(col_full, col_pruned),   border = \"white\",   bty = \"o\",   bg = NA )"},{"path":[]},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"workflow-2-survey-data-analysis","dir":"Articles","previous_headings":"","what":"Workflow 2: Survey Data Analysis","title":"Complete Workflows: Real-World Examples","text":"Goal: Reduce questionnaire length preserving construct coverage protecting key demographic variables. Challenge: Survey instruments often contain redundant items within constructs (e.g., multiple satisfaction questions highly correlated). Reducing number items improves response rates reduces respondent burden without losing measurement quality. Strategy: Use corrPrune() force_in : Ensure critical variables (like age) appear final model Remove redundant Likert items within constructs Maintain balanced representation across satisfaction, engagement, loyalty domains","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"data-1","dir":"Articles","previous_headings":"Workflow 2: Survey Data Analysis","what":"Data","title":"Complete Workflows: Real-World Examples","text":"dataset contains 500 respondents, 30 Likert-scale items (10 satisfaction, engagement, loyalty), plus demographics (age, gender, education) overall satisfaction outcome.","code":"data(survey_example)  # Data structure dim(survey_example) #> [1] 200  35 str(survey_example[, 1:10])  # First 10 columns #> 'data.frame':    200 obs. of  10 variables: #>  $ respondent_id       : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ age                 : num  38 32 18 18 19 39 33 26 26 42 ... #>  $ gender              : Factor w/ 3 levels \"Female\",\"Male\",..: 2 3 1 2 2 1 1 2 2 1 ... #>  $ education           : Ord.factor w/ 4 levels \"High School\"<..: 3 1 4 2 2 1 1 1 2 3 ... #>  $ overall_satisfaction: num  58 40 44 40 58 67 61 49 51 52 ... #>  $ satisfaction_1      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 5 3 4 5 5 4 4 5 ... #>  $ satisfaction_2      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 4 3 4 6 6 4 4 3 ... #>  $ satisfaction_3      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 4 3 3 4 5 3 4 4 ... #>  $ satisfaction_4      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 4 4 4 5 4 3 2 4 ... #>  $ satisfaction_5      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 7 4 5 5 5 4 3 6 4 6 ..."},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"prune-with-protected-variables","dir":"Articles","previous_headings":"Workflow 2: Survey Data Analysis","what":"Prune with protected variables","title":"Complete Workflows: Real-World Examples","text":"use force_in = \"age\" ensure age remains analysis regardless correlation variables. useful domain knowledge identifies theoretically important covariates must removed. pruning reduced questionnaire 31 items ~10 items ensuring age retained. remaining items span three constructs, avoiding loss entire domains.","code":"# Exclude respondent_id, overall_satisfaction, and factor variables survey_predictors <- survey_example[, !(names(survey_example) %in%                                          c(\"respondent_id\", \"overall_satisfaction\",                                            \"gender\", \"education\"))]  # Convert ordered factors (Likert items 1-7) to numeric for correlation analysis survey_numeric <- as.data.frame(lapply(survey_predictors, function(x) {   if (is.ordered(x)) as.numeric(as.character(x)) else as.numeric(x) }))  # Prune with protected variables survey_clean <- corrPrune(   data = survey_numeric,   threshold = 0.6,   force_in = \"age\" )  # How many items remain? cat(sprintf(\"Reduced from %d → %d variables\\n\",             ncol(survey_numeric),             ncol(survey_clean))) #> Reduced from 31 → 4 variables  # Which items were kept? selected <- attr(survey_clean, \"selected_vars\") print(selected) #> [1] \"age\"            \"satisfaction_9\" \"engagement_1\"   \"loyalty_3\""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"construct-coverage","dir":"Articles","previous_headings":"Workflow 2: Survey Data Analysis","what":"Construct coverage","title":"Complete Workflows: Real-World Examples","text":"’s important verify pruning didn’t eliminate entire constructs. check many items domain (satisfaction, engagement, loyalty) survived correlation threshold. Good balance: three constructs retained representation, ensuring reduced questionnaire still measures intended dimensions.","code":"# Count items per construct satisfaction_kept <- sum(grepl(\"satisfaction_\", selected)) engagement_kept <- sum(grepl(\"engagement_\", selected)) loyalty_kept <- sum(grepl(\"loyalty_\", selected))  cat(sprintf(\"Satisfaction: %d/10 items kept\\n\", satisfaction_kept)) #> Satisfaction: 1/10 items kept cat(sprintf(\"Engagement: %d/10 items kept\\n\", engagement_kept)) #> Engagement: 1/10 items kept cat(sprintf(\"Loyalty: %d/10 items kept\\n\", loyalty_kept)) #> Loyalty: 1/10 items kept"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"visualization-1","dir":"Articles","previous_headings":"Workflow 2: Survey Data Analysis","what":"Visualization","title":"Complete Workflows: Real-World Examples","text":"barplots show (1) many items survived pruning within construct (2) overall variable reduction.","code":"par(mfrow = c(1, 2))  # Items kept per construct construct_data <- rbind(   c(10, 10, 10),   c(satisfaction_kept, engagement_kept, loyalty_kept) )  barplot(construct_data,         beside = TRUE,         names.arg = c(\"Satisfaction\", \"Engagement\", \"Loyalty\"),         col = c(\"lightgray\", \"lightblue\"),         legend.text = c(\"Original (10)\", \"After pruning\"),         args.legend = list(x = \"topright\", bty = \"n\"),         main = \"Items per Construct\",         ylab = \"Number of Items\",         ylim = c(0, 12))  # Percentage reduction barplot(c(ncol(survey_numeric), ncol(survey_clean)),         names.arg = c(\"Before\", \"After\"),         col = c(\"salmon\", \"lightgreen\"),         main = \"Total Variables\",         ylab = \"Count\",         ylim = c(0, max(ncol(survey_numeric)) * 1.2)) text(0.7, ncol(survey_numeric) + 1, ncol(survey_numeric), pos = 3) text(1.9, ncol(survey_clean) + 1, ncol(survey_clean), pos = 3)"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"model-satisfaction","dir":"Articles","previous_headings":"Workflow 2: Survey Data Analysis","what":"Model satisfaction","title":"Complete Workflows: Real-World Examples","text":"Now fit regression model predicting overall satisfaction pruned item set. Despite using fewer predictors, model maintain good explanatory power removed redundant items.","code":"# Add response back survey_model_data <- data.frame(   overall_satisfaction = survey_example$overall_satisfaction,   survey_clean )  # Fit regression model model_survey <- lm(overall_satisfaction ~ ., data = survey_model_data)  # Summary summary(model_survey) #>  #> Call: #> lm(formula = overall_satisfaction ~ ., data = survey_model_data) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -17.0653  -4.1416  -0.1467   4.2400  22.2405  #>  #> Coefficients: #>                Estimate Std. Error t value Pr(>|t|)     #> (Intercept)    24.85040    2.03665  12.202   <2e-16 *** #> age             0.01951    0.04109   0.475   0.6354     #> satisfaction_9  4.96640    0.34387  14.443   <2e-16 *** #> engagement_1    0.24937    0.32131   0.776   0.4386     #> loyalty_3       0.54190    0.30190   1.795   0.0742 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 6.372 on 195 degrees of freedom #> Multiple R-squared:  0.6893, Adjusted R-squared:  0.683  #> F-statistic: 108.2 on 4 and 195 DF,  p-value: < 2.2e-16"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"model-comparison-1","dir":"Articles","previous_headings":"Workflow 2: Survey Data Analysis","what":"Model comparison","title":"Complete Workflows: Real-World Examples","text":"Comparing pruned model full 33-variable model shows retain explanatory power (R²) dramatically reducing model complexity.","code":"# Full model (all 30 items + demographics) full_survey_data <- data.frame(   overall_satisfaction = survey_example$overall_satisfaction,   survey_predictors )  model_full_survey <- lm(overall_satisfaction ~ ., data = full_survey_data)  # Compare data.frame(   Model = c(\"Full (33 vars)\", \"Pruned (10 vars)\"),   R2 = c(summary(model_full_survey)$r.squared,          summary(model_survey)$r.squared),   Adj_R2 = c(summary(model_full_survey)$adj.r.squared,              summary(model_survey)$adj.r.squared),   Num_Predictors = c(33, 10) ) #>              Model        R2    Adj_R2 Num_Predictors #> 1   Full (33 vars) 0.9790144 0.7679931             33 #> 2 Pruned (10 vars) 0.6893327 0.6829600             10"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"workflow-3-high-dimensional-data","dir":"Articles","previous_headings":"","what":"Workflow 3: High-Dimensional Data","title":"Complete Workflows: Real-World Examples","text":"Goal: Reduce dimensionality gene expression dataset number predictors far exceeds number samples (p >> n). Challenge: 200 genes 100 samples, exact enumeration maximal subsets becomes computationally expensive. Standard regression also impossible due rank deficiency. Strategy: Use mode = \"greedy\" fast, approximate pruning: greedy algorithm scales linearly number variables guaranteed find largest subset, provides high-quality solution orders magnitude faster Ideal exploratory analysis high-dimensional settings","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"data-2","dir":"Articles","previous_headings":"Workflow 3: High-Dimensional Data","what":"Data","title":"Complete Workflows: Real-World Examples","text":"dataset contains gene expression measurements 200 genes across 100 samples, binary disease outcome. classic p >> n scenario regularization dimensionality reduction essential.","code":"data(genes_example)  # Data structure dim(genes_example) #> [1] 100 202  # Disease prevalence table(genes_example$disease_status) #>  #> Healthy Disease  #>       2      98"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"greedy-pruning","dir":"Articles","previous_headings":"Workflow 3: High-Dimensional Data","what":"Greedy pruning","title":"Complete Workflows: Real-World Examples","text":"greedy algorithm iteratively selects variables, prioritizing lowest maximum correlation already-selected variables. heuristic runs O(p²) time compared exponential complexity exact methods. greedy algorithm completed milliseconds, reducing gene set ~50% ensuring pairwise correlations remain 0.8.","code":"# Extract gene expression data (exclude ID and outcome) gene_expr <- genes_example[, -(1:2)]  # Greedy pruning system.time({   genes_pruned <- corrPrune(     data = gene_expr,     threshold = 0.8,     mode = \"greedy\"  # Fast for large p   ) }) #>    user  system elapsed  #>    0.01    0.00    0.02  # Reduction cat(sprintf(\"Reduced from %d → %d genes\\n\",             ncol(gene_expr),             ncol(genes_pruned))) #> Reduced from 200 → 177 genes"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"dimensionality-reduction","dir":"Articles","previous_headings":"Workflow 3: High-Dimensional Data","what":"Dimensionality reduction","title":"Complete Workflows: Real-World Examples","text":"","code":"# Barplot showing reduction reduction_data <- c(ncol(gene_expr), ncol(genes_pruned)) barplot(reduction_data,         names.arg = c(\"Original\", \"After Pruning\"),         main = \"Gene Dimensionality Reduction\",         ylab = \"Number of Genes\",         col = c(\"salmon\", \"lightblue\"),         ylim = c(0, max(reduction_data) * 1.2)) text(0.7, reduction_data[1] + 10, paste(reduction_data[1], \"genes\"), pos = 3) text(1.9, reduction_data[2] + 10, paste(reduction_data[2], \"genes\\n(\",      round(100 * reduction_data[2] / reduction_data[1], 1), \"% retained)\"), pos = 3)"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"exact-vs-greedy-comparison","dir":"Articles","previous_headings":"Workflow 3: High-Dimensional Data","what":"Exact vs greedy comparison","title":"Complete Workflows: Real-World Examples","text":"demonstrate speed advantage greedy algorithm, benchmark approaches smaller subset. performance gap widens dramatically number variables increases. greedy mode substantially faster. full 200-gene dataset, exact enumeration prohibitively slow, greedy mode completes seconds.","code":"library(microbenchmark)  # Subset for comparison (use smaller subset for vignette build speed) gene_subset <- gene_expr[, 1:20]  # Reduced from 50 to 20 for faster builds  # Benchmark exact mode exact_time <- median(microbenchmark(   exact_result <- corrPrune(gene_subset, threshold = 0.8, mode = \"exact\"),   times = 3,   unit = \"ms\" )$time) / 1e6  # Convert nanoseconds to milliseconds  # Benchmark greedy mode greedy_time <- median(microbenchmark(   greedy_result <- corrPrune(gene_subset, threshold = 0.8, mode = \"greedy\"),   times = 3,   unit = \"ms\" )$time) / 1e6  # Convert nanoseconds to milliseconds  # Run once more to get actual results for comparison exact_result <- corrPrune(gene_subset, threshold = 0.8, mode = \"exact\") greedy_result <- corrPrune(gene_subset, threshold = 0.8, mode = \"greedy\")  # Compare cat(sprintf(\"Exact mode: %d genes kept (%.1f ms)\\n\", ncol(exact_result), exact_time)) #> Exact mode: 11 genes kept (3.3 ms) cat(sprintf(\"Greedy mode: %d genes kept (%.1f ms)\\n\", ncol(greedy_result), greedy_time)) #> Greedy mode: 10 genes kept (0.5 ms) cat(sprintf(\"Speedup: %.1fx faster\\n\", exact_time / greedy_time)) #> Speedup: 6.3x faster"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"classification","dir":"Articles","previous_headings":"Workflow 3: High-Dimensional Data","what":"Classification","title":"Complete Workflows: Real-World Examples","text":"Finally, demonstrate pruned gene set suitable downstream classification, despite dramatic dimensionality reduction.","code":"# Prepare classification data classification_data <- data.frame(   disease_status = genes_example$disease_status,   genes_pruned )  # Logistic regression model_genes <- glm(disease_status ~ .,                    data = classification_data,                    family = binomial())  # Prediction accuracy predictions <- ifelse(predict(model_genes, type = \"response\") > 0.5,                      \"Disease\", \"Healthy\") accuracy <- mean(predictions == genes_example$disease_status)  cat(sprintf(\"Classification accuracy: %.1f%%\\n\", accuracy * 100)) #> Classification accuracy: 100.0%"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"workflow-4-mixed-models","dir":"Articles","previous_headings":"","what":"Workflow 4: Mixed Models","title":"Complete Workflows: Real-World Examples","text":"Goal: Apply correlation-based pruning fixed effects mixed-effects model longitudinal data. Challenge: Longitudinal data hierarchical structure (observations nested within subjects, subjects nested within sites). Standard VIF calculations don’t account random effects, still need control multicollinearity among fixed-effect predictors. Strategy: Use modelPrune() engine = \"lme4\": fixed effects pruned based VIF Random effects (1|subject) (1|site) preserved model formula maintains hierarchical structure reducing collinearity Note: workflow requires lme4 package shown eval=FALSE portability.","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"data-3","dir":"Articles","previous_headings":"Workflow 4: Mixed Models","what":"Data","title":"Complete Workflows: Real-World Examples","text":"dataset 500 observations 50 subjects across 2 sites, 10 measurements per subject. 5 correlated fixed-effect predictors (x1-x5).","code":"data(longitudinal_example)  # Data structure dim(longitudinal_example) #> [1] 500  25 head(longitudinal_example) #>   obs_id subject site time  outcome          x1         x2          x3 #> 1      1       1    1    1 12.10893 -0.62045078 -0.8629274 -0.39625483 #> 2      2       1    1    2 13.97195  0.04255998  0.4086801  0.27069591 #> 3      3       1    1    3 15.71622  0.20445067 -1.0457672  1.69557480 #> 4      4       1    1    4 12.61343  0.38437289  2.0301256 -1.17894923 #> 5      5       1    1    5 15.73139 -0.06402543  1.1645219 -0.19757260 #> 6      6       1    1    6 13.19571  0.65208588 -1.5040345  0.07671762 #>           x4         x5          x6         x7         x8           x9 #> 1 -0.4557709 -1.2550770 -0.35966846 -1.9288176  0.8393628  0.151111346 #> 2 -0.7652493 -1.2136755 -0.05181211 -0.4759841  0.9063848  1.429621637 #> 3  0.5569423 -1.4962408  0.23986408 -1.7077822  1.0343806  0.797974670 #> 4  0.1604224 -0.8192580 -0.94858863 -1.9314095 -0.0639552  0.887413164 #> 5 -1.3331022 -0.6162473 -0.61339605 -0.9439486 -0.1220047 -0.796244469 #> 6  0.5205094 -0.8547461  0.24120308 -1.3083776  0.8716209  0.005423635 #>          x10        x11       x12         x13        x14        x15         x16 #> 1  0.9681971 -0.1384342 0.2598923  0.37082493 -0.1829715 -1.1853721 -0.72703262 #> 2  2.0989531  0.6303914 0.6211064 -0.91103843 -0.1125705 -0.3458058 -0.44331165 #> 3  0.2329831 -0.8618361 2.0854289  0.02634099 -1.3522055 -1.2151804  0.00311954 #> 4  1.5491815  1.7595441 0.8198899  0.18115372 -0.5638401 -1.3637072 -0.55274223 #> 5  2.3485234 -0.1267969 2.6230203 -1.13918808 -1.1559678 -0.3214032 -0.87713586 #> 6 -0.1860167 -0.4620834 1.6075140 -1.81586413 -0.0172986 -0.1041232 -0.56268259 #>           x17        x18        x19       x20 #> 1  0.91333399  1.2398417  1.2973339 1.0590334 #> 2 -0.09799793 -1.3670758 -1.0050198 0.9990451 #> 3 -0.14957945 -0.8573087  0.9004974 0.5399732 #> 4  0.55650397  0.7458841  0.7415159 1.7067212 #> 5  0.36401595 -0.4019171  1.1669136 0.7799264 #> 6  0.30234421 -0.6646296 -0.6615620 1.5092899  # Study design cat(sprintf(\"Subjects: %d\\n\", length(unique(longitudinal_example$subject)))) #> Subjects: 50 cat(sprintf(\"Sites: %d\\n\", length(unique(longitudinal_example$site)))) #> Sites: 5 cat(sprintf(\"Observations per subject: %d\\n\",             nrow(longitudinal_example) / length(unique(longitudinal_example$subject)))) #> Observations per subject: 10"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"prune-fixed-effects","dir":"Articles","previous_headings":"Workflow 4: Mixed Models","what":"Prune fixed effects","title":"Complete Workflows: Real-World Examples","text":"modelPrune() function engine = \"lme4\" respects random-effects structure. fixed effects (x1-x5) candidates removal; random intercepts subject site remain untouched. algorithm sequentially removes fixed effects VIF > 5 remaining predictors satisfy limit. random effects structure never modified.","code":"# Note: This example requires lme4 package library(lme4)  # Define formula with random effects # Note: Only fixed effects (x1-x5) will be pruned #       Random effects (1|subject), (1|site) are preserved  pruned_mixed <- modelPrune(   formula = outcome ~ x1 + x2 + x3 + x4 + x5 + (1|subject) + (1|site),   data = longitudinal_example,   engine = \"lme4\",   limit = 5 )  # Which fixed effects were kept? selected_fixed <- attr(pruned_mixed, \"selected_vars\") cat(\"Fixed effects kept:\\n\") print(selected_fixed)  # Which were removed? removed_fixed <- attr(pruned_mixed, \"removed_vars\") cat(\"\\nFixed effects removed:\\n\") print(removed_fixed)"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"final-model","dir":"Articles","previous_headings":"Workflow 4: Mixed Models","what":"Final model","title":"Complete Workflows: Real-World Examples","text":"final model contains fixed effects passed VIF threshold, along original random effects.","code":"final_mixed <- attr(pruned_mixed, \"final_model\") summary(final_mixed)"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"vif-verification","dir":"Articles","previous_headings":"Workflow 4: Mixed Models","what":"VIF verification","title":"Complete Workflows: Real-World Examples","text":"can manually verify pruning successfully reduced multicollinearity comparing VIF values pruning. remaining predictors now VIF < 5, indicating acceptable multicollinearity levels mixed-effects modeling.","code":"# Note: This example requires lme4 package library(lme4)  # Fit full model full_formula <- as.formula(paste(\"outcome ~\",                                  paste(paste0(\"x\", 1:5), collapse = \" + \"),                                  \"+ (1|subject) + (1|site)\"))  model_full_mixed <- lmer(full_formula, data = longitudinal_example)  # Extract fixed effects design matrices X_full <- getME(model_full_mixed, \"X\") X_pruned <- getME(final_mixed, \"X\")  # Compute VIF compute_vif <- function(X) {   X_scaled <- scale(X[, -1])  # Remove intercept   sapply(seq_len(ncol(X_scaled)), function(i) {     r2 <- summary(lm(X_scaled[, i] ~ X_scaled[, -i]))$r.squared     1 / (1 - r2)   }) }  vif_full <- compute_vif(X_full) vif_pruned <- compute_vif(X_pruned)  # Compare comparison_vif <- data.frame(   Predictor = colnames(X_pruned)[-1],   VIF_Before = vif_full,   VIF_After = vif_pruned ) print(comparison_vif)"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"vif-reduction-visualization","dir":"Articles","previous_headings":"Workflow 4: Mixed Models","what":"VIF reduction visualization","title":"Complete Workflows: Real-World Examples","text":"plot shows pruning reduced VIF values across retained fixed effects:","code":"# Extract VIF values predictors <- comparison_vif$Predictor vif_before <- comparison_vif$VIF_Before vif_after <- comparison_vif$VIF_After  # Set up bar positions x <- seq_along(predictors) width <- 0.35  # Create plot par(mar = c(5, 4, 4, 2)) plot(   x, vif_before,   type = \"n\",   xaxt = \"n\",   xlab = \"Fixed Effects\",   ylab = \"VIF\",   main = \"VIF Reduction After Pruning (Mixed Model)\",   ylim = c(0, max(vif_before) * 1.15) )  # Add horizontal line at VIF = 5 threshold abline(h = 5, col = \"red\", lty = 2, lwd = 2)  # Before bars (darker) rect(   x - width, 0,   x, vif_before,   col = rgb(0.8, 0.2, 0.2, 0.6),   border = \"white\" )  # After bars (lighter) rect(   x, 0,   x + width, vif_after,   col = rgb(0.2, 0.5, 0.8, 0.6),   border = \"white\" )  # Add x-axis labels axis(1, at = x, labels = predictors, las = 2)  # Add legend legend(   \"topright\",   legend = c(\"Before Pruning\", \"After Pruning\", \"VIF = 5 Threshold\"),   fill = c(rgb(0.8, 0.2, 0.2, 0.6), rgb(0.2, 0.5, 0.8, 0.6), NA),   border = c(\"white\", \"white\", NA),   lty = c(NA, NA, 2),   lwd = c(NA, NA, 2),   col = c(NA, NA, \"red\"),   bty = \"o\",   bg = NA )"},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Complete Workflows: Real-World Examples","text":"four workflows demonstrate corrselect integrates diverse analytical pipelines: Key takeaways: Ecological modeling: Two-stage pruning (correlation → VIF) balances fit stability Survey analysis: force_in protects theoretically important variables reducing redundancy High-dimensional data: Greedy mode enables fast pruning p >> n Mixed models: VIF-based pruning respects hierarchical structure targeting fixed effects Workflow selection guide: use approach: corrPrune(): First-stage dimensionality reduction based purely pairwise correlations modelPrune(): Second-stage refinement accounts model-specific multicollinearity (VIF) Combining : Often yields best results (correlation pruning → VIF refinement → final model)","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"see-also","dir":"Articles","previous_headings":"","what":"See Also","title":"Complete Workflows: Real-World Examples","text":"vignette(\"quickstart\") - Interface overview vignette(\"advanced\") - Custom engines algorithmic control vignette(\"comparison\") - Comparison alternatives vignette(\"theory\") - Mathematical foundations","code":""},{"path":"https://gillescolling.com/corrselect/articles/workflows.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Complete Workflows: Real-World Examples","text":"Thresholds: O’Brien, R. M. (2007). caution regarding rules thumb variance inflation factors. Quality & Quantity, 41(5), 673-690. Dormann, C. F., et al. (2013). Collinearity: review methods deal . Ecography, 36(1), 27-46. Methods: - See package documentation JOSS paper algorithm details","code":""},{"path":"https://gillescolling.com/corrselect/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Gilles Colling. Author, maintainer.","code":""},{"path":"https://gillescolling.com/corrselect/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Colling G (2025). corrselect: Correlation-Based Variable Subset Selection. R package version 1.0.0, https://github.com/gcol33/corrselect.","code":"@Manual{,   title = {corrselect: Correlation-Based Variable Subset Selection},   author = {Gilles Colling},   year = {2025},   note = {R package version 1.0.0},   url = {https://github.com/gcol33/corrselect}, }"},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"CLAUDE.md","text":"corrselect R package performs exhaustive, model-agnostic variable subset selection based pairwise correlation association. identifies maximal subsets variables whose pairwise correlations/associations remain user-defined threshold, helping reduce multicollinearity maintaining interpretability.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"build-and-install","dir":"","previous_headings":"Development Commands","what":"Build and Install","title":"CLAUDE.md","text":"","code":"# Load package for development devtools::load_all()  # Install package locally devtools::install()  # If modifying C++ code, clean DLL first devtools::clean_dll() devtools::install()  # Regenerate documentation (after roxygen2 changes) devtools::document()"},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"testing","dir":"","previous_headings":"Development Commands","what":"Testing","title":"CLAUDE.md","text":"","code":"# Run full test suite devtools::test()  # Run complete package check (includes tests, examples, documentation) devtools::check()  # Run specific test file during development testthat::test_file(\"tests/testthat/test-corrSelect.R\")  # Run test directory testthat::test_dir(\"tests/testthat\")"},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"documentation","dir":"","previous_headings":"Development Commands","what":"Documentation","title":"CLAUDE.md","text":"","code":"# Build vignettes devtools::build_vignettes()  # Build pkgdown site locally (output in docs/) pkgdown::build_site()"},{"path":[]},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"high-level-design","dir":"","previous_headings":"Architecture","what":"High-Level Design","title":"CLAUDE.md","text":"package provides three main user-facing functions converge common C++ backend: corrSelect() - Data frame interface numeric correlation assocSelect() - Data frame interface mixed-type data (numeric, factor, ordered) MatSelect() - Direct correlation/association matrix interface three functions: - Preprocess input (handle missing data, validate types) - Compute validate correlation/association matrix - Call C++ backend findAllMaxSets() enumerate maximal subsets - Return CorrCombo S4 object results","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"c-backend-architecture","dir":"","previous_headings":"Architecture","what":"C++ Backend Architecture","title":"CLAUDE.md","text":"Entry point: src/corrselect_main.cpp::findAllMaxSets() C++ layer implements two exact graph-theoretic algorithms enumerating maximal cliques graph edges represent “sufficiently low correlation”: Recommended using force_in (variables must appear subsets) Exact enumeration maximal independent sets Optional pivoting performance (use_pivot = TRUE) Default algorithm force_in specified Key types (src/corrselect_types.h): - Combo = std::vector<int> (variable indices) - ComboList = std::vector<Combo> (collection subsets) Utilities (src/utils.cpp, src/utils.h): - Matrix validation - Correlation statistics (mean, min, max subsets)","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"r-layer-architecture","dir":"","previous_headings":"Architecture","what":"R Layer Architecture","title":"CLAUDE.md","text":"Data frame preprocessing: - corrSelect(): Filters numeric columns , removes NA rows, computes correlation matrix using cor_method parameter - assocSelect(): Handles mixed types computing appropriate metrics pair type (Pearson, Spearman, Kendall, Eta-squared, Cramér’s V) CorrCombo S4 class (R/CorrCombo.R): - Stores discovered subsets metadata - Slots: subset_list, avg_corr, min_corr, max_corr, threshold, forced_in, search_type, cor_method, n_rows_used, names - Custom show() method user-friendly output - .data.frame() method tidy data extraction Helper functions: - corrSubset() (R/corrSubset.R): Extracts specific subsets original data, option keep non-numeric columns (keepExtra = TRUE) - findAllMaxSets() (R/findAllMaxSets.R): R wrapper calling C++ via Rcpp","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"algorithm-selection-logic","dir":"","previous_headings":"Architecture","what":"Algorithm Selection Logic","title":"CLAUDE.md","text":"Default method selection corrSelect() assocSelect(): - force_in provided → use ELS algorithm - Otherwise → use Bron-Kerbosch algorithm Users can override explicitly setting method = \"els\" method = \"bron-kerbosch\".","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"association-metrics-for-mixed-data","dir":"","previous_headings":"Architecture","what":"Association Metrics for Mixed Data","title":"CLAUDE.md","text":"assocSelect() automatically selects metrics based variable pair types: External correlation methods require additional packages: - bicor: WGCNA package - distance: energy package - maximal: minerva package","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"file-organization","dir":"","previous_headings":"","what":"File Organization","title":"CLAUDE.md","text":"","code":"R/                           # R source files ├── corrSelect.R            # Numeric data frame interface ├── assocSelect.R           # Mixed-type data frame interface ├── MatSelect.R             # Matrix interface ├── CorrCombo.R             # S4 class definition and methods ├── corrSubset.R            # Subset extraction helper └── findAllMaxSets.R        # R wrapper for C++ entry point  src/                         # C++ source files (Rcpp) ├── corrselect_main.cpp     # Main entry point and algorithm dispatch ├── corrselect_types.h      # Type definitions (Combo, ComboList) ├── method_els.{cpp,h}      # Eppstein-Löffler-Strash implementation ├── method_bronkerbosch.{cpp,h}  # Bron-Kerbosch implementation ├── utils.{cpp,h}           # Matrix validation and correlation stats └── RcppExports.cpp         # Generated Rcpp bindings  tests/testthat/             # Unit tests ├── test-corrSelect.R ├── test-assocSelect.R ├── test-CorrCombo.R ├── test-corrMatSelect-els.R ├── test-corrMatSelect-bron-kerbosch.R └── test-corrSubset.R  vignettes/                   # Long-form documentation man/                         # Generated documentation (roxygen2) docs/                        # Generated pkgdown website claude/                      # Claude Code working files (git-ignored)                              # Store generated .md files and notes here"},{"path":[]},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"index-conversion","dir":"","previous_headings":"Important Conventions","what":"Index Conversion","title":"CLAUDE.md","text":"R layer: 1-based indexing (standard R convention) C++ layer: 0-based indexing (standard C++ convention) Conversion happens corrselect_main.cpp: subtract 1 entering C++, add 1 returning R","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"force-in-variables","dir":"","previous_headings":"Important Conventions","what":"Force-In Variables","title":"CLAUDE.md","text":"force_in parameter can : - Character vector variable names (R layer converts indices) - Numeric vector column indices (user must provide 1-based) variables appear every returned subset. converted 0-based indices passed C++.","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"result-ordering","dir":"","previous_headings":"Important Conventions","what":"Result Ordering","title":"CLAUDE.md","text":"Results sorted : 1. Size (descending) - larger subsets first 2. Average absolute correlation (ascending) - lower correlation preferred happens C++ returning R.","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"missing-data-handling","dir":"","previous_headings":"Important Conventions","what":"Missing Data Handling","title":"CLAUDE.md","text":"corrSelect() assocSelect() remove rows NA values computing correlation matrix. warning issued rows dropped. number rows actually used stored CorrCombo object’s n_rows_used slot.","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"testing-guidelines","dir":"","previous_headings":"","what":"Testing Guidelines","title":"CLAUDE.md","text":"Keep tests fast reproducible Always use set.seed() random data Include edge cases (empty data, single variable, correlated, uncorrelated) Test algorithms (ELS Bron-Kerbosch) Test force_in functionality Test mixed-type data scenarios assocSelect tests","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"documentation-1","dir":"","previous_headings":"","what":"Documentation","title":"CLAUDE.md","text":"exported functions use roxygen2 documentation : - @param descriptions - @return type structure - @details algorithm specifics - @examples executable - @seealso cross-references Vignettes provide: - Usage examples real-world scenarios - Performance comparisons algorithms - Guidance threshold selection - Mixed-type data workflows","code":""},{"path":"https://gillescolling.com/corrselect/CLAUDE.html","id":"critical-doi-and-url-handling","dir":"","previous_headings":"Documentation","what":"CRITICAL: DOI and URL Handling","title":"CLAUDE.md","text":"NEVER remove unlink DOIs/URLs without explicit user approval. CRAN check reports broken DOIs URLs: 1. ALWAYS verify correct DOI/URL first 2. ASK user making changes 3. DOI incorrect, search correct one propose update 4. unlink/remove explicit user permission Example: DOI like 10.1016/S0167-5060(06)80001-3 returns 404, search correct DOI (e.g., 10.1016/S0167-5060(13)71063-X) ask user verify updating.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Code of Conduct","text":"aim maintain open, friendly, professional environment within corrselect project. Everyone taking part (contributors, maintainers, users) feel welcome respected, regardless background, identity, experience level.","code":""},{"path":"https://gillescolling.com/corrselect/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Code of Conduct","text":"Examples behavior helps create good environment: Communicating clearly respectfully open different opinions approaches Offering constructive feedback Helping others learn contribute Staying focused collaboration shared goals Examples behavior acceptable: Personal attacks insulting language Disrespectful dismissive comments Sharing private information without consent form harassment hostility toward others","code":""},{"path":"https://gillescolling.com/corrselect/CODE_OF_CONDUCT.html","id":"responsibilities","dir":"","previous_headings":"","what":"Responsibilities","title":"Code of Conduct","text":"Project maintainers responsible clarifying standards acceptable behavior taking fair action necessary. may edit, remove, reject contributions violate Code Conduct disrupt collaboration.","code":""},{"path":"https://gillescolling.com/corrselect/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Code of Conduct","text":"Code Conduct applies project spaces, including discussions, issues, pull requests, community interactions related corrselect.","code":""},{"path":"https://gillescolling.com/corrselect/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Code of Conduct","text":"experience witness behavior violates Code Conduct, please contact maintainer privately. Reports handled discretion, maintainers take appropriate action needed.","code":""},{"path":"https://gillescolling.com/corrselect/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, simplified better fit tone open scientific software project. Contact: gilles.colling051@gmail.com","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contribution Guidelines","title":"Contribution Guidelines","text":"First , thank much taking time contribute corrselect project! document provides guidelines contributing corrselect—codebase documentation. guidelines meant guide , restrict . doubt, use best judgment feel free propose improvements issue pull request.","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"table-of-contents","dir":"","previous_headings":"","what":"Table Of Contents","title":"Contribution Guidelines","text":"Code Conduct Obtaining source Setting R environment Installing source Testing Install dependencies Building documentation Design docs Project organization Contributing workflow Style guidelines Pull request checklist Reporting bugs","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contribution Guidelines","text":"project everyone participating governed Code Conduct (CODE_OF_CONDUCT.md). participating, expected uphold code maintain respectful, inclusive environment.","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Contribution Guidelines","text":"installation guide focused development. regular installation, please see README.","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"obtaining-the-source","dir":"","previous_headings":"Installation","what":"Obtaining the source","title":"Contribution Guidelines","text":"Clone corrselect repository: work development branch:","code":"git clone https://github.com/gcol33/corrselect.git cd corrselect git checkout dev git pull origin dev"},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"setting-up-your-r-environment","dir":"","previous_headings":"Installation","what":"Setting up your R environment","title":"Contribution Guidelines","text":"corrselect R package uses C++ code via Rcpp. Install required tools R (≥ 4.0) Rtools (Windows) Xcode Command Line Tools (macOS) Git editor IDE (RStudio, VS Code, etc.) Install development dependencies Load development build","code":"install.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"rmarkdown\", \"knitr\", \"pkgdown\")) devtools::load_all()"},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"installing-from-source","dir":"","previous_headings":"Installation","what":"Installing from source","title":"Contribution Guidelines","text":"Build install package locally: modify C++ code, rebuild DLL reinstalling: Regenerate documentation :","code":"devtools::install() devtools::clean_dll() devtools::install() devtools::document()"},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"testing","dir":"","previous_headings":"","what":"Testing","title":"Contribution Guidelines","text":"corrselect uses testthat testing. tests located tests/testthat/. Run full test suite: Run complete package check: Run subset tests development: Guidelines: - Keep tests fast reproducible. - Use set.seed() random data. - Include edge cases expected failures. - Prefer small examples large datasets.","code":"devtools::test() devtools::check() testthat::test_dir(\"tests/testthat\") testthat::test_file(\"tests/testthat/test-corrSelect.R\")"},{"path":[]},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"install-dependencies","dir":"","previous_headings":"Documentation","what":"Install dependencies","title":"Contribution Guidelines","text":"","code":"install.packages(c(\"rmarkdown\", \"knitr\", \"pkgdown\"))"},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"building-the-documentation","dir":"","previous_headings":"Documentation","what":"Building the documentation","title":"Contribution Guidelines","text":"Build vignettes: Build pkgdown site locally: generated site saved docs/ directory. Open docs/index.html browser view .","code":"devtools::build_vignettes() pkgdown::build_site()"},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"design-of-the-docs","dir":"","previous_headings":"Documentation","what":"Design of the docs","title":"Contribution Guidelines","text":"Function documentation: man/ (generated roxygen2) Tutorials examples: vignettes/ Website configuration: _pkgdown.yml Package overview: README.md Changelog: NEWS.md","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"project-organization","dir":"","previous_headings":"","what":"Project organization","title":"Contribution Guidelines","text":"","code":"corrselect/ ├── .github/                <- Continuous integration workflows ├── .gitignore ├── .Rbuildignore ├── corrselect.Rproj ├── DESCRIPTION             <- Package metadata ├── NAMESPACE               <- Function exports and imports ├── LICENSE ├── LICENSE.md ├── NEWS.md ├── README.md ├── _pkgdown.yml ├── R/                      <- R source files ├── src/                    <- C++ source files (Rcpp) ├── man/                    <- Generated documentation ├── inst/                   <- Installed files (e.g., CITATION, extdata) ├── vignettes/              <- Long-form documentation and usage examples ├── tests/ │   └── testthat/           <- Unit tests ├── docs/                   <- pkgdown website (generated) ├── doc/                    <- Built vignettes for local preview (ignored in Git) ├── Meta/                   <- Metadata created during package build (ignored) └── corrselect.Rcheck/      <- Artifacts from local package checks (ignored)"},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"contributing-workflow","dir":"","previous_headings":"","what":"Contributing workflow","title":"Contribution Guidelines","text":"Create feature branch Make focused commits clear messages. Run tests checks committing: Update documentation roxygen2 NEWS.md. Update vignettes/examples user-facing behavior changes. Open pull request short description change. Respond review feedback constructively.","code":"git checkout -b feature/my-feature devtools::test() devtools::check()"},{"path":[]},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"r-code","dir":"","previous_headings":"Style guidelines","what":"R code","title":"Contribution Guidelines","text":"Use descriptive names consistent indentation. Prefer vectorized operations loops. Validate inputs early clear error messages. Document exported functions roxygen2.","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"c-code","dir":"","previous_headings":"Style guidelines","what":"C++ code","title":"Contribution Guidelines","text":"Keep headers minimal separate interface implementation. Use RAII possible. Comment algorithmic details numerical behavior. Avoid unnecessary memory allocations.","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"tests","dir":"","previous_headings":"Style guidelines","what":"Tests","title":"Contribution Guidelines","text":"Add update tests functionality changes. Keep tests minimal reproducible. Avoid external dependencies unless essential.","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"pull-request-checklist","dir":"","previous_headings":"","what":"Pull request checklist","title":"Contribution Guidelines","text":"Tests pass (devtools::test() devtools::check()) Documentation updated (roxygen + NEWS.md) Vignettes/examples updated needed unrelated formatting changes PR description clearly explains change","code":""},{"path":"https://gillescolling.com/corrselect/CONTRIBUTING.html","id":"reporting-bugs","dir":"","previous_headings":"","what":"Reporting bugs","title":"Contribution Guidelines","text":"reporting issue, please include: - minimal reproducible example (reprex) - Output sessionInfo() - Expected vs. actual results - Threshold method used (e.g., \"els\" \"bron-kerbosch\") - R operating system version - Toolchain info relevant (e.g., Rtools Windows) contributing corrselect, agree code released license package.","code":""},{"path":"https://gillescolling.com/corrselect/index.html","id":"corrselect","dir":"","previous_headings":"","what":"corrselect: Correlation-Based Variable Selection in R","title":"corrselect: Correlation-Based Variable Selection in R","text":"Fast Flexible Predictor Pruning Data Analysis Modeling corrselect package provides simple, high-level functions predictor pruning using association-based model-based approaches. Whether need reduce multicollinearity modeling clean correlated predictors dataset, corrselect offers fast, deterministic solutions minimal code.","code":""},{"path":"https://gillescolling.com/corrselect/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"corrselect: Correlation-Based Variable Selection in R","text":"","code":"library(corrselect) data(mtcars)  # Association-based pruning (model-free) pruned <- corrPrune(mtcars, threshold = 0.7) names(pruned)  # Model-based pruning (VIF) pruned <- modelPrune(mpg ~ ., data = mtcars, limit = 5) attr(pruned, \"selected_vars\")"},{"path":"https://gillescolling.com/corrselect/index.html","id":"statement-of-need","dir":"","previous_headings":"","what":"Statement of Need","title":"corrselect: Correlation-Based Variable Selection in R","text":"Variable selection central task statistics machine learning, particularly working high-dimensional collinear data. many applications, users aim retain sets variables weakly associated one another avoid redundancy reduce overfitting. Common approaches greedy filtering regularized regression either discard useful features guarantee bounded pairwise associations. package addresses admissible set problem: selecting maximal subsets variables pair exceeds user-defined threshold. generalizes mixed-type data, supports multiple association metrics, allows constrained subset selection via force_in (e.g. always include key predictors). features make package useful domains like: ecological bioclimatic modeling, trait-based species selection, interpretable machine learning pipelines.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/index.html","id":"high-level-pruning-functions","dir":"","previous_headings":"Features","what":"High-Level Pruning Functions","title":"corrselect: Correlation-Based Variable Selection in R","text":"Model-free, works raw data Automatic correlation/association measure selection Exact mode guaranteed optimal solutions (recommended p ≤ 100) Fast greedy mode large datasets (p > 100) Protect important variables force_in VIF-based iterative removal Supports lm, glm, lme4, glmmTMB engines Custom engine support modeling package (INLA, mgcv, brms, etc.) Prunes fixed effects mixed models Returns fitted model pruned predictors","code":""},{"path":"https://gillescolling.com/corrselect/index.html","id":"advanced-subset-enumeration","dir":"","previous_headings":"Features","what":"Advanced Subset Enumeration","title":"corrselect: Correlation-Based Variable Selection in R","text":"Eppstein–Löffler–Strash (ELS) Bron–Kerbosch (optional pivoting) Used internally corrPrune(mode = \"exact\") \"pearson\", \"spearman\", \"kendall\" \"bicor\" (WGCNA), \"distance\" (energy), \"maximal\" (minerva) \"eta\", \"cramersv\" mixed-type data force_in: protect variables removal Deterministic tie-breaking reproducibility","code":""},{"path":"https://gillescolling.com/corrselect/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"corrselect: Correlation-Based Variable Selection in R","text":"","code":"# Install from GitHub remotes::install_github(\"gcol33/corrselect\")"},{"path":[]},{"path":"https://gillescolling.com/corrselect/index.html","id":"association-based-pruning-corrprune","dir":"","previous_headings":"Usage Examples","what":"Association-Based Pruning (corrPrune)","title":"corrselect: Correlation-Based Variable Selection in R","text":"","code":"library(corrselect) data(mtcars)  # Basic: Remove correlated predictors pruned <- corrPrune(mtcars, threshold = 0.7) names(pruned)  # Protect important variables pruned <- corrPrune(mtcars, threshold = 0.7, force_in = \"mpg\")  # Use exact mode (slower, guaranteed optimal) pruned <- corrPrune(mtcars, threshold = 0.7, mode = \"exact\")  # Use greedy mode (faster for large datasets) pruned <- corrPrune(mtcars, threshold = 0.7, mode = \"greedy\")  # Check what was removed attr(pruned, \"selected_vars\")"},{"path":"https://gillescolling.com/corrselect/index.html","id":"model-based-pruning-modelprune","dir":"","previous_headings":"Usage Examples","what":"Model-Based Pruning (modelPrune)","title":"corrselect: Correlation-Based Variable Selection in R","text":"","code":"# Linear model with VIF threshold pruned <- modelPrune(mpg ~ cyl + disp + hp + wt, data = mtcars, limit = 5) attr(pruned, \"removed_vars\")  # GLM with binomial family mtcars$am_binary <- as.factor(mtcars$am) pruned <- modelPrune(am_binary ~ cyl + disp + hp,                      data = mtcars, engine = \"glm\",                      family = binomial(), limit = 5)  # Mixed model (requires lme4) if (requireNamespace(\"lme4\", quietly = TRUE)) {   df <- data.frame(     y = rnorm(100),     x1 = rnorm(100),     x2 = rnorm(100),     group = rep(1:10, each = 10)   )   pruned <- modelPrune(y ~ x1 + x2 + (1|group),                        data = df, engine = \"lme4\", limit = 5) }  # Custom engine (advanced: works with any modeling package) # Example: INLA-based pruning if (requireNamespace(\"INLA\", quietly = TRUE)) {   inla_engine <- list(     name = \"inla\",     fit = function(formula, data, ...) {       INLA::inla(formula = formula, data = data,                  family = \"gaussian\", ...)     },     diagnostics = function(model, fixed_effects) {       # Use posterior SD as badness metric       scores <- model$summary.fixed[, \"sd\"]       names(scores) <- rownames(model$summary.fixed)       scores[fixed_effects]     }   )    pruned <- modelPrune(y ~ x1 + x2, data = df,                        engine = inla_engine, limit = 0.5) }"},{"path":"https://gillescolling.com/corrselect/index.html","id":"exact-subset-enumeration-advanced","dir":"","previous_headings":"Usage Examples","what":"Exact Subset Enumeration (Advanced)","title":"corrselect: Correlation-Based Variable Selection in R","text":"","code":"# Find ALL maximal subsets res <- corrSelect(mtcars, threshold = 0.7) show(res)  # Extract a specific subset subset1 <- corrSubset(res, mtcars, which = 1)  # Convert to data frame as.data.frame(res)"},{"path":"https://gillescolling.com/corrselect/index.html","id":"choosing-between-corrprune-and-modelprune","dir":"","previous_headings":"","what":"Choosing Between corrPrune and modelPrune","title":"corrselect: Correlation-Based Variable Selection in R","text":"Tip: Use corrPrune() first reduce dimensionality, modelPrune() final cleanup within modeling framework.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/index.html","id":"mixed-type-data","dir":"","previous_headings":"Advanced Features","what":"Mixed-Type Data","title":"corrselect: Correlation-Based Variable Selection in R","text":"Use assocSelect() exact enumeration mixed data types:","code":"df <- data.frame(   height = rnorm(30, 170, 10),   weight = rnorm(30, 70, 12),   group  = factor(sample(c(\"A\",\"B\"), 30, TRUE)),   rating = ordered(sample(c(\"low\",\"med\",\"high\"), 30, TRUE)) )  res <- assocSelect(df, threshold = 0.6) show(res)"},{"path":"https://gillescolling.com/corrselect/index.html","id":"precomputed-correlation-matrices","dir":"","previous_headings":"Advanced Features","what":"Precomputed Correlation Matrices","title":"corrselect: Correlation-Based Variable Selection in R","text":"Work directly correlation matrices:","code":"mat <- cor(mtcars) res <- MatSelect(mat, threshold = 0.7, method = \"els\")"},{"path":"https://gillescolling.com/corrselect/index.html","id":"joss-paper","dir":"","previous_headings":"","what":"JOSS Paper","title":"corrselect: Correlation-Based Variable Selection in R","text":"repository includes short paper prepared submission Journal Open Source Software (JOSS). can find manuscript references paper/ directory: paper/paper.md – main text paper/paper.bib – bibliography","code":""},{"path":"https://gillescolling.com/corrselect/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"corrselect: Correlation-Based Variable Selection in R","text":"MIT (see LICENSE.md file)","code":""},{"path":"https://gillescolling.com/corrselect/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Gilles Colling Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://gillescolling.com/corrselect/reference/as.data.frame.CorrCombo.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce CorrCombo to a Data Frame — as.data.frame.CorrCombo","title":"Coerce CorrCombo to a Data Frame — as.data.frame.CorrCombo","text":"Converts CorrCombo object data frame variable combinations.","code":""},{"path":"https://gillescolling.com/corrselect/reference/as.data.frame.CorrCombo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce CorrCombo to a Data Frame — as.data.frame.CorrCombo","text":"","code":"# S3 method for class 'CorrCombo' as.data.frame(x, row.names = NULL, optional = FALSE, ...)"},{"path":"https://gillescolling.com/corrselect/reference/as.data.frame.CorrCombo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce CorrCombo to a Data Frame — as.data.frame.CorrCombo","text":"x CorrCombo object. row.names Optional row names output data frame. optional Logical. Passed data.frame(). ... Additional arguments passed data.frame().","code":""},{"path":"https://gillescolling.com/corrselect/reference/as.data.frame.CorrCombo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce CorrCombo to a Data Frame — as.data.frame.CorrCombo","text":"data frame row corresponds subset variables. Columns named VarName01, VarName02, ..., size largest subset. Subsets shorter maximum length padded NA.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/as.data.frame.CorrCombo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce CorrCombo to a Data Frame — as.data.frame.CorrCombo","text":"","code":"set.seed(1) mat <- matrix(rnorm(100), ncol = 10) colnames(mat) <- paste0(\"V\", 1:10) res <- corrSelect(cor(mat), threshold = 0.5) as.data.frame(res) #>                      VarName01 VarName02 VarName03 #> Subset01 [avg=0.261]        V3        V7       V10 #> Subset02 [avg=0.295]        V1        V7       V10 #> Subset03 [avg=0.300]        V8        V9       V10 #> Subset04 [avg=0.331]        V2        V7       V10 #> Subset05 [avg=0.343]        V9        V7       V10 #> Subset06 [avg=0.350]        V3        V8       V10 #> Subset07 [avg=0.374]        V2        V8       V10 #> Subset08 [avg=0.384]        V6        V7       V10 #> Subset09 [avg=0.388]        V6        V8       V10 #> Subset10 [avg=0.086]        V3        V4      <NA> #> Subset11 [avg=0.098]        V5        V8      <NA> #> Subset12 [avg=0.208]        V1        V4      <NA> #> Subset13 [avg=0.220]        V2        V4      <NA> #> Subset14 [avg=0.295]        V9        V4      <NA> #> Subset15 [avg=0.319]        V6        V4      <NA> #> Subset16 [avg=0.407]        V5        V7      <NA>"},{"path":"https://gillescolling.com/corrselect/reference/assocSelect.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Variable Subsets with Low Association (Mixed-Type Data Frame Interface) — assocSelect","title":"Select Variable Subsets with Low Association (Mixed-Type Data Frame Interface) — assocSelect","text":"Identifies combinations variables common data type (numeric, ordered factors, unordered) factors—whose pair-wise association exceed user-supplied threshold. routine wraps MatSelect() handles pre-processing (type conversion, missing-row removal, constant-column checks) typical data-frame/tibble/data-table inputs.","code":""},{"path":"https://gillescolling.com/corrselect/reference/assocSelect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Variable Subsets with Low Association (Mixed-Type Data Frame Interface) — assocSelect","text":"","code":"assocSelect(   df,   threshold = 0.7,   method = NULL,   force_in = NULL,   method_num_num = c(\"pearson\", \"spearman\", \"kendall\", \"bicor\", \"distance\", \"maximal\"),   method_num_ord = c(\"spearman\", \"kendall\"),   method_ord_ord = c(\"spearman\", \"kendall\"),   ... )"},{"path":"https://gillescolling.com/corrselect/reference/assocSelect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Variable Subsets with Low Association (Mixed-Type Data Frame Interface) — assocSelect","text":"df data frame (tibble / data.table). May contain mix : numeric / integer (treated numeric) ordered factors unordered factors (character vectors coerced factors) threshold Numeric \\((0,1)\\). Maximum allowed pair-wise absolute association. Default 0.7. method Character; subset-search algorithm. One \"els\" \"bron-kerbosch\".  NULL (default) function selects automatically: ELS force_in supplied, otherwise Bron–Kerbosch. force_in Optional character vector column indices specifying variables must appear every returned subset. method_num_num Association measure numeric–numeric pairs. One \"pearson\" (default), \"spearman\", \"kendall\", \"bicor\", \"distance\", \"maximal\". method_num_ord Association measure numeric–ordered pairs. One \"spearman\" (default) \"kendall\". method_ord_ord Association measure ordered–ordered pairs. One \"spearman\" (default) \"kendall\". ... Additional arguments passed unchanged MatSelect() (e.g., use_pivot = TRUE Bron–Kerbosch).","code":""},{"path":"https://gillescolling.com/corrselect/reference/assocSelect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Variable Subsets with Low Association (Mixed-Type Data Frame Interface) — assocSelect","text":"CorrCombo S4 object containing: valid subsets, summary association statistics, metadata (algorithm used, rows kept, forced-variables, etc.). object’s show() method prints association metrics actually used data set.","code":""},{"path":"https://gillescolling.com/corrselect/reference/assocSelect.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select Variable Subsets with Low Association (Mixed-Type Data Frame Interface) — assocSelect","text":"single call can therefore screen data set mixes continuous categorical features return every subset whose internal associations “sufficiently low” metric(s) choose. Rows containing NA dropped warning; constant columns treated zero association every variable. default association measure variable-type combination : numeric – numeric method_num_num (default \"pearson\") numeric – ordered method_num_ord numeric – unordered \"eta\" (ANOVA \\(\\eta^{2}\\)) ordered – ordered method_ord_ord ordered – unordered \"cramersv\" unordered – unordered \"cramersv\" association measures rescaled \\([0,1]\\) thresholding. External packages required \"bicor\" (WGCNA), \"distance\" (energy), \"maximal\" (minerva); informative error thrown missing.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/assocSelect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Variable Subsets with Low Association (Mixed-Type Data Frame Interface) — assocSelect","text":"","code":"set.seed(42) df <- data.frame(   height = rnorm(15, 170, 10),   weight = rnorm(15, 70, 12),   group  = factor(rep(LETTERS[1:3], each = 5)),   score  = ordered(sample(c(\"low\",\"med\",\"high\"), 15, TRUE)) )  ## keep every subset whose internal associations <= 0.6 assocSelect(df, threshold = 0.6) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: mixed #>   AssocMethod: numeric_numeric = pearson, numeric_factor = eta, numeric_ordered #>                = spearman, factor_ordered = cramersv #>   Threshold:   0.600 #>   Subsets:     1 valid combinations #>   Data Rows:   15 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] height, weight, group, score      0.219  0.475     4  ## use Kendall for all rank-based comparisons and force 'height' to appear assocSelect(df,             threshold       = 0.5,             method_num_num  = \"kendall\",             method_num_ord  = \"kendall\",             method_ord_ord  = \"kendall\",             force_in        = \"height\") #> CorrCombo object #> ----------------- #>   Method:      els #>   Correlation: mixed #>   AssocMethod: numeric_numeric = kendall, numeric_factor = eta, numeric_ordered #>                = kendall, factor_ordered = cramersv #>   Threshold:   0.500 #>   Subsets:     1 valid combinations #>   Data Rows:   15 used in correlation #>   Forced-in:   height #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] height, weight, group, score      0.193  0.365     4"},{"path":"https://gillescolling.com/corrselect/reference/bioclim_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Bioclimatic Data for Ecological Modeling — bioclim_example","title":"Example Bioclimatic Data for Ecological Modeling — bioclim_example","text":"simulated dataset 19 WorldClim bioclimatic variables (https://www.worldclim.org/data/bioclim.html) measured 100 geographic locations, species richness response variable. Variables organized correlated blocks representing temperature (BIO1-BIO11) precipitation (BIO12-BIO19).","code":""},{"path":"https://gillescolling.com/corrselect/reference/bioclim_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example Bioclimatic Data for Ecological Modeling — bioclim_example","text":"","code":"bioclim_example"},{"path":"https://gillescolling.com/corrselect/reference/bioclim_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Bioclimatic Data for Ecological Modeling — bioclim_example","text":"data frame 100 rows 20 variables: species_richness Integer. Number species observed (response variable) BIO1 Numeric. Annual Mean Temperature BIO2 Numeric. Mean Diurnal Range BIO3 Numeric. Isothermality BIO4 Numeric. Temperature Seasonality BIO5 Numeric. Max Temperature Warmest Month BIO6 Numeric. Min Temperature Coldest Month BIO7 Numeric. Temperature Annual Range BIO8 Numeric. Mean Temperature Wettest Quarter BIO9 Numeric. Mean Temperature Driest Quarter BIO10 Numeric. Mean Temperature Warmest Quarter BIO11 Numeric. Mean Temperature Coldest Quarter BIO12 Numeric. Annual Precipitation BIO13 Numeric. Precipitation Wettest Month BIO14 Numeric. Precipitation Driest Month BIO15 Numeric. Precipitation Seasonality BIO16 Numeric. Precipitation Wettest Quarter BIO17 Numeric. Precipitation Driest Quarter BIO18 Numeric. Precipitation Warmest Quarter BIO19 Numeric. Precipitation Coldest Quarter","code":""},{"path":"https://gillescolling.com/corrselect/reference/bioclim_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example Bioclimatic Data for Ecological Modeling — bioclim_example","text":"Simulated data based 19 WorldClim bioclimatic variables","code":""},{"path":"https://gillescolling.com/corrselect/reference/bioclim_example.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example Bioclimatic Data for Ecological Modeling — bioclim_example","text":"dataset demonstrates common problem ecological modeling: bioclimatic predictors highly correlated within groups (temperature variables BIO1-BIO11 highly correlated; precipitation variables BIO12-BIO19 moderately correlated), leading multicollinearity issues. species richness response depends subset predictors. Use case: Demonstrating corrPrune() modelPrune() reducing correlated environmental predictors fitting species distribution models.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/bioclim_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Bioclimatic Data for Ecological Modeling — bioclim_example","text":"","code":"data(bioclim_example)  # The 19 WorldClim bioclimatic variables (https://www.worldclim.org/data/bioclim.html) # Many are highly correlated, making them ideal for pruning  # Remove highly correlated variables pruned <- corrPrune(bioclim_example[, -1], threshold = 0.7) ncol(pruned)  # Reduced from 19 to ~8 variables #> [1] 12  # Model-based pruning with VIF model_data <- modelPrune(species_richness ~ .,                          data = bioclim_example,                          limit = 5) attr(model_data, \"selected_vars\") #>  [1] \"BIO1\"  \"BIO3\"  \"BIO4\"  \"BIO6\"  \"BIO8\"  \"BIO9\"  \"BIO10\" \"BIO11\" \"BIO12\" #> [10] \"BIO13\" \"BIO14\" \"BIO15\" \"BIO16\" \"BIO17\" \"BIO18\" \"BIO19\""},{"path":"https://gillescolling.com/corrselect/reference/CorrCombo.html","id":null,"dir":"Reference","previous_headings":"","what":"CorrCombo S4 class — CorrCombo","title":"CorrCombo S4 class — CorrCombo","text":"Holds result corrSelect MatSelect: list valid variable combinations correlation statistics. class stores subsets variables meet specified correlation constraint, along metadata algorithm used, correlation method(s), variables forced every subset, summary statistics combination.","code":""},{"path":"https://gillescolling.com/corrselect/reference/CorrCombo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CorrCombo S4 class — CorrCombo","text":"","code":"# S4 method for class 'CorrCombo' show(object)"},{"path":"https://gillescolling.com/corrselect/reference/CorrCombo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CorrCombo S4 class — CorrCombo","text":"object CorrCombo object printed.","code":""},{"path":"https://gillescolling.com/corrselect/reference/CorrCombo.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"CorrCombo S4 class — CorrCombo","text":"subset_list list character vectors. vector valid subset (variable names). avg_corr numeric vector. Average absolute correlation within subset. min_corr numeric vector. Minimum pairwise absolute correlation subset. max_corr numeric vector. Maximum pairwise absolute correlation within subset. names Character vector variable names used decoding. threshold Numeric scalar. correlation threshold used selection. forced_in Character vector. Variable names forced subset. search_type Character string. One \"els\" \"bron-kerbosch\". cor_method Character string. Either single method (e.g. \"pearson\") \"mixed\" multiple methods used. n_rows_used Integer. Number rows used computing correlation matrix (removing missing values).","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/CorrCombo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CorrCombo S4 class — CorrCombo","text":"","code":"show(new(\"CorrCombo\",   subset_list = list(c(\"A\", \"B\"), c(\"A\", \"C\")),   avg_corr = c(0.2, 0.3),   min_corr = c(0.1, 0.2),   max_corr = c(0.3, 0.4),   names = c(\"A\", \"B\", \"C\"),   threshold = 0.5,   forced_in = character(),   search_type = \"els\",   cor_method = \"mixed\",   n_rows_used = as.integer(5) )) #> CorrCombo object #> ----------------- #>   Method:      els #>   Correlation: mixed #>   Threshold:   0.500 #>   Subsets:     2 valid combinations #>   Data Rows:   5 used in correlation #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] A, B                              0.200  0.300     2 #>   [ 2] A, C                              0.300  0.400     2"},{"path":"https://gillescolling.com/corrselect/reference/corrPrune.html","id":null,"dir":"Reference","previous_headings":"","what":"Association-Based Predictor Pruning — corrPrune","title":"Association-Based Predictor Pruning — corrPrune","text":"corrPrune() performs model-free variable subset selection iteratively removing predictors pairwise associations fall specified threshold. returns single pruned data frame predictors satisfy association constraint.","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrPrune.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Association-Based Predictor Pruning — corrPrune","text":"","code":"corrPrune(   data,   threshold = 0.7,   measure = \"auto\",   mode = \"auto\",   force_in = NULL,   by = NULL,   group_q = 1,   max_exact_p = 20,   ... )"},{"path":"https://gillescolling.com/corrselect/reference/corrPrune.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Association-Based Predictor Pruning — corrPrune","text":"data data.frame containing candidate predictors. threshold Numeric scalar. Maximum allowed pairwise association (default: 0.7). Must non-negative. measure Character string specifying association measure use. Options: \"auto\" (default), \"pearson\", \"spearman\", \"kendall\", \"cramersv\", \"eta\", etc. \"auto\", Pearson correlation used -numeric data, appropriate measures selected mixed-type data. mode Character string specifying search algorithm. Options: \"auto\" (default): uses exact search number predictors <= max_exact_p, otherwise uses greedy search \"exact\": exhaustive search maximal subsets (may slow large p) \"greedy\": fast approximate search using iterative removal force_in Character vector variable names must retained final subset. Default: NULL. Character vector naming one grouping variables. provided, associations computed separately within group, aggregated using quantile specified group_q. Default: NULL (grouping). group_q Numeric scalar (0, 1]. Quantile used aggregate associations across groups provided. Default: 1 (maximum, ensuring threshold holds groups). Use 0.9 90th percentile, etc. max_exact_p Integer. Maximum number predictors exact mode used mode = \"auto\". Default: 20. ... Additional arguments (reserved future use).","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrPrune.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Association-Based Predictor Pruning — corrPrune","text":"data.frame containing pruned subset predictors. result following attributes: selected_vars Character vector retained variable names removed_vars Character vector removed variable names mode Character string indicating mode used (\"exact\" \"greedy\") measure Character string indicating association measure used threshold threshold value used","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrPrune.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Association-Based Predictor Pruning — corrPrune","text":"corrPrune() identifies subset predictors whose pairwise associations threshold. function works several stages: Variable type detection: Identifies numeric vs. categorical predictors Association measurement: Computes appropriate pairwise associations Grouping (optional): specified, computes associations within group aggregates using specified quantile Feasibility check: Verifies force_in variables satisfy threshold constraint Subset selection: Uses either exact greedy search find valid subset Grouped Pruning: provided, function ensures selected predictors satisfy threshold constraint across groups. example, group_q = 1 (default), returned predictors pairwise associations threshold groups. group_q = 0.9, satisfy constraint least 90% groups. Mode Selection: Exact mode guarantees finding maximal subsets returns largest one (deterministic tie-breaking). Greedy mode faster approximate, using deterministic removal strategy based association scores.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/corrPrune.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Association-Based Predictor Pruning — corrPrune","text":"","code":"# Basic numeric data pruning data(mtcars) pruned <- corrPrune(mtcars, threshold = 0.7) names(pruned) #> [1] \"mpg\"  \"drat\" \"qsec\" \"gear\" \"carb\"  # Force certain variables to be included pruned <- corrPrune(mtcars, threshold = 0.7, force_in = \"mpg\")  # Use greedy mode for faster computation pruned <- corrPrune(mtcars, threshold = 0.7, mode = \"greedy\")"},{"path":"https://gillescolling.com/corrselect/reference/corrSelect.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Variable Subsets with Low Correlation (Data Frame Interface) — corrSelect","title":"Select Variable Subsets with Low Correlation (Data Frame Interface) — corrSelect","text":"Identifies combinations numeric variables data frame pairwise absolute correlations fall specified threshold. function wrapper around MatSelect() accepts data frames, tibbles, data tables automatic preprocessing.","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrSelect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Variable Subsets with Low Correlation (Data Frame Interface) — corrSelect","text":"","code":"corrSelect(   df,   threshold = 0.7,   method = NULL,   force_in = NULL,   cor_method = c(\"pearson\", \"spearman\", \"kendall\", \"bicor\", \"distance\", \"maximal\"),   ... )"},{"path":"https://gillescolling.com/corrselect/reference/corrSelect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Variable Subsets with Low Correlation (Data Frame Interface) — corrSelect","text":"df data frame. numeric columns used. threshold numeric value (0, 1). Maximum allowed absolute correlation. Defaults 0.7. method Character. Selection algorithm use. One \"els\" \"bron-kerbosch\". specified, function chooses automatically: \"els\" force_in provided, otherwise \"bron-kerbosch\". force_in Optional character vector numeric indices columns force subsets. cor_method Character string indicating correlation method use. One \"pearson\" (default), \"spearman\", \"kendall\", \"bicor\", \"distance\", \"maximal\". ... Additional arguments passed MatSelect(), e.g., use_pivot.","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrSelect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Variable Subsets with Low Correlation (Data Frame Interface) — corrSelect","text":"object class CorrCombo, containing selected subsets correlation statistics.","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrSelect.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select Variable Subsets with Low Correlation (Data Frame Interface) — corrSelect","text":"numeric columns used correlation analysis. Non‐numeric columns (factors, characters, logicals, etc.) ignored, names types printed inform user. can optionally reattached later using corrSubset() keepExtra = TRUE. Rows missing values removed computing correlations. warning issued rows dropped. cor_method controls correlation matrix computed: \"pearson\": Standard linear correlation. \"spearman\": Rank-based monotonic correlation. \"kendall\": Kendall's tau. \"bicor\": Biweight midcorrelation (WGCNA::bicor). \"distance\": Distance correlation (energy::dcor). \"maximal\": Maximal information coefficient (minerva::mine). \"bicor\", \"distance\", \"maximal\", corresponding package must installed.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/corrSelect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Variable Subsets with Low Correlation (Data Frame Interface) — corrSelect","text":"","code":"set.seed(42) n <- 100  # Create 20 variables: 5 blocks of correlated variables + some noise block1 <- matrix(rnorm(n * 4), ncol = 4) block2 <- matrix(rnorm(n), ncol = 1) block2 <- matrix(rep(block2, 4), ncol = 4) + matrix(rnorm(n * 4, sd = 0.1), ncol = 4) block3 <- matrix(rnorm(n * 4), ncol = 4) block4 <- matrix(rnorm(n * 4), ncol = 4) block5 <- matrix(rnorm(n * 4), ncol = 4)  df <- as.data.frame(cbind(block1, block2, block3, block4, block5)) colnames(df) <- paste0(\"V\", 1:20)  # Add a non-numeric column to be ignored df$label <- factor(sample(c(\"A\", \"B\"), n, replace = TRUE))  # Basic usage corrSelect(df, threshold = 0.8) #> The following variables were excluded from the correlation analysis: #>   - label: unordered factor (excluded) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: pearson #>   Threshold:   0.800 #>   Subsets:     4 valid combinations #>   Data Rows:   100 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] V1, V2, V3, V4, V7, V9, ...       0.075  0.241    17 #>   [ 2] V1, V2, V3, V4, V6, V9, ...       0.075  0.259    17 #>   [ 3] V1, V2, V3, V4, V8, V9, ...       0.075  0.269    17 #>   [ 4] V1, V2, V3, V4, V5, V9, ...       0.076  0.288    17  # Try Bron–Kerbosch with pivoting corrSelect(df, threshold = 0.6, method = \"bron-kerbosch\", use_pivot = TRUE) #> The following variables were excluded from the correlation analysis: #>   - label: unordered factor (excluded) #> CorrCombo object #> ----------------- #>   Method:      bron-kerbosch #>   Correlation: pearson #>   Threshold:   0.600 #>   Subsets:     4 valid combinations #>   Data Rows:   100 used in correlation #>   Pivot:       TRUE #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] V1, V2, V3, V4, V7, V9, ...       0.075  0.241    17 #>   [ 2] V1, V2, V3, V4, V6, V9, ...       0.075  0.259    17 #>   [ 3] V1, V2, V3, V4, V8, V9, ...       0.075  0.269    17 #>   [ 4] V1, V2, V3, V4, V5, V9, ...       0.076  0.288    17  # Force in a specific variable and use Spearman correlation corrSelect(df, threshold = 0.6, force_in = \"V10\", cor_method = \"spearman\") #> The following variables were excluded from the correlation analysis: #>   - label: unordered factor (excluded) #> CorrCombo object #> ----------------- #>   Method:      els #>   Correlation: spearman #>   Threshold:   0.600 #>   Subsets:     4 valid combinations #>   Data Rows:   100 used in correlation #>   Forced-in:   V10 #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] V1, V2, V3, V4, V7, V9, ...       0.076  0.239    17 #>   [ 2] V1, V2, V3, V4, V5, V9, ...       0.076  0.269    17 #>   [ 3] V1, V2, V3, V4, V8, V9, ...       0.076  0.246    17 #>   [ 4] V1, V2, V3, V4, V6, V9, ...       0.076  0.252    17"},{"path":"https://gillescolling.com/corrselect/reference/corrSubset.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Variable Subsets from a CorrCombo Object — corrSubset","title":"Extract Variable Subsets from a CorrCombo Object — corrSubset","text":"Extracts one variable subsets CorrCombo object data frames. Typically used corrSelect MatSelect obtain filtered versions original dataset containing low‐correlation variable combinations.","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrSubset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Variable Subsets from a CorrCombo Object — corrSubset","text":"","code":"corrSubset(res, df, which = \"best\", keepExtra = FALSE)"},{"path":"https://gillescolling.com/corrselect/reference/corrSubset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Variable Subsets from a CorrCombo Object — corrSubset","text":"res CorrCombo object returned corrSelect MatSelect. df data frame matrix. Must contain variables listed res@names. Columns res@names ignored unless keepExtra = TRUE. Subsets extract. One : \"best\" (default) 1: top‐ranked subset. single integer (e.g. 2): nth ranked subset. vector integers (e.g. 1:3): multiple subsets. \"\": available subsets. Subsets ranked decreasing size, increasing average correlation. keepExtra Logical. TRUE, columns df res@names (e.g., factors, characters) retained. Defaults FALSE.","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrSubset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Variable Subsets from a CorrCombo Object — corrSubset","text":"data frame single subset extracted, list data frames multiple subsets extracted. data frame contains selected variables (optionally extras).","code":""},{"path":"https://gillescolling.com/corrselect/reference/corrSubset.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Extract Variable Subsets from a CorrCombo Object — corrSubset","text":"warning issued rows contain missing values selected variables.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/corrSubset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Variable Subsets from a CorrCombo Object — corrSubset","text":"","code":"# Simulate input data set.seed(123) df <- as.data.frame(matrix(rnorm(100), nrow = 10)) colnames(df) <- paste0(\"V\", 1:10)  # Compute correlation matrix cmat <- cor(df)  # Select subsets using corrSelect res <- corrSelect(cmat, threshold = 0.5)  # Extract the best subset (default) corrSubset(res, df) #>            V2          V5        V10          V7 #> 1   1.2240818 -0.69470698  0.9935039  0.37963948 #> 2   0.3598138 -0.20791728  0.5483970 -0.50232345 #> 3   0.4007715 -1.26539635  0.2387317 -0.33320738 #> 4   0.1106827  2.16895597 -0.6279061 -1.01857538 #> 5  -0.5558411  1.20796200  1.3606524 -1.07179123 #> 6   1.7869131 -1.12310858 -0.6002596  0.30352864 #> 7   0.4978505 -0.40288484  2.1873330  0.44820978 #> 8  -1.9666172 -0.46665535  1.5326106  0.05300423 #> 9   0.7013559  0.77996512 -0.2357004  0.92226747 #> 10 -0.4727914 -0.08336907 -1.0264209  2.05008469  # Extract the second-best subset corrSubset(res, df, which = 2) #>             V1          V5        V10          V7 #> 1  -0.56047565 -0.69470698  0.9935039  0.37963948 #> 2  -0.23017749 -0.20791728  0.5483970 -0.50232345 #> 3   1.55870831 -1.26539635  0.2387317 -0.33320738 #> 4   0.07050839  2.16895597 -0.6279061 -1.01857538 #> 5   0.12928774  1.20796200  1.3606524 -1.07179123 #> 6   1.71506499 -1.12310858 -0.6002596  0.30352864 #> 7   0.46091621 -0.40288484  2.1873330  0.44820978 #> 8  -1.26506123 -0.46665535  1.5326106  0.05300423 #> 9  -0.68685285  0.77996512 -0.2357004  0.92226747 #> 10 -0.44566197 -0.08336907 -1.0264209  2.05008469  # Extract the first three subsets corrSubset(res, df, which = 1:3) #> $Subset1 #>            V2          V5        V10          V7 #> 1   1.2240818 -0.69470698  0.9935039  0.37963948 #> 2   0.3598138 -0.20791728  0.5483970 -0.50232345 #> 3   0.4007715 -1.26539635  0.2387317 -0.33320738 #> 4   0.1106827  2.16895597 -0.6279061 -1.01857538 #> 5  -0.5558411  1.20796200  1.3606524 -1.07179123 #> 6   1.7869131 -1.12310858 -0.6002596  0.30352864 #> 7   0.4978505 -0.40288484  2.1873330  0.44820978 #> 8  -1.9666172 -0.46665535  1.5326106  0.05300423 #> 9   0.7013559  0.77996512 -0.2357004  0.92226747 #> 10 -0.4727914 -0.08336907 -1.0264209  2.05008469 #>  #> $Subset2 #>             V1          V5        V10          V7 #> 1  -0.56047565 -0.69470698  0.9935039  0.37963948 #> 2  -0.23017749 -0.20791728  0.5483970 -0.50232345 #> 3   1.55870831 -1.26539635  0.2387317 -0.33320738 #> 4   0.07050839  2.16895597 -0.6279061 -1.01857538 #> 5   0.12928774  1.20796200  1.3606524 -1.07179123 #> 6   1.71506499 -1.12310858 -0.6002596  0.30352864 #> 7   0.46091621 -0.40288484  2.1873330  0.44820978 #> 8  -1.26506123 -0.46665535  1.5326106  0.05300423 #> 9  -0.68685285  0.77996512 -0.2357004  0.92226747 #> 10 -0.44566197 -0.08336907 -1.0264209  2.05008469 #>  #> $Subset3 #>            V2          V5          V6          V7 #> 1   1.2240818 -0.69470698  0.25331851  0.37963948 #> 2   0.3598138 -0.20791728 -0.02854676 -0.50232345 #> 3   0.4007715 -1.26539635 -0.04287046 -0.33320738 #> 4   0.1106827  2.16895597  1.36860228 -1.01857538 #> 5  -0.5558411  1.20796200 -0.22577099 -1.07179123 #> 6   1.7869131 -1.12310858  1.51647060  0.30352864 #> 7   0.4978505 -0.40288484 -1.54875280  0.44820978 #> 8  -1.9666172 -0.46665535  0.58461375  0.05300423 #> 9   0.7013559  0.77996512  0.12385424  0.92226747 #> 10 -0.4727914 -0.08336907  0.21594157  2.05008469 #>   # Extract all subsets corrSubset(res, df, which = \"all\") #> $Subset1 #>            V2          V5        V10          V7 #> 1   1.2240818 -0.69470698  0.9935039  0.37963948 #> 2   0.3598138 -0.20791728  0.5483970 -0.50232345 #> 3   0.4007715 -1.26539635  0.2387317 -0.33320738 #> 4   0.1106827  2.16895597 -0.6279061 -1.01857538 #> 5  -0.5558411  1.20796200  1.3606524 -1.07179123 #> 6   1.7869131 -1.12310858 -0.6002596  0.30352864 #> 7   0.4978505 -0.40288484  2.1873330  0.44820978 #> 8  -1.9666172 -0.46665535  1.5326106  0.05300423 #> 9   0.7013559  0.77996512 -0.2357004  0.92226747 #> 10 -0.4727914 -0.08336907 -1.0264209  2.05008469 #>  #> $Subset2 #>             V1          V5        V10          V7 #> 1  -0.56047565 -0.69470698  0.9935039  0.37963948 #> 2  -0.23017749 -0.20791728  0.5483970 -0.50232345 #> 3   1.55870831 -1.26539635  0.2387317 -0.33320738 #> 4   0.07050839  2.16895597 -0.6279061 -1.01857538 #> 5   0.12928774  1.20796200  1.3606524 -1.07179123 #> 6   1.71506499 -1.12310858 -0.6002596  0.30352864 #> 7   0.46091621 -0.40288484  2.1873330  0.44820978 #> 8  -1.26506123 -0.46665535  1.5326106  0.05300423 #> 9  -0.68685285  0.77996512 -0.2357004  0.92226747 #> 10 -0.44566197 -0.08336907 -1.0264209  2.05008469 #>  #> $Subset3 #>            V2          V5          V6          V7 #> 1   1.2240818 -0.69470698  0.25331851  0.37963948 #> 2   0.3598138 -0.20791728 -0.02854676 -0.50232345 #> 3   0.4007715 -1.26539635 -0.04287046 -0.33320738 #> 4   0.1106827  2.16895597  1.36860228 -1.01857538 #> 5  -0.5558411  1.20796200 -0.22577099 -1.07179123 #> 6   1.7869131 -1.12310858  1.51647060  0.30352864 #> 7   0.4978505 -0.40288484 -1.54875280  0.44820978 #> 8  -1.9666172 -0.46665535  0.58461375  0.05300423 #> 9   0.7013559  0.77996512  0.12385424  0.92226747 #> 10 -0.4727914 -0.08336907  0.21594157  2.05008469 #>  #> $Subset4 #>             V1          V5          V6          V7 #> 1  -0.56047565 -0.69470698  0.25331851  0.37963948 #> 2  -0.23017749 -0.20791728 -0.02854676 -0.50232345 #> 3   1.55870831 -1.26539635 -0.04287046 -0.33320738 #> 4   0.07050839  2.16895597  1.36860228 -1.01857538 #> 5   0.12928774  1.20796200 -0.22577099 -1.07179123 #> 6   1.71506499 -1.12310858  1.51647060  0.30352864 #> 7   0.46091621 -0.40288484 -1.54875280  0.44820978 #> 8  -1.26506123 -0.46665535  0.58461375  0.05300423 #> 9  -0.68685285  0.77996512  0.12385424  0.92226747 #> 10 -0.44566197 -0.08336907  0.21594157  2.05008469 #>  #> $Subset5 #>             V4          V5        V10 #> 1   0.42646422 -0.69470698  0.9935039 #> 2  -0.29507148 -0.20791728  0.5483970 #> 3   0.89512566 -1.26539635  0.2387317 #> 4   0.87813349  2.16895597 -0.6279061 #> 5   0.82158108  1.20796200  1.3606524 #> 6   0.68864025 -1.12310858 -0.6002596 #> 7   0.55391765 -0.40288484  2.1873330 #> 8  -0.06191171 -0.46665535  1.5326106 #> 9  -0.30596266  0.77996512 -0.2357004 #> 10 -0.38047100 -0.08336907 -1.0264209 #>  #> $Subset6 #>              V9          V5        V10 #> 1   0.005764186 -0.69470698  0.9935039 #> 2   0.385280401 -0.20791728  0.5483970 #> 3  -0.370660032 -1.26539635  0.2387317 #> 4   0.644376549  2.16895597 -0.6279061 #> 5  -0.220486562  1.20796200  1.3606524 #> 6   0.331781964 -1.12310858 -0.6002596 #> 7   1.096839013 -0.40288484  2.1873330 #> 8   0.435181491 -0.46665535  1.5326106 #> 9  -0.325931586  0.77996512 -0.2357004 #> 10  1.148807618 -0.08336907 -1.0264209 #>  #> $Subset7 #>            V3          V5        V10 #> 1  -1.0678237 -0.69470698  0.9935039 #> 2  -0.2179749 -0.20791728  0.5483970 #> 3  -1.0260044 -1.26539635  0.2387317 #> 4  -0.7288912  2.16895597 -0.6279061 #> 5  -0.6250393  1.20796200  1.3606524 #> 6  -1.6866933 -1.12310858 -0.6002596 #> 7   0.8377870 -0.40288484  2.1873330 #> 8   0.1533731 -0.46665535  1.5326106 #> 9  -1.1381369  0.77996512 -0.2357004 #> 10  1.2538149 -0.08336907 -1.0264209 #>  #> $Subset8 #>             V4          V5          V6 #> 1   0.42646422 -0.69470698  0.25331851 #> 2  -0.29507148 -0.20791728 -0.02854676 #> 3   0.89512566 -1.26539635 -0.04287046 #> 4   0.87813349  2.16895597  1.36860228 #> 5   0.82158108  1.20796200 -0.22577099 #> 6   0.68864025 -1.12310858  1.51647060 #> 7   0.55391765 -0.40288484 -1.54875280 #> 8  -0.06191171 -0.46665535  0.58461375 #> 9  -0.30596266  0.77996512  0.12385424 #> 10 -0.38047100 -0.08336907  0.21594157 #>  #> $Subset9 #>            V8          V6          V7 #> 1  -0.4910312  0.25331851  0.37963948 #> 2  -2.3091689 -0.02854676 -0.50232345 #> 3   1.0057385 -0.04287046 -0.33320738 #> 4  -0.7092008  1.36860228 -1.01857538 #> 5  -0.6880086 -0.22577099 -1.07179123 #> 6   1.0255714  1.51647060  0.30352864 #> 7  -0.2847730 -1.54875280  0.44820978 #> 8  -1.2207177  0.58461375  0.05300423 #> 9   0.1813035  0.12385424  0.92226747 #> 10 -0.1388914  0.21594157  2.05008469 #>  #> $Subset10 #>              V9          V5          V6 #> 1   0.005764186 -0.69470698  0.25331851 #> 2   0.385280401 -0.20791728 -0.02854676 #> 3  -0.370660032 -1.26539635 -0.04287046 #> 4   0.644376549  2.16895597  1.36860228 #> 5  -0.220486562  1.20796200 -0.22577099 #> 6   0.331781964 -1.12310858  1.51647060 #> 7   1.096839013 -0.40288484 -1.54875280 #> 8   0.435181491 -0.46665535  0.58461375 #> 9  -0.325931586  0.77996512  0.12385424 #> 10  1.148807618 -0.08336907  0.21594157 #>   # Extract best subset and retain additional numeric column df$CopyV1 <- df$V1 corrSubset(res, df, which = 1, keepExtra = TRUE) #>            V2          V5        V10          V7      CopyV1 #> 1   1.2240818 -0.69470698  0.9935039  0.37963948 -0.56047565 #> 2   0.3598138 -0.20791728  0.5483970 -0.50232345 -0.23017749 #> 3   0.4007715 -1.26539635  0.2387317 -0.33320738  1.55870831 #> 4   0.1106827  2.16895597 -0.6279061 -1.01857538  0.07050839 #> 5  -0.5558411  1.20796200  1.3606524 -1.07179123  0.12928774 #> 6   1.7869131 -1.12310858 -0.6002596  0.30352864  1.71506499 #> 7   0.4978505 -0.40288484  2.1873330  0.44820978  0.46091621 #> 8  -1.9666172 -0.46665535  1.5326106  0.05300423 -1.26506123 #> 9   0.7013559  0.77996512 -0.2357004  0.92226747 -0.68685285 #> 10 -0.4727914 -0.08336907 -1.0264209  2.05008469 -0.44566197"},{"path":"https://gillescolling.com/corrselect/reference/cor_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Correlation Matrix with Block Structure — cor_example","title":"Example Correlation Matrix with Block Structure — cor_example","text":"20x20 correlation matrix known block structure designed demonstrating threshold selection, algorithm comparison, visualization examples vignettes.","code":""},{"path":"https://gillescolling.com/corrselect/reference/cor_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example Correlation Matrix with Block Structure — cor_example","text":"","code":"cor_example"},{"path":"https://gillescolling.com/corrselect/reference/cor_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Correlation Matrix with Block Structure — cor_example","text":"20x20 numeric correlation matrix row column names V1-V20. matrix four distinct correlation blocks: Block 1 (V1-V5) High correlation: mean = 0.81, range = (0.75, 0.95) Block 2 (V6-V10) Moderate correlation: mean = 0.57, range = (0.5, 0.7) Block 3 (V11-V15) Low correlation: mean = 0.28, range = (0.2, 0.4) Block 4 (V16-V20) Minimal correlation: mean = 0.06, range = (0.0, 0.15) -block correlations low (range = (0.0, 0.3)). matrix guaranteed positive definite.","code":""},{"path":"https://gillescolling.com/corrselect/reference/cor_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example Correlation Matrix with Block Structure — cor_example","text":"Generated data-raw/create_cor_example.R using seed 20250125.","code":""},{"path":"https://gillescolling.com/corrselect/reference/cor_example.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example Correlation Matrix with Block Structure — cor_example","text":"dataset provides controlled correlation structure useful : Threshold sensitivity analysis (comparing results tau = 0.5, 0.7, 0.9) Algorithm comparison (exact vs greedy modes) Visualization examples (heatmaps, correlation distributions) Reproducible benchmarks across vignettes Expected behavior different thresholds: tau = 0.5: Block 1 requires pruning (pairs > 0.75) tau = 0.7: Blocks 1-2 require pruning tau = 0.9: Block 1 requires pruning","code":""},{"path":"https://gillescolling.com/corrselect/reference/cor_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Correlation Matrix with Block Structure — cor_example","text":"","code":"data(cor_example)  # Matrix dimensions dim(cor_example) #> [1] 20 20  # Visualize structure if (requireNamespace(\"corrplot\", quietly = TRUE)) {   corrplot::corrplot(cor_example, method = \"color\", type = \"upper\",                      tl.col = \"black\", tl.cex = 0.7) }   # Distribution of correlations hist(cor_example[upper.tri(cor_example)],      breaks = 30,      main = \"Distribution of Correlations in cor_example\",      xlab = \"Correlation\",      col = \"steelblue\")   # Use with MatSelect library(corrselect) results <- MatSelect(cor_example, threshold = 0.7, method = \"els\") show(results) #> CorrCombo object #> ----------------- #>   Method:      els #>   Threshold:   0.700 #>   Subsets:     5 valid combinations #>   Data Rows:   20 used in correlation #>  #> Top combinations: #>   No.  Variables                          Avg    Max    Size #>   ------------------------------------------------------------ #>   [ 1] V2, V6, V7, V8, V9, V10, ...      0.173  0.627    16 #>   [ 2] V4, V6, V7, V8, V9, V10, ...      0.176  0.627    16 #>   [ 3] V1, V6, V7, V8, V9, V10, ...      0.181  0.627    16 #>   [ 4] V3, V6, V7, V8, V9, V10, ...      0.182  0.627    16 #>   [ 5] V5, V6, V7, V8, V9, V10, ...      0.183  0.627    16"},{"path":"https://gillescolling.com/corrselect/reference/genes_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Gene Expression Data for Bioinformatics — genes_example","title":"Example Gene Expression Data for Bioinformatics — genes_example","text":"simulated gene expression dataset 200 genes measured across 100 samples, organized co-expression modules binary disease outcome.","code":""},{"path":"https://gillescolling.com/corrselect/reference/genes_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example Gene Expression Data for Bioinformatics — genes_example","text":"","code":"genes_example"},{"path":"https://gillescolling.com/corrselect/reference/genes_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Gene Expression Data for Bioinformatics — genes_example","text":"data frame 100 rows 202 variables: sample_id Character. Unique sample identifier disease_status Factor. Disease status (Healthy, Disease) GENE001, GENE002, GENE003, GENE004, GENE005, GENE006, GENE007, GENE008, GENE009, GENE010, GENE011, GENE012, GENE013, GENE014, GENE015, GENE016, GENE017, GENE018, GENE019, GENE020, GENE021, GENE022, GENE023, GENE024, GENE025, GENE026, GENE027, GENE028, GENE029, GENE030, GENE031, GENE032, GENE033, GENE034, GENE035, GENE036, GENE037, GENE038, GENE039, GENE040, GENE041, GENE042, GENE043, GENE044, GENE045, GENE046, GENE047, GENE048, GENE049, GENE050, GENE051, GENE052, GENE053, GENE054, GENE055, GENE056, GENE057, GENE058, GENE059, GENE060, GENE061, GENE062, GENE063, GENE064, GENE065, GENE066, GENE067, GENE068, GENE069, GENE070, GENE071, GENE072, GENE073, GENE074, GENE075, GENE076, GENE077, GENE078, GENE079, GENE080, GENE081, GENE082, GENE083, GENE084, GENE085, GENE086, GENE087, GENE088, GENE089, GENE090, GENE091, GENE092, GENE093, GENE094, GENE095, GENE096, GENE097, GENE098, GENE099, GENE100, GENE101, GENE102, GENE103, GENE104, GENE105, GENE106, GENE107, GENE108, GENE109, GENE110, GENE111, GENE112, GENE113, GENE114, GENE115, GENE116, GENE117, GENE118, GENE119, GENE120, GENE121, GENE122, GENE123, GENE124, GENE125, GENE126, GENE127, GENE128, GENE129, GENE130, GENE131, GENE132, GENE133, GENE134, GENE135, GENE136, GENE137, GENE138, GENE139, GENE140, GENE141, GENE142, GENE143, GENE144, GENE145, GENE146, GENE147, GENE148, GENE149, GENE150, GENE151, GENE152, GENE153, GENE154, GENE155, GENE156, GENE157, GENE158, GENE159, GENE160, GENE161, GENE162, GENE163, GENE164, GENE165, GENE166, GENE167, GENE168, GENE169, GENE170, GENE171, GENE172, GENE173, GENE174, GENE175, GENE176, GENE177, GENE178, GENE179, GENE180, GENE181, GENE182, GENE183, GENE184, GENE185, GENE186, GENE187, GENE188, GENE189, GENE190, GENE191, GENE192, GENE193, GENE194, GENE195, GENE196, GENE197, GENE198, GENE199, GENE200 Numeric. Gene expression values (log-transformed)","code":""},{"path":"https://gillescolling.com/corrselect/reference/genes_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example Gene Expression Data for Bioinformatics — genes_example","text":"Simulated data based typical gene expression microarray structures","code":""},{"path":"https://gillescolling.com/corrselect/reference/genes_example.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example Gene Expression Data for Bioinformatics — genes_example","text":"dataset simulates high-dimensional, low-sample scenario common genomics. Genes organized four co-expression modules: Module 1 (GENE001-GENE050): Highly correlated (r ~= 0.80), disease-associated Module 2 (GENE051-GENE100): Moderately correlated (r ~= 0.60) Module 3 (GENE101-GENE150): Weakly correlated (r ~= 0.40) Module 4 (GENE151-GENE200): Independent (r ~= 0) Disease outcome depends subset genes Module 1. Use case: Demonstrating corrPrune() mode = \"greedy\" handling high-dimensional data efficiently.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/genes_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Gene Expression Data for Bioinformatics — genes_example","text":"","code":"data(genes_example)  # Greedy pruning for high-dimensional data gene_data <- genes_example[, -(1:2)]  # Exclude ID and outcome pruned <- corrPrune(gene_data, threshold = 0.8, mode = \"greedy\") ncol(pruned)  # Reduced from 200 to ~50 genes #> [1] 177  # Use pruned genes for classification pruned_with_outcome <- data.frame(   disease_status = genes_example$disease_status,   pruned )"},{"path":"https://gillescolling.com/corrselect/reference/longitudinal_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Longitudinal Data for Clinical Research — longitudinal_example","title":"Example Longitudinal Data for Clinical Research — longitudinal_example","text":"simulated longitudinal study dataset 50 subjects measured 10 timepoints , 20 correlated predictors nested random effects (subject site).","code":""},{"path":"https://gillescolling.com/corrselect/reference/longitudinal_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example Longitudinal Data for Clinical Research — longitudinal_example","text":"","code":"longitudinal_example"},{"path":"https://gillescolling.com/corrselect/reference/longitudinal_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Longitudinal Data for Clinical Research — longitudinal_example","text":"data frame 500 rows 25 variables: obs_id Integer. Observation identifier (1-500) subject Factor. Subject identifier (1-50) site Factor. Study site identifier (1-5) time Integer. Measurement timepoint (1-10) outcome Numeric. Continuous outcome variable x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20 Numeric. Correlated predictor variables","code":""},{"path":"https://gillescolling.com/corrselect/reference/longitudinal_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example Longitudinal Data for Clinical Research — longitudinal_example","text":"Simulated data based typical clinical trial designs","code":""},{"path":"https://gillescolling.com/corrselect/reference/longitudinal_example.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example Longitudinal Data for Clinical Research — longitudinal_example","text":"dataset represents typical longitudinal study repeated measures. Predictors correlated within subjects: Predictors x1-x10: Highly correlated (r ~= 0.75) Predictors x11-x20: Moderately correlated (r ~= 0.50) outcome depends time (linear trend), random effects (subject site), subset fixed-effect predictors (x1, x5, x15). Use case: Demonstrating modelPrune() mixed models (lme4 engine) prune fixed effects preserving random effects structure.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/longitudinal_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Longitudinal Data for Clinical Research — longitudinal_example","text":"","code":"data(longitudinal_example)  if (FALSE) { # \\dontrun{ # Prune fixed effects in mixed model (requires lme4) if (requireNamespace(\"lme4\", quietly = TRUE)) {   pruned <- modelPrune(     outcome ~ x1 + x2 + x3 + x4 + x5 + (1|subject) + (1|site),     data = longitudinal_example,     engine = \"lme4\",     limit = 5   )    # Random effects preserved, only fixed effects pruned   attr(pruned, \"selected_vars\") } } # }"},{"path":"https://gillescolling.com/corrselect/reference/MatSelect.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Variable Subsets with Low Correlation or Association (Matrix Interface) — MatSelect","title":"Select Variable Subsets with Low Correlation or Association (Matrix Interface) — MatSelect","text":"Identifies maximal subsets variables symmetric matrix (typically correlation matrix) pairwise absolute values stay specified threshold. Implements exact algorithms Eppstein–Löffler–Strash (ELS) Bron–Kerbosch (without pivoting).","code":""},{"path":"https://gillescolling.com/corrselect/reference/MatSelect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Variable Subsets with Low Correlation or Association (Matrix Interface) — MatSelect","text":"","code":"MatSelect(mat, threshold = 0.7, method = NULL, force_in = NULL, ...)"},{"path":"https://gillescolling.com/corrselect/reference/MatSelect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Variable Subsets with Low Correlation or Association (Matrix Interface) — MatSelect","text":"mat numeric, symmetric matrix 1s diagonal (e.g. correlation matrix). Column names (present) used label output variables. threshold numeric scalar (0, 1). Maximum allowed absolute pairwise value. Defaults 0.7. method Character. Selection algorithm use. One \"els\" \"bron-kerbosch\". specified, function chooses automatically: \"els\" force_in provided, otherwise \"bron-kerbosch\". force_in Optional integer vector 1-based column indices force every subset. ... Additional arguments passed backend, e.g., use_pivot (logical) enabling pivoting Bron–Kerbosch (ignored ELS).","code":""},{"path":"https://gillescolling.com/corrselect/reference/MatSelect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Variable Subsets with Low Correlation or Association (Matrix Interface) — MatSelect","text":"object class CorrCombo, containing valid subsets correlation statistics.","code":""},{"path":"https://gillescolling.com/corrselect/reference/MatSelect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Variable Subsets with Low Correlation or Association (Matrix Interface) — MatSelect","text":"","code":"set.seed(42) mat <- matrix(rnorm(100), ncol = 10) colnames(mat) <- paste0(\"V\", 1:10) cmat <- cor(mat)  # Default method (Bron-Kerbosch) res1 <- MatSelect(cmat, threshold = 0.5)  # Bron–Kerbosch without pivot res2 <- MatSelect(cmat, threshold = 0.5, method = \"bron-kerbosch\", use_pivot = FALSE)  # Bron–Kerbosch with pivoting res3 <- MatSelect(cmat, threshold = 0.5, method = \"bron-kerbosch\", use_pivot = TRUE)  # Force variable 1 into every subset (with warning if too correlated) res4 <- MatSelect(cmat, threshold = 0.5, force_in = 1)"},{"path":"https://gillescolling.com/corrselect/reference/modelPrune.html","id":null,"dir":"Reference","previous_headings":"","what":"Model-Based Predictor Pruning — modelPrune","title":"Model-Based Predictor Pruning — modelPrune","text":"modelPrune() performs iterative removal fixed-effect predictors based model diagnostics (e.g., VIF) remaining predictors satisfy specified threshold. supports linear models, generalized linear models, mixed models.","code":""},{"path":"https://gillescolling.com/corrselect/reference/modelPrune.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model-Based Predictor Pruning — modelPrune","text":"","code":"modelPrune(   formula,   data,   engine = \"lm\",   criterion = \"vif\",   limit = 5,   force_in = NULL,   max_steps = NULL,   ... )"},{"path":"https://gillescolling.com/corrselect/reference/modelPrune.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model-Based Predictor Pruning — modelPrune","text":"formula model formula specifying response predictors. May include random effects mixed models (e.g., y ~ x1 + x2 + (1|group)). data data.frame containing variables formula. engine Either character string built-engines, list defining custom engine. Built-engines (character string): \"lm\" (default): Linear models via stats::lm() \"glm\": Generalized linear models via stats::glm() (requires family argument) \"lme4\": Mixed models via lme4::lmer() lme4::glmer() (requires lme4 package) \"glmmTMB\": Generalized linear mixed models via glmmTMB::glmmTMB() (requires glmmTMB package) Custom engine (named list required components): fit: function(formula, data, ...) returns fitted model object diagnostics: function(model, fixed_effects) returns named numeric vector diagnostic scores (one per fixed effect, higher values = worse) name (optional): character string used error messages (default: \"custom\") criterion Character string specifying diagnostic criterion pruning. built-engines, \"vif\" (Variance Inflation Factor) supported. custom engines, parameter ignored (diagnostics computed engine's diagnostics function). Default: \"vif\". limit Numeric scalar. Maximum allowed value criterion. Predictors diagnostic values exceeding limit iteratively removed. Default: 5 (common VIF threshold). force_in Character vector predictor names must retained final model. variables removed pruning. Default: NULL. max_steps Integer. Maximum number pruning iterations. NULL (default), pruning continues diagnostics limit removable predictors remain. ... Additional arguments passed modeling function (e.g., family glm/glmer, control parameters lme4/glmmTMB).","code":""},{"path":"https://gillescolling.com/corrselect/reference/modelPrune.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model-Based Predictor Pruning — modelPrune","text":"data.frame containing retained predictors (response). result following attributes: selected_vars Character vector retained predictor names removed_vars Character vector removed predictor names (order removal) engine Character string indicating engine used (custom engines, engine's name field) criterion Character string indicating criterion used limit threshold value used final_model final fitted model object (optional)","code":""},{"path":"https://gillescolling.com/corrselect/reference/modelPrune.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model-Based Predictor Pruning — modelPrune","text":"modelPrune() works : Parsing formula identify fixed-effect predictors Fitting initial model Computing diagnostics fixed-effect predictor Checking feasibility force_in constraints Iteratively removing predictor worst diagnostic value (excluding force_in variables) diagnostics <= limit Returning pruned data frame Random Effects: mixed models (lme4, glmmTMB), fixed-effect predictors considered pruning. Random-effect structure preserved exactly specified original formula. VIF Computation: Variance Inflation Factors computed fixed-effects design matrix. categorical predictors, VIF represents inflation entire factor (individual dummy variables). Determinism: algorithm deterministic. Ties diagnostic values broken removing predictor appears last formula. Force-Constraints: variables force_in violate diagnostic threshold, function error. ensures constraint feasible pruning begins.","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/modelPrune.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model-Based Predictor Pruning — modelPrune","text":"","code":"# Linear model with VIF-based pruning data(mtcars) pruned <- modelPrune(mpg ~ ., data = mtcars, engine = \"lm\", limit = 5) names(pruned) #> [1] \"mpg\"  \"drat\" \"qsec\" \"vs\"   \"am\"   \"gear\" \"carb\"  # Force certain predictors to remain pruned <- modelPrune(mpg ~ ., data = mtcars, force_in = \"drat\", limit = 20)  # GLM example (requires family argument) pruned <- modelPrune(am ~ ., data = mtcars, engine = \"glm\",                      family = binomial(), limit = 5) #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred  if (FALSE) { # \\dontrun{ # Custom engine example (INLA) inla_engine <- list(   name = \"inla\",   fit = function(formula, data, ...) {     inla::inla(formula = formula, data = data,                family = list(...)$family %||% \"gaussian\",                control.compute = list(config = TRUE))   },   diagnostics = function(model, fixed_effects) {     scores <- model$summary.fixed[, \"sd\"]     names(scores) <- rownames(model$summary.fixed)     scores[fixed_effects]   } )  pruned <- modelPrune(y ~ x1 + x2 + x3, data = df,                      engine = inla_engine, limit = 0.5) } # }"},{"path":"https://gillescolling.com/corrselect/reference/survey_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Survey Data for Social Science Research — survey_example","title":"Example Survey Data for Social Science Research — survey_example","text":"simulated questionnaire dataset 30 Likert-scale items measuring three latent constructs (satisfaction, engagement, loyalty), plus demographic variables overall satisfaction score.","code":""},{"path":"https://gillescolling.com/corrselect/reference/survey_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example Survey Data for Social Science Research — survey_example","text":"","code":"survey_example"},{"path":"https://gillescolling.com/corrselect/reference/survey_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Survey Data for Social Science Research — survey_example","text":"data frame 200 rows 35 variables: respondent_id Integer. Unique respondent identifier age Integer. Respondent age (18-75 years) gender Factor. Gender (Male, Female, ) education Ordered factor. Education level (High School, Bachelor, Master, PhD) overall_satisfaction Integer. Overall satisfaction score (0-100) satisfaction_1, satisfaction_2, satisfaction_3, satisfaction_4, satisfaction_5, satisfaction_6, satisfaction_7, satisfaction_8, satisfaction_9, satisfaction_10 Ordered factor. Satisfaction items (1-7 Likert scale) engagement_1, engagement_2, engagement_3, engagement_4, engagement_5, engagement_6, engagement_7, engagement_8, engagement_9, engagement_10 Ordered factor. Engagement items (1-7 Likert scale) loyalty_1, loyalty_2, loyalty_3, loyalty_4, loyalty_5, loyalty_6, loyalty_7, loyalty_8, loyalty_9, loyalty_10 Ordered factor. Loyalty items (1-7 Likert scale)","code":""},{"path":"https://gillescolling.com/corrselect/reference/survey_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example Survey Data for Social Science Research — survey_example","text":"Simulated data based typical customer satisfaction survey structures","code":""},{"path":"https://gillescolling.com/corrselect/reference/survey_example.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Example Survey Data for Social Science Research — survey_example","text":"dataset represents common scenario survey research: multiple items measuring similar constructs lead redundancy multicollinearity. Items within construct correlated (satisfaction, engagement, loyalty), constructs inter-correlated. Use case: Demonstrating assocSelect() identifying redundant questionnaire items mixed-type data (ordered factors + numeric variables).","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/reference/survey_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Survey Data for Social Science Research — survey_example","text":"","code":"data(survey_example)  # This dataset has mixed types: numeric (age, overall_satisfaction), # factors (gender, education), and ordered factors (Likert items) str(survey_example[, 1:10]) #> 'data.frame':\t200 obs. of  10 variables: #>  $ respondent_id       : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ age                 : num  38 32 18 18 19 39 33 26 26 42 ... #>  $ gender              : Factor w/ 3 levels \"Female\",\"Male\",..: 2 3 1 2 2 1 1 2 2 1 ... #>  $ education           : Ord.factor w/ 4 levels \"High School\"<..: 3 1 4 2 2 1 1 1 2 3 ... #>  $ overall_satisfaction: num  58 40 44 40 58 67 61 49 51 52 ... #>  $ satisfaction_1      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 5 3 4 5 5 4 4 5 ... #>  $ satisfaction_2      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 4 3 4 6 6 4 4 3 ... #>  $ satisfaction_3      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 4 3 3 4 5 3 4 4 ... #>  $ satisfaction_4      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 6 3 4 4 4 5 4 3 2 4 ... #>  $ satisfaction_5      : Ord.factor w/ 7 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 7 4 5 5 5 4 3 6 4 6 ...  # \\donttest{ # Use assocSelect() for mixed-type data pruning # This may take a few seconds with 34 variables pruned <- assocSelect(survey_example[, -1],  # Exclude respondent_id                       threshold = 0.8,                       method_ord_ord = \"spearman\") length(attr(pruned, \"selected_vars\")) #> [1] 0 # }"},{"path":[]},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"major-release-predictor-pruning-toolkit-3-0-0","dir":"Changelog","previous_headings":"","what":"Major Release: Predictor Pruning Toolkit","title":"corrselect 3.0.0","text":"Version 3.0.0 represents major expansion corrselect specialized subset enumeration tool comprehensive predictor pruning toolkit. Fully backward compatible 2.x - existing code continues work.","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"bug-fixes-3-0-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"corrselect 3.0.0","text":"Added proper handling Inf NA VIF values pruning loop Clamped extreme R² values (> 0.9999) prevent division near-zero Added safety checks prevent removing variables Now uses stats::model.matrix() engines (robust) Eliminated “find columns” warnings Test suite: 261 tests pass zero warnings (CRAN-compliant)","code":""},{"path":[]},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"new-functions-3-0-0","dir":"Changelog","previous_headings":"Major Features","what":"New Functions","title":"corrselect 3.0.0","text":"Model-free pruning using pairwise correlations associations Automatic measure selection (measure = \"auto\") Supports exact mode (small p), greedy mode (large p), auto-selection force_in parameter protect important predictors Returns single pruned data.frame pairwise associations ≤ threshold VIF-based iterative removal multicollinear predictors Supports multiple engines: lm, glm, lme4, glmmTMB Custom engine support: Define modeling backends (INLA, mgcv, brms, etc.) Prunes fixed effects (preserves random effects mixed models) force_in parameter protecting important variables Returns pruned data.frame final fitted model","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"new-c-backend-3-0-0","dir":"Changelog","previous_headings":"Major Features","what":"New C++ Backend","title":"corrselect 3.0.0","text":"Polynomial-time complexity O(p² × k) vs exponential exact search Handles p > 100 efficiently Deterministic tie-breaking reproducibility Used corrPrune(mode = \"greedy\") mode = \"auto\"","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"enhancements-3-0-0","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"corrselect 3.0.0","text":"Exact methods (corrSelect(), assocSelect()) now integrate seamlessly corrPrune() Deterministic subset selection multiple maximal sets exist Improved error messages threshold feasibility checks Better handling edge cases (single predictor, correlated, etc.) Custom engine interface modelPrune(): Users can define custom modeling backends fit diagnostics functions, enabling integration R modeling package","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"documentation-3-0-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"corrselect 3.0.0","text":"Quick Start: 5-minute introduction corrPrune() modelPrune() Complete Workflows: Real-world examples across 4 domains (ecology, social science, genomics, clinical) Comparison Alternatives: choose corrselect vs caret, Boruta, glmnet Performance Benchmarks: Timing comparisons, scalability tests, optimization guidelines Advanced Topics: Algorithms, custom engines (INLA, mgcv), performance optimization, troubleshooting Four new example datasets full documentation (bioclim, survey, genes, longitudinal) Updated README quickstart examples custom engine support Full documentation corrPrune() modelPrune() Usage examples modeling engines","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"package-changes-3-0-0","dir":"Changelog","previous_headings":"","what":"Package Changes","title":"corrselect 3.0.0","text":"Added lme4 glmmTMB Suggests (required respective engines) Version bumped 3.0.0 (major feature release) Updated package description reflect expanded pruning functionality","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"notes-3-0-0","dir":"Changelog","previous_headings":"","what":"Notes","title":"corrselect 3.0.0","text":"breaking changes: Version 3.0.0 fully backward compatible 2.0.1 large predictor sets (p > 20), use corrPrune(mode = \"auto\") best performance Mixed model engines require optional packages: install install.packages(c(\"lme4\", \"glmmTMB\"))","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"corrselect-201","dir":"Changelog","previous_headings":"","what":"corrselect 2.0.1","title":"corrselect 2.0.1","text":"CRAN release: 2025-09-08","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"bug-fixes-2-0-1","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"corrselect 2.0.1","text":"force_in MatSelect() now correctly accepts character column names. els now correctly lists valid subsets single variable forced . corrSelect() now displays appropriate warning one variable remains dropping unsupported columns. Association matrix construction assocSelect() now safely falls back 0 failed meaningless associations (e.g. empty chi-squared tables due sparse combinations unused factor levels).","code":""},{"path":"https://gillescolling.com/corrselect/news/index.html","id":"features-added-2-0-1","dir":"Changelog","previous_headings":"","what":"Features Added","title":"corrselect 2.0.1","text":"assocSelect() now supports logical columns automatically converting factors.","code":""}]
